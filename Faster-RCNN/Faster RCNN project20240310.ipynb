{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HNRS3cs41WZ"
      },
      "source": [
        "## Setup\n",
        "\n",
        "To start, we need to install FiftyOne:\n",
        "\n",
        "*If you're working in Google Colab, be sure to [enable a GPU runtime](https://colab.research.google.com/drive/1P7okDVh6viCIOkii6UAF2O9sTAcKGNWq) before running any cell*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzo4pIFGfTI6",
        "outputId": "966e6b5b-9bbd-4450-d2d8-329b85eb9daf"
      },
      "outputs": [],
      "source": [
        "%pip install git+https://github.com/deepvision-class/starter-code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gtvfsp3wWRH_",
        "outputId": "dc597e68-fcde-4766-f82b-9ff093b2c95e"
      },
      "outputs": [],
      "source": [
        "%pip install fiftyone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-JQbWN4r0Rd"
      },
      "source": [
        "We'll also need pytorch, and torchvision, as well as clone the torchvision GitHub repository to use the training and evaluation utilities provided for the [Torchvision Object Deteciton Tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html#defining-the-dataset) that we are using to train a basic object detection model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dVQMF0SWg5I",
        "outputId": "5df80621-37ad-40ec-dcf7-e06c6170cee3"
      },
      "outputs": [],
      "source": [
        "%%sh\n",
        "\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "# cp references/detection/engine.py ../\n",
        "# cp references/detection/coco_eval.py ../ #change the torch._six to six\n",
        "cp references/detection/coco_utils.py ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "import json\n",
        "import tempfile\n",
        "import numpy as np\n",
        "import copy\n",
        "import torch\n",
        "import six\n",
        "import utils\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from pycocotools.coco import COCO\n",
        "from coco_utils import get_coco_api_from_dataset\n",
        "from collections import defaultdict\n",
        "import pycocotools.mask as mask_util\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "import fiftyone.utils.coco as fouc\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import transforms as T\n",
        "from fiftyone import ViewField "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ2cvwpPWXBt",
        "outputId": "f872ed55-2198-464f-e3bf-90ad11fb1cb3"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FiftyOne and Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5crNDNsRWdPT",
        "outputId": "dc900cb7-485a-48c5-c97b-43a4bbd85353"
      },
      "outputs": [],
      "source": [
        "classes = [\"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\"]\n",
        "#bottle,fork,knife,spoon\n",
        "\n",
        "dataset = foz.load_zoo_dataset(\n",
        "    \"coco-2017\",\n",
        "    splits=[\"validation\",\"train\"],\n",
        "    classes=classes,\n",
        "    max_samples=100,\n",
        "    only_matching=True,\n",
        "    dataset_dir=\"/C/Users/Bliss/Desktop\",\n",
        "    dataset_name=\"Open-c\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc43DB9jNqGD"
      },
      "source": [
        "We will be needing the height and width of images later in this notebook so we need to compute metadata on our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOdY2KQeGz1r"
      },
      "outputs": [],
      "source": [
        "# dataset.compute_metadata()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99mcj42PN4OE"
      },
      "source": [
        "We can create a session and visualize this dataset in the [FiftyOne App](https://voxel51.com/docs/fiftyone/user_guide/app.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m63jPSOmCtK1",
        "outputId": "d19c79a4-e9bc-4dfc-8f60-b786145c77be"
      },
      "outputs": [],
      "source": [
        "session = fo.launch_app(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRR2fVMlWeKI"
      },
      "outputs": [],
      "source": [
        "class FiftyOneTorchDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"A class to construct a PyTorch dataset from a FiftyOne dataset.\n",
        "\n",
        "    Args:\n",
        "        fiftyone_dataset: a FiftyOne dataset or view that will be used for training or testing\n",
        "        transforms (None): a list of PyTorch transforms to apply to images and targets when loading\n",
        "        gt_field (\"ground_truth\"): the name of the field in fiftyone_dataset that contains the\n",
        "            desired labels to load\n",
        "        classes (None): a list of class strings that are used to define the mapping between\n",
        "            class names and indices. If None, it will use all classes present in the given fiftyone_dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        fiftyone_dataset,\n",
        "        transforms=None,\n",
        "        gt_field=\"ground_truth\",\n",
        "        classes=None,\n",
        "    ):\n",
        "        self.samples = fiftyone_dataset\n",
        "        self.transforms = transforms\n",
        "        self.gt_field = gt_field\n",
        "\n",
        "        self.img_paths = self.samples.values(\"filepath\")\n",
        "\n",
        "        self.classes = classes\n",
        "        if not self.classes:\n",
        "            # Get list of distinct labels that exist in the view\n",
        "            self.classes = self.samples.distinct(\n",
        "                \"%s.detections.label\" % gt_field\n",
        "            )\n",
        "\n",
        "        if self.classes[0] != \"background\":\n",
        "            self.classes = [\"background\"] + self.classes\n",
        "\n",
        "        self.labels_map_rev = {c: i for i, c in enumerate(self.classes)}\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        sample = self.samples[img_path]\n",
        "        metadata = sample.metadata\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        area = []\n",
        "        iscrowd = []\n",
        "        detections = sample[self.gt_field].detections\n",
        "        for det in detections:\n",
        "            category_id = self.labels_map_rev[det.label]\n",
        "            coco_obj = fouc.COCOObject.from_label(\n",
        "                det, metadata, category_id=category_id,\n",
        "            )\n",
        "            x, y, w, h = coco_obj.bbox\n",
        "            boxes.append([x, y, x + w, y + h])\n",
        "            labels.append(coco_obj.category_id)\n",
        "            area.append(coco_obj.area)\n",
        "            iscrowd.append(coco_obj.iscrowd)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        target[\"image_id\"] = torch.as_tensor([idx])\n",
        "        target[\"area\"] = torch.as_tensor(area, dtype=torch.float32)\n",
        "        target[\"iscrowd\"] = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def get_classes(self):\n",
        "        return self.classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aLHV0VXVW3l"
      },
      "source": [
        "The following code loads Faster-RCNN with a ResNet50 backbone from Torchvision and modifies the classifier for the number of classes we are training on:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLACOukJFUxd"
      },
      "outputs": [],
      "source": [
        "busy_view = dataset.match(ViewField(\"ground_truth.detections\").length() > 10)\n",
        "\n",
        "busy_torch_dataset = FiftyOneTorchDataset(busy_view)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804
        },
        "id": "xVs_jYcLFXII",
        "outputId": "e8653fa4-3228-4916-a77d-ad3137709821"
      },
      "outputs": [],
      "source": [
        "session.view = busy_view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BjUPS6FWndn"
      },
      "outputs": [],
      "source": [
        "train_transforms = T.Compose([T.ToTensor(), T.RandomHorizontalFlip(0.5)])\n",
        "test_transforms = T.Compose([T.ToTensor()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g6hT1XkWsrn"
      },
      "outputs": [],
      "source": [
        "# split the dataset in train and test set\n",
        "train_view = busy_view.take(size=500)\n",
        "test_view = busy_view.exclude([s.id for s in train_view])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "506vhQ9gZmWX"
      },
      "outputs": [],
      "source": [
        "# use our dataset and defined transformations\n",
        "train_dataset = FiftyOneTorchDataset(train_view, train_transforms,\n",
        "        classes=classes)\n",
        "test_dataset = FiftyOneTorchDataset(test_view, test_transforms,\n",
        "        classes=classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5je6lVBWz5r"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "\n",
        "\n",
        "In this section, we use the functions and datasets we defined above to initialize, train, and evaluate a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83sE-ju4jlda"
      },
      "outputs": [],
      "source": [
        "def coord_trans(bbox, w_pixel, h_pixel, w_amap=7, h_amap=7, mode='a2p'):\n",
        "  assert mode in ('p2a', 'a2p'), 'invalid coordinate transformation mode!'\n",
        "  assert bbox.shape[-1] >= 4, 'the transformation is applied to the first 4 values of dim -1'\n",
        "\n",
        "  if bbox.shape[0] == 0: # corner cases\n",
        "    return bbox\n",
        "    assert mode in ('p2a', 'a2p'), 'invalid coordinate transformation mode!'\n",
        "  assert bbox.shape[-1] >= 4, 'the transformation is applied to the first 4 values of dim -1'\n",
        "\n",
        "  if bbox.shape[0] == 0: # corner cases\n",
        "    return bbox\n",
        "\n",
        "  resized_bbox = bbox.clone()\n",
        "  # could still work if the first dim of bbox is not batch size\n",
        "  # in that case, w_pixel and h_pixel will be scalars\n",
        "  resized_bbox = resized_bbox.view(bbox.shape[0], -1, bbox.shape[-1])\n",
        "  invalid_bbox_mask = (resized_bbox == -1) # indicating invalid bbox\n",
        "\n",
        "  if mode == 'p2a':\n",
        "    # pixel to activation\n",
        "    width_ratio = w_pixel * 1. / w_amap\n",
        "    height_ratio = h_pixel * 1. / h_amap\n",
        "    resized_bbox[:, :, [0, 2]] /= width_ratio.view(-1, 1, 1)\n",
        "    resized_bbox[:, :, [1, 3]] /= height_ratio.view(-1, 1, 1)\n",
        "  else:\n",
        "    # activation to pixel\n",
        "    width_ratio = w_pixel * 1. / w_amap\n",
        "    height_ratio = h_pixel * 1. / h_amap\n",
        "    resized_bbox[:, :, [0, 2]] *= width_ratio.view(-1, 1, 1)\n",
        "    resized_bbox[:, :, [1, 3]] *= height_ratio.view(-1, 1, 1)\n",
        "\n",
        "  resized_bbox.masked_fill_(invalid_bbox_mask, -1)\n",
        "  resized_bbox.resize_as_(bbox)\n",
        "  return resized_bbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcqUlvvepN7o"
      },
      "outputs": [],
      "source": [
        "# Naeimeh added to debug\n",
        "def rel_error(x, y):\n",
        "    \"\"\"Returns relative error between x and y\"\"\"\n",
        "    return torch.max(torch.abs(x - y) / (torch.maximum(torch.abs(x), torch.abs(y)) + 1e-8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fix_random_seed(seed_no=0):\n",
        "  torch.manual_seed(seed_no)\n",
        "  torch.cuda.manual_seed(seed_no)\n",
        "  random.seed(seed_no)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCZJNGlqe5pq"
      },
      "outputs": [],
      "source": [
        "# for plotting\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# data type and device for torch.tensor\n",
        "to_float = {'dtype': torch.float, 'device': 'cpu'}\n",
        "to_float_cuda = {'dtype': torch.float, 'device': 'cuda'}\n",
        "to_double = {'dtype': torch.double, 'device': 'cpu'}\n",
        "to_double_cuda = {'dtype': torch.double, 'device': 'cuda'}\n",
        "to_long = {'dtype': torch.long, 'device': 'cpu'}\n",
        "to_long_cuda = {'dtype': torch.long, 'device': 'cuda'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiB6xpnLimRQ"
      },
      "outputs": [],
      "source": [
        "# Add this code because I want to follow the RCNN tutorial\n",
        "\n",
        "def img_PIL(img):\n",
        "  numpy_array = img.permute(1, 2, 0).mul(255).byte().numpy()\n",
        "  pil_image = Image.fromarray(numpy_array)\n",
        "  return pil_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTWVTn8RPRPe"
      },
      "source": [
        "# Get a sample subset to visualize the code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJFeOZCTe1VP",
        "outputId": "fa16070d-43b4-425d-920f-bc9e1f92e7f1"
      },
      "outputs": [],
      "source": [
        "# default examples for visualization\n",
        "fix_random_seed(1)\n",
        "batch_size = 3\n",
        "sampled_idx = torch.linspace(0, len(train_dataset)-1, steps=batch_size).long()\n",
        "# get the size of each image first\n",
        "h_list = []\n",
        "w_list = []\n",
        "img_list = [] # list of images\n",
        "MAX_NUM_BBOX = 50\n",
        "box_list = torch.LongTensor(batch_size, MAX_NUM_BBOX, 4).fill_(-1) # PADDED GT boxes\n",
        "\n",
        "for idx, i in enumerate(sampled_idx):\n",
        "  # hack to get the original image so we don't have to load from local again...\n",
        "  img, target = train_dataset.__getitem__(i)\n",
        "  img = img_PIL(img)\n",
        "  img_list.append(img)\n",
        "  all_bbox = target['boxes']\n",
        "  if type(all_bbox) == dict:\n",
        "    all_bbox = [all_bbox]\n",
        "  for bbox_idx, one_bbox in enumerate(all_bbox):\n",
        "    bbox = all_bbox[bbox_idx]\n",
        "    obj_cls = target['labels'][bbox_idx]\n",
        "    box_list[idx][bbox_idx] = all_bbox[bbox_idx]\n",
        "\n",
        "  # get sizes\n",
        "  img = np.array(img)\n",
        "\n",
        "  w_list.append(img.shape[1])\n",
        "  h_list.append(img.shape[0])\n",
        "\n",
        "w_list = torch.tensor(w_list, **to_float_cuda)\n",
        "h_list = torch.tensor(h_list, **to_float_cuda)\n",
        "box_list = torch.tensor(box_list, **to_float_cuda)\n",
        "resized_box_list = coord_trans(box_list, w_list, h_list, mode='p2a')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tUBzaiUj8C6"
      },
      "outputs": [],
      "source": [
        "def data_visualizer(img, idx_to_class, bbox=None, pred=None):\n",
        "    img_copy = np.array(img).astype('uint8')\n",
        "\n",
        "    if bbox is not None:\n",
        "        for bbox_idx in range(bbox.shape[0]):\n",
        "            one_bbox = bbox[bbox_idx][:4].int()  # Ensure integer type\n",
        "            cv2.rectangle(img_copy, (int(one_bbox[0]), int(one_bbox[1])), (int(one_bbox[2]), int(one_bbox[3])), (255, 0, 0), 2)\n",
        "            if bbox.shape[1] > 4:  # if class info provided\n",
        "                obj_cls = idx_to_class[bbox[bbox_idx][4].item()]\n",
        "                cv2.putText(img_copy, '%s' % obj_cls, (int(one_bbox[0]), int(one_bbox[1])+15), cv2.FONT_HERSHEY_PLAIN, 1.0, (0, 0, 255), thickness=1)\n",
        "\n",
        "    if pred is not None:\n",
        "        for bbox_idx in range(pred.shape[0]):\n",
        "            one_bbox = pred[bbox_idx][:4].int()  # Ensure integer type\n",
        "            cv2.rectangle(img_copy, (int(one_bbox[0]), int(one_bbox[1])), (int(one_bbox[2]), int(one_bbox[3])), (0, 255, 0), 2)\n",
        "\n",
        "            if pred.shape[1] > 4:  # if class and conf score info provided\n",
        "                obj_cls = idx_to_class[pred[bbox_idx][4].item()]\n",
        "                conf_score = pred[bbox_idx][5].item()\n",
        "                cv2.putText(img_copy, '%s, %.2f' % (obj_cls, conf_score), (int(one_bbox[0]), int(one_bbox[1])+15), cv2.FONT_HERSHEY_PLAIN, 1.0, (0, 0, 255), thickness=1)\n",
        "\n",
        "    plt.imshow(img_copy)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b2gOgME8v-LW",
        "outputId": "44730722-3615-4bb5-dd0b-298dbd8bd994"
      },
      "outputs": [],
      "source": [
        "# visualize GT boxes\n",
        "class_to_idx = {\"bottle\":0, \"wine glass\":1, \"cup\":2, \"fork\":3, \"knife\":4,\n",
        "                \"spoon\":5, \"bowl\":6}\n",
        "idx_to_class = {i:c for c, i in class_to_idx.items()}\n",
        "for i in range(len(img_list)):\n",
        "  valid_box = sum([1 if j != -1 else 0 for j in box_list[i][:, 0]])\n",
        "  data_visualizer(img_list[i], idx_to_class, box_list[i][:valid_box])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VttzMDxx2G9o"
      },
      "source": [
        "# RPN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kan7Ur072JTc",
        "outputId": "2d9f46ba-0bc3-4c1a-db5b-ce3500669f0d"
      },
      "outputs": [],
      "source": [
        "anchor_list = torch.tensor([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [2, 3], [3, 2], [3, 5], [5, 3]], **to_float_cuda)\n",
        "print(anchor_list.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQH31yWG4kxu"
      },
      "source": [
        "## Activated (positive) and negative anchors\n",
        "When training the RPN, we compare the anchor boxes with the ground-truth boxes in order to determine a ground-truth label for the anchor boxes -- should each anchor predict object or background?\n",
        "\n",
        "We assign a positive label to two kinds of anchors:\n",
        "\n",
        "(i) the anchor/anchors with the highest Intersection-overUnion (IoU) overlap with a ground-truth box, or\n",
        "\n",
        "(ii) an anchor that has an IoU overlap higher than 0.7 with any ground-truth box. Note that a single ground-truth box may assign positive labels to multiple anchors.\n",
        "\n",
        "Usually the second condition is sufficient to determine the positive samples; but we still adopt the first condition for the reason that in some rare cases the second condition may find no positive sample.\n",
        "\n",
        "We assign a negative label to a non-positive anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes. Anchors that are neither positive nor negative do not contribute to the training objective"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9WgkTSh5DLM"
      },
      "outputs": [],
      "source": [
        "#helper function\n",
        "def IoU(proposals, bboxes):\n",
        "  iou_mat = None\n",
        "  B, A, H, W, _ = proposals.shape\n",
        "  proposals = proposals.reshape(B, A*H*W, 4)\n",
        "  tl = torch.max(proposals[:, :, :2].unsqueeze(2), bboxes[:, :, :2].unsqueeze(1))\n",
        "  br = torch.min(proposals[:, :, 2:].unsqueeze(2), bboxes[:, :, 2:4].unsqueeze(1))\n",
        "  intersect = torch.prod(br - tl, dim=3) * (tl < br).all(dim=3)\n",
        "  a = torch.prod(bboxes[:, :, 2:4] - bboxes[:, :, :2], dim=2)\n",
        "  b = torch.prod(proposals[:, :, 2:] - proposals[:, :, :2], dim=2)\n",
        "  iou_mat = torch.div(intersect, a.unsqueeze(1) + b.unsqueeze(2) - intersect)\n",
        "\n",
        "  return iou_mat\n",
        "\n",
        "\n",
        "\n",
        "def GenerateGrid(batch_size, w_amap=7, h_amap=7, dtype=torch.float32, device='cuda'):\n",
        "  \"\"\"\n",
        "  Return a grid cell given a batch size (center coordinates).\n",
        "\n",
        "  Inputs:\n",
        "  - batch_size, B\n",
        "  - w_amap: or W', width of the activation map (number of grids in the horizontal dimension)\n",
        "  - h_amap: or H', height of the activation map (number of grids in the vertical dimension)\n",
        "  - W' and H' are always 7 in our case while w and h might vary.\n",
        "\n",
        "  Outputs:\n",
        "  grid: A float32 tensor of shape (B, H', W', 2) giving the (x, y) coordinates\n",
        "        of the centers of each feature for a feature map of shape (B, D, H', W')\n",
        "  \"\"\"\n",
        "  w_range = torch.arange(0, w_amap, dtype=dtype, device=device) + 0.5\n",
        "  h_range = torch.arange(0, h_amap, dtype=dtype, device=device) + 0.5\n",
        "\n",
        "  w_grid_idx = w_range.unsqueeze(0).repeat(h_amap, 1)\n",
        "  h_grid_idx = h_range.unsqueeze(1).repeat(1, w_amap)\n",
        "  grid = torch.stack([w_grid_idx, h_grid_idx], dim=-1)\n",
        "  grid = grid.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n",
        "\n",
        "  return grid\n",
        "\n",
        "\n",
        "\n",
        "def GenerateAnchor(anc, grid):\n",
        "    anchors = None\n",
        "    B, H, W, _ = grid.shape\n",
        "    A, _ = anc.shape\n",
        "    anchors = torch.zeros((B, A, H, W, 4), device = grid.device, dtype = grid.dtype)\n",
        "    for a in range(A):\n",
        "      anchors[:,a,:,:,0] = grid[:,:,:,0] - anc[a,0]/2\n",
        "      anchors[:,a,:,:,1] = grid[:,:,:,1] - anc[a,1]/2\n",
        "      anchors[:,a,:,:,2] = grid[:,:,:,0] + anc[a,0]/2\n",
        "      anchors[:,a,:,:,3] = grid[:,:,:,1] + anc[a,1]/2\n",
        "\n",
        "    return anchors\n",
        "\n",
        "\n",
        "def ReferenceOnActivatedAnchors(anchors, bboxes, grid, iou_mat, pos_thresh=0.7, neg_thresh=0.3):\n",
        "    B, A, h_amap, w_amap, _ = anchors.shape\n",
        "    N = bboxes.shape[1]\n",
        "\n",
        "    # activated/positive anchors\n",
        "    max_iou_per_anc, max_iou_per_anc_ind = iou_mat.max(dim=-1)\n",
        "    max_iou_per_box = iou_mat.max(dim=1, keepdim=True)[0]\n",
        "    activated_anc_mask = (iou_mat == max_iou_per_box) & (max_iou_per_box > 0)\n",
        "    activated_anc_mask |= (iou_mat > pos_thresh) # using the pos_thresh condition as well\n",
        "    # if an anchor matches multiple GT boxes, choose the box with the largest iou\n",
        "    activated_anc_mask = activated_anc_mask.max(dim=-1)[0] # Bx(AxH’xW’)\n",
        "    activated_anc_ind = torch.nonzero(activated_anc_mask.view(-1)).squeeze(-1)\n",
        "\n",
        "    # GT conf scores\n",
        "    GT_conf_scores = max_iou_per_anc[activated_anc_mask] # M\n",
        "\n",
        "    # # GT class\n",
        "    # box_cls = bboxes[:, :, 4].view(B, 1, N).expand((B, A*h_amap*w_amap, N))\n",
        "    # GT_class = torch.gather(box_cls, -1, max_iou_per_anc_ind.unsqueeze(-1)).squeeze(-1) # M\n",
        "    # GT_class = GT_class[activated_anc_mask].long()\n",
        "\n",
        "    bboxes_expand = bboxes[:, :, :4].view(B, 1, N, 4).expand((B, A*h_amap*w_amap, N, 4))\n",
        "    bboxes = torch.gather(bboxes_expand, -2, max_iou_per_anc_ind.unsqueeze(-1) \\\n",
        "      .unsqueeze(-1).expand(B, A*h_amap*w_amap, 1, 4)).view(-1, 4)\n",
        "    bboxes = bboxes[activated_anc_ind]\n",
        "\n",
        "    print('number of pos proposals: ', activated_anc_ind.shape[0])\n",
        "    activated_anc_coord = anchors.view(-1, 4)[activated_anc_ind]\n",
        "\n",
        "    # GT offsets\n",
        "    # bbox and anchor coordinates are x_tl, y_tl, x_br, y_br\n",
        "    # offsets are t_x, t_y, t_w, t_h\n",
        "    wh_offsets = torch.log((bboxes[:, 2:4] - bboxes[:, :2]) \\\n",
        "      / (activated_anc_coord[:, 2:4] - activated_anc_coord[:, :2]))\n",
        "\n",
        "    xy_offsets = (bboxes[:, :2] + bboxes[:, 2:4] - \\\n",
        "      activated_anc_coord[:, :2] - activated_anc_coord[:, 2:4]) / 2.\n",
        "\n",
        "    xy_offsets /= (activated_anc_coord[:, 2:4] - activated_anc_coord[:, :2])\n",
        "\n",
        "    GT_offsets = torch.cat((xy_offsets, wh_offsets), dim=-1)\n",
        "\n",
        "    # negative anchors\n",
        "    negative_anc_mask = (max_iou_per_anc < neg_thresh) # Bx(AxH’xW’)\n",
        "    negative_anc_ind = torch.nonzero(negative_anc_mask.view(-1)).squeeze(-1)\n",
        "    negative_anc_ind = negative_anc_ind[torch.randint(0, negative_anc_ind.shape[0], (activated_anc_ind.shape[0],))]\n",
        "    negative_anc_coord = anchors.view(-1, 4)[negative_anc_ind.view(-1)]\n",
        "\n",
        "    # activated_anc_coord and negative_anc_coord are mainly for visualization purposes\n",
        "    return activated_anc_ind, negative_anc_ind, GT_conf_scores, GT_offsets, \\\n",
        "          activated_anc_coord, negative_anc_coord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fn62Q4oake59",
        "outputId": "ab7f6c20-d9c4-4fbb-ccc0-9c2834749248"
      },
      "outputs": [],
      "source": [
        "# visualization\n",
        "# simply create an activation grid where the cells are in green and the centers in red\n",
        "# you should see the entire image divided by a 7x7 grid, with no gaps on the edges\n",
        "\n",
        "grid_list = GenerateGrid(w_list.shape[0])\n",
        "\n",
        "center = torch.cat((grid_list, grid_list), dim=-1)\n",
        "grid_cell = center.clone()\n",
        "grid_cell[:, :, :, [0, 1]] -= 1. / 2.\n",
        "grid_cell[:, :, :, [2, 3]] += 1. / 2.\n",
        "center = coord_trans(center, w_list, h_list)\n",
        "grid_cell = coord_trans(grid_cell, w_list, h_list)\n",
        "\n",
        "for img, anc, grid in zip(img_list, center, grid_cell):\n",
        "  data_visualizer(img, idx_to_class, anc.reshape(-1, 4), grid.reshape(-1, 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fXazD3I4umC",
        "outputId": "d33db05a-b250-4322-88a1-6ea2ed9e051c"
      },
      "outputs": [],
      "source": [
        "fix_random_seed(0)\n",
        "\n",
        "grid_list = GenerateGrid(w_list.shape[0])\n",
        "anc_list = GenerateAnchor(anchor_list, grid_list)\n",
        "iou_mat = IoU(anc_list, resized_box_list)\n",
        "activated_anc_ind, negative_anc_ind, GT_conf_scores, GT_offsets, \\\n",
        "  activated_anc_coord, negative_anc_coord = ReferenceOnActivatedAnchors(anc_list, resized_box_list, grid_list, iou_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l5uAZ3ib9-IJ",
        "outputId": "a8538be0-d27c-4bee-a3dd-b1331474d505"
      },
      "outputs": [],
      "source": [
        "# visualize the activated anchors\n",
        "anc_per_img = torch.prod(torch.tensor(anc_list.shape[1:-1]))\n",
        "\n",
        "print('*'*80)\n",
        "print('Activated (positive) anchors:')\n",
        "for img, bbox, idx in zip(img_list, box_list, torch.arange(box_list.shape[0])):\n",
        "  anc_ind_in_img = (activated_anc_ind >= idx * anc_per_img) & (activated_anc_ind < (idx+1) * anc_per_img)\n",
        "  print('{} activated anchors!'.format(torch.sum(anc_ind_in_img)))\n",
        "  data_visualizer(img, idx_to_class, bbox[:, :4], coord_trans(activated_anc_coord[anc_ind_in_img], w_list[idx], h_list[idx]))\n",
        "\n",
        "print('*'*80)\n",
        "print('Negative anchors:')\n",
        "for img, bbox, idx in zip(img_list, box_list, torch.arange(box_list.shape[0])):\n",
        "  anc_ind_in_img = (negative_anc_ind >= idx * anc_per_img) & (negative_anc_ind < (idx+1) * anc_per_img)\n",
        "  print('{} negative anchors!'.format(torch.sum(anc_ind_in_img)))\n",
        "  data_visualizer(img, idx_to_class, bbox[:, :4], coord_trans(negative_anc_coord[anc_ind_in_img], w_list[idx], h_list[idx]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WYWIzCpGlql_",
        "outputId": "be1a5cf7-669b-4448-baf5-05b639b6c83a"
      },
      "outputs": [],
      "source": [
        "# visualization\n",
        "print('*'*80)\n",
        "print('All nine anchors should be exactly centered:')\n",
        "anc_list = GenerateAnchor(anchor_list, grid_list[:, 3:4, 3:4])\n",
        "for img, anc in zip(img_list, coord_trans(anc_list, w_list, h_list)):\n",
        "  print(anc.shape)\n",
        "  data_visualizer(img, idx_to_class, anc.reshape(-1, 4))\n",
        "\n",
        "print('*'*80)\n",
        "print('All anchors of the image (cluttered):')\n",
        "anc_list = GenerateAnchor(anchor_list, grid_list) # all\n",
        "for img, anc in zip(img_list, coord_trans(anc_list, w_list, h_list)):\n",
        "  print(anc.shape)\n",
        "  data_visualizer(img, idx_to_class, anc.reshape(-1, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W5kId1qolVo"
      },
      "source": [
        "### TRAINING RPN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyA_ePz1nY6q"
      },
      "outputs": [],
      "source": [
        "def GenerateProposal(anchors, offsets):\n",
        "  proposals = torch.zeros_like(anchors)\n",
        "  anc_trans = torch.zeros_like(anchors)\n",
        "  anc_trans[:, :, :, :, 2:] = (anchors[:, :, :, :, 2:] - anchors[:, :, :, :, :2]) # w, h = br - tl\n",
        "  anc_trans[:, :, :, :, :2] = (anchors[:, :, :, :, 2:] + anchors[:, :, :, :, :2]) / 2 # (br + tl) / 2\n",
        "  new_anc_trans = anc_trans.clone() # avoid inplace operation\n",
        "  new_anc_trans[:, :, :, :, :2] = anc_trans[:, :, :, :, :2] + offsets[:, :, :, :, :2] * anc_trans[:, :, :, :, 2:]\n",
        "  new_anc_trans[:, :, :, :, 2:] = torch.mul(anc_trans[:, :, :, :, 2:], torch.exp(offsets[:, :, :, :, 2:]))\n",
        "\n",
        "  # tansform back\n",
        "  proposals[:, :, :, :, :2] =  new_anc_trans[:, :, :, :, :2] - (new_anc_trans[:, :, :, :, 2:] / 2)\n",
        "  proposals[:, :, :, :, 2:] =  new_anc_trans[:, :, :, :, :2] + (new_anc_trans[:, :, :, :, 2:] / 2)\n",
        "  # print(\"From 1\")\n",
        "  return proposals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDhN0SyVok5C"
      },
      "outputs": [],
      "source": [
        "class ProposalModule(nn.Module):\n",
        "  def __init__(self, in_dim, hidden_dim=256, num_anchors=9, drop_ratio=0.3):\n",
        "    super().__init__()\n",
        "\n",
        "    assert(num_anchors != 0)\n",
        "    self.num_anchors = num_anchors\n",
        "\n",
        "    self.predictHead = nn.Sequential(\n",
        "          nn.Conv2d(in_dim,hidden_dim,3,padding=1),\n",
        "          nn.Dropout(drop_ratio),\n",
        "          nn.LeakyReLU(),\n",
        "          nn.Conv2d(hidden_dim,6*self.num_anchors,1)\n",
        "        )\n",
        "\n",
        "  def _extract_anchor_data(self, anchor_data, anchor_idx):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    - anchor_data: Tensor of shape (B, A, D, H, W) giving a vector of length\n",
        "      D for each of A anchors at each point in an H x W grid.\n",
        "    - anchor_idx: int64 Tensor of shape (M,) giving anchor indices to extract\n",
        "\n",
        "    Returns:\n",
        "    - extracted_anchors: Tensor of shape (M, D) giving anchor data for each\n",
        "      of the anchors specified by anchor_idx.\n",
        "    \"\"\"\n",
        "    B, A, D, H, W = anchor_data.shape\n",
        "    anchor_data = anchor_data.permute(0, 1, 3, 4, 2).contiguous().view(-1, D)\n",
        "    extracted_anchors = anchor_data[anchor_idx]\n",
        "    return extracted_anchors\n",
        "\n",
        "  def forward(self, features, pos_anchor_coord=None, \\\n",
        "              pos_anchor_idx=None, neg_anchor_idx=None):\n",
        "    \n",
        "    if pos_anchor_coord is None or pos_anchor_idx is None or neg_anchor_idx is None:\n",
        "      mode = 'eval'\n",
        "    else:\n",
        "      mode = 'train'\n",
        "    conf_scores, offsets, proposals = None, None, None\n",
        "\n",
        "    anchor_features=self.predictHead(features) #Bx(Ax6)x7x7\n",
        "    # split features into conf_package and offsets_package\n",
        "    B,_,H,W = anchor_features.shape\n",
        "    anchor_features = anchor_features.reshape(B,self.num_anchors,6,H,W)\n",
        "    conf_package = anchor_features[:,:,:2,:,:]\n",
        "    offsets_package = anchor_features[:,:,2:,:,:]\n",
        "    if mode == 'eval':\n",
        "      conf_scores, offsets = conf_package, offsets_package\n",
        "    else:\n",
        "      # train mode\n",
        "      extracted_conf_package_pos = self._extract_anchor_data(conf_package,pos_anchor_idx)\n",
        "      extracted_conf_package_neg = self._extract_anchor_data(conf_package,neg_anchor_idx)\n",
        "      conf_scores = torch.cat((extracted_conf_package_pos,extracted_conf_package_neg), dim=0)[:,0:2]\n",
        "\n",
        "      offsets = self._extract_anchor_data(offsets_package,pos_anchor_idx)\n",
        "\n",
        "      M,_=pos_anchor_coord.shape\n",
        "\n",
        "      proposals = GenerateProposal(pos_anchor_coord.reshape(1,1,1,M,4),\n",
        "                                   offsets.reshape(1,1,1,M,4)).reshape(M,4)\n",
        "\n",
        "    if mode == 'train':\n",
        "      return conf_scores, offsets, proposals\n",
        "    elif mode == 'eval':\n",
        "      return conf_scores, offsets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAWkioh6LF30"
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27ke1GvFLJi8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def ConfScoreRegression(conf_scores, batch_size):\n",
        "  \"\"\"\n",
        "  Binary cross-entropy loss\n",
        "\n",
        "  Inputs:\n",
        "  - conf_scores: Predicted confidence scores, of shape (2M, 2). Assume that the\n",
        "    first M are positive samples, and the last M are negative samples.\n",
        "\n",
        "  Outputs:\n",
        "  - conf_score_loss: Torch scalar\n",
        "  \"\"\"\n",
        "  # the target conf_scores for positive samples are ones and negative are zeros\n",
        "  M = conf_scores.shape[0] // 2\n",
        "  GT_conf_scores = torch.zeros_like(conf_scores)\n",
        "  GT_conf_scores[:M, 0] = 1.\n",
        "  GT_conf_scores[M:, 1] = 1.\n",
        "\n",
        "  conf_score_loss = F.binary_cross_entropy_with_logits(conf_scores, GT_conf_scores, \\\n",
        "                                     reduction='sum') * 1. / batch_size\n",
        "  return conf_score_loss\n",
        "def BboxRegression(offsets, GT_offsets, batch_size):\n",
        "  \"\"\"\"\n",
        "  Use SmoothL1 loss as in Faster R-CNN\n",
        "\n",
        "  Inputs:\n",
        "  - offsets: Predicted box offsets, of shape (M, 4)\n",
        "  - GT_offsets: GT box offsets, of shape (M, 4)\n",
        "\n",
        "  Outputs:\n",
        "  - bbox_reg_loss: Torch scalar\n",
        "  \"\"\"\n",
        "  bbox_reg_loss = F.smooth_l1_loss(offsets, GT_offsets, reduction='sum') * 1. / batch_size\n",
        "  return bbox_reg_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the feature extractor for mobilenet v2, not for AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "  \"\"\"\n",
        "  Image feature extraction with MobileNet.\n",
        "  \"\"\"\n",
        "  def __init__(self, reshape_size=224, pooling=False, verbose=False):\n",
        "    super().__init__()\n",
        "\n",
        "    self.mobilenet = models.mobilenet_v2(pretrained=True)\n",
        "    self.mobilenet = nn.Sequential(*list(self.mobilenet.children())[:-1]) # Remove the last classifier\n",
        "\n",
        "    # average pooling\n",
        "    if pooling:\n",
        "      self.mobilenet.add_module('LastAvgPool', nn.AvgPool2d(math.ceil(reshape_size/32.))) # input: N x 1280 x 7 x 7\n",
        "\n",
        "    for i in self.mobilenet.named_parameters():\n",
        "      i[1].requires_grad = True # fine-tune all\n",
        "\n",
        "    if verbose:\n",
        "      summary(self.mobilenet.cuda(), (3, reshape_size, reshape_size))\n",
        "\n",
        "  def forward(self, img, verbose=False):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    - img: Batch of resized images, of shape Nx3x224x224\n",
        "\n",
        "    Outputs:\n",
        "    - feat: Image feature, of shape Nx1280 (pooled) or Nx1280x7x7\n",
        "    \"\"\"\n",
        "    num_img = img.shape[0]\n",
        "\n",
        "    img_prepro = img\n",
        "\n",
        "    feat = []\n",
        "    process_batch = 500\n",
        "    for b in range(math.ceil(num_img/process_batch)):\n",
        "      feat.append(self.mobilenet(img_prepro[b*process_batch:(b+1)*process_batch]\n",
        "                              ).squeeze(-1).squeeze(-1)) # forward and squeeze\n",
        "    feat = torch.cat(feat)\n",
        "\n",
        "    if verbose:\n",
        "      print('Output feature shape: ', feat.shape)\n",
        "\n",
        "    return feat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FeatureExtractor(verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the feature extractor for AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#It is the code to extract feature from transferlearning Alexnet\n",
        "class AlexFeatureExtractor(nn.Module):\n",
        "  \"\"\"\n",
        "  Image feature extraction with MobileNet.\n",
        "  \"\"\"\n",
        "  def __init__(self, reshape_size=224, pooling=False, verbose=False):\n",
        "    super().__init__()\n",
        "\n",
        "    self.alexnet = models.alexnet(pretrained=True)\n",
        "    self.alexnet = nn.Sequential(*list(self.alexnet.children())[:-1]) # Remove the last classifier\n",
        "\n",
        "    # average pooling\n",
        "    if pooling:\n",
        "      self.alexnet.add_module('LastAvgPool', nn.AvgPool2d(math.ceil(reshape_size/32.))) # input: N x 256 x 7 x 7\n",
        "\n",
        "    for i in self.alexnet.named_parameters():\n",
        "      i[1].requires_grad = True # fine-tune all\n",
        "\n",
        "    if verbose:\n",
        "      summary(self.alexnet.cuda(), (3, reshape_size, reshape_size))\n",
        "\n",
        "  def forward(self, img, verbose=False):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    - img: Batch of resized images, of shape Nx3x224x224\n",
        "\n",
        "    Outputs:\n",
        "    - feat: Image feature, of shape Nx1280 (pooled) or Nx1280x7x7\n",
        "    \"\"\"\n",
        "    num_img = img.shape[0]\n",
        "\n",
        "    img_prepro = img\n",
        "\n",
        "    feat = []\n",
        "    process_batch = 500\n",
        "    for b in range(math.ceil(num_img/process_batch)):\n",
        "      feat.append(self.alexnet(img_prepro[b*process_batch:(b+1)*process_batch]\n",
        "                              ).squeeze(-1).squeeze(-1)) # forward and squeeze\n",
        "    feat = torch.cat(feat)\n",
        "    if verbose:\n",
        "      print('Output feature shape: ', feat.shape)\n",
        "    return feat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLdsouIZMCje"
      },
      "outputs": [],
      "source": [
        "class RPN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # READ ONLY\n",
        "    self.anchor_list = torch.tensor([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [2, 3], [3, 2], [3, 5], [5, 3]])\n",
        "    self.feat_extractor = FeatureExtractor()\n",
        "    self.prop_module = ProposalModule(1280, num_anchors=self.anchor_list.shape[0]) #input size for mobile net 1280\n",
        "\n",
        "  def forward(self, images, bboxes):\n",
        "\n",
        "    # weights to multiply to each loss term\n",
        "    w_conf = 1 # for conf_scores\n",
        "    w_reg = 5 # for offsets\n",
        "    total_loss = None\n",
        "    conf_scores, proposals, features, GT_class, pos_anchor_idx, anc_per_img = \\\n",
        "      None, None, None, None, None, None\n",
        "    # i) Image feature extraction\n",
        "    features = self.feat_extractor(images)\n",
        "\n",
        "    # ii) Grid and anchor generation\n",
        "    batch_size = images.shape[0]\n",
        "    grid = GenerateGrid(batch_size)\n",
        "    # anchors = GenerateAnchor(self.anchor_list.cuda(), grid)\n",
        "    anchors = GenerateAnchor(self.anchor_list.to(grid.device, grid.dtype), grid) # why this affects the # of proposals\n",
        "\n",
        "    # iii-1) Compute IoU between anchors and GT boxes\n",
        "    iou_mat = IoU(anchors, bboxes)\n",
        "    # iii-2) determine activated & negative anchors, and GT_offsets, GT_class\n",
        "    pos_anchor_idx, negative_anc_ind, _, GT_offsets,activated_anc_coord,_ = \\\n",
        "      ReferenceOnActivatedAnchors(anchors, bboxes, grid, iou_mat)\n",
        "\n",
        "    # iv) Compute conf_scores, offsets, proposals through the prediction network\n",
        "    conf_scores, offsets, proposals = self.prop_module(features,activated_anc_coord,\n",
        "                                                       pos_anchor_idx, negative_anc_ind)\n",
        "    anc_per_img = torch.prod(torch.tensor(anchors.shape[1:-1]))\n",
        "\n",
        "    # v) Compute total loss\n",
        "    conf_loss = ConfScoreRegression(conf_scores, features.shape[0]) # conf_loss\n",
        "    # print(conf_scores)\n",
        "    reg_loss = BboxRegression(offsets, GT_offsets, features.shape[0]) # reg_loss\n",
        "    total_loss = w_conf * conf_loss + w_reg * reg_loss\n",
        "    \n",
        "    return total_loss\n",
        "  \n",
        "  \n",
        "  def inference(self, images, thresh=0.5, nms_thresh=0.7, mode='RPN'):\n",
        "    \n",
        "    assert mode in ('RPN', 'FasterRCNN'), 'invalid inference mode!'\n",
        "\n",
        "    features, final_conf_probs, final_proposals = None, None, None\n",
        "\n",
        "    # Here we predict the RPN proposal coordinates `final_proposals` and        #\n",
        "    # confidence scores `final_conf_probs`.                                     #\n",
        "    # The overall steps are similar to the forward pass but now you do not need  #\n",
        "    # to decide the activated nor negative anchors.                              #\n",
        "    # Threshold the conf_scores based on the threshold value `thresh`.     #\n",
        "    # Then, apply NMS to the filtered proposals given the threshold `nms_thresh`.#\n",
        "\n",
        "\n",
        "    final_conf_probs, final_proposals = [],[]\n",
        "    # i) Image feature extraction\n",
        "    features = self.feat_extractor(images)\n",
        "\n",
        "    # ii) Grid and anchor generation\n",
        "    batch_size = images.shape[0]\n",
        "    grid = GenerateGrid(batch_size)\n",
        "    # anchors = GenerateAnchor(self.anchor_list.cuda(), grid)\n",
        "    anchors = GenerateAnchor(self.anchor_list.to(grid.device, grid.dtype), grid)\n",
        "\n",
        "    # iii) Compute conf_scores, proposals, class_prob through the prediction network\n",
        "    conf_scores, offsets = self.prop_module(features)\n",
        "    #offsets: (B, A, 4, H', W')\n",
        "    #conf_scores: (B, A, 2, H', W')\n",
        "    B,A,_,H,W = conf_scores.shape\n",
        "\n",
        "    offsets = offsets.permute((0,1,3,4,2))\n",
        "    proposals = GenerateProposal(anchors, offsets) #proposals:B,A,H,W,4\n",
        "    # transform\n",
        "    conf_scores = torch.sigmoid(conf_scores[:,:,0,:,:]) # only look at the 1st confidence score which represent obj_conf\n",
        "    conf_scores = conf_scores.permute((0,2,3,1)).reshape(batch_size,-1)\n",
        "    proposals = proposals.permute((0,2,3,1,4)).reshape(batch_size,-1,4)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      # get proposals, confidence scores for i-th image\n",
        "      sub_conf_scores = conf_scores[i]\n",
        "      sub_proposals = proposals[i]\n",
        "\n",
        "      # filter by conf_scores\n",
        "      mask1 = sub_conf_scores > thresh\n",
        "      sub_conf_scores = sub_conf_scores[mask1]\n",
        "      sub_proposals = sub_proposals[mask1,:]\n",
        "\n",
        "      # filter by nms\n",
        "      mask2 = nms(sub_proposals, sub_conf_scores, iou_threshold=nms_thresh)\n",
        "      # append result\n",
        "      final_proposals.append(sub_proposals[mask2,:])\n",
        "      final_conf_probs.append(sub_conf_scores[mask2].unsqueeze(1))\n",
        "\n",
        "\n",
        "    if mode == 'RPN':\n",
        "      features = [torch.zeros_like(i) for i in final_conf_probs] # dummy class\n",
        "    return final_proposals, final_conf_probs, features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# COCO dataset collate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def coco_collate_fn(batch_lst, reshape_size=224):\n",
        "  preprocess = transforms.Compose([\n",
        "    transforms.Resize((reshape_size, reshape_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "  batch_size = len(batch_lst)\n",
        "\n",
        "  img_batch = torch.zeros(batch_size, 3, reshape_size, reshape_size)\n",
        "\n",
        "  max_num_box = max(len(batch_lst[i][1]['labels']) \\\n",
        "                    for i in range(batch_size))\n",
        "\n",
        "  box_batch = torch.Tensor(batch_size, max_num_box, 5).fill_(-1.)# create box_batch filled with -1 due to inconsistency of box numbers\n",
        "  w_list = []\n",
        "  h_list = []\n",
        "  img_id_list = []  \n",
        "\n",
        "  for i in range(batch_size):\n",
        "    img, ann = batch_lst[i]\n",
        "    w_list.append(img.size[0]) # image width\n",
        "    h_list.append(img.size[1]) # image height\n",
        "    img_id_list.append(ann['image_id'].item()) #Image Id\n",
        "    img_batch[i] = preprocess(img)\n",
        "    all_bbox = ann['boxes']\n",
        "    for bbox_idx, one_bbox in enumerate(all_bbox):\n",
        "      bbox = one_bbox\n",
        "      obj_cls = idx_to_class[ann['labels'][bbox_idx].item()-1]\n",
        "      box_batch[i][bbox_idx] =torch.Tensor([float(bbox[0]), float(bbox[1]),\n",
        "            float(bbox[2]), float(bbox[3]), class_to_idx[obj_cls]])\n",
        "  h_batch = torch.tensor(h_list)\n",
        "  w_batch = torch.tensor(w_list)\n",
        "\n",
        "  return img_batch, box_batch, w_batch, h_batch, img_id_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def DetectionSolver(detector, torch_dataset, learning_rate=3e-3,\n",
        "                    lr_decay=1, num_epochs=20, checkpoint_path='detector_checkpoint.pth'):\n",
        "    # I changed the code since I don't use dataloader\n",
        "    # and simply input torch_dataset for this function\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        torch_dataset, batch_size=2, shuffle=True, num_workers=0,\n",
        "        collate_fn=coco_collate_fn)\n",
        "\n",
        "    # ship model to GPU\n",
        "    detector.to(**to_float_cuda)\n",
        "\n",
        "    # optimizer setup\n",
        "    optimizer = optim.Adam(\n",
        "        filter(lambda p: p.requires_grad, detector.parameters()),\n",
        "        learning_rate)  # leave betas and eps by default\n",
        "    lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer,\n",
        "                                               lambda epoch: lr_decay ** epoch)\n",
        "\n",
        "    # sample minibatch data\n",
        "    loss_history = []\n",
        "\n",
        "    detector.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        for iter_num, data_batch in enumerate(train_loader):\n",
        "            images, boxes, w_batch, h_batch, _ = data_batch\n",
        "            resized_boxes = coord_trans(boxes, w_batch, h_batch, mode='p2a')\n",
        "            images = images.to(**to_float_cuda)\n",
        "            resized_boxes = resized_boxes.to(**to_float_cuda)\n",
        "\n",
        "            # Forward pass\n",
        "            loss = detector(images, resized_boxes)\n",
        "            \n",
        "            # Backward pass and optimization step\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            loss_history.append(loss.item())\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print progress\n",
        "            print('(Epoch {}/{} Iter {}/{}) Loss: {:.4f}'.format(\n",
        "                epoch + 1, num_epochs, iter_num + 1, len(train_loader), loss.item()))\n",
        "\n",
        "        # Save checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': detector.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss.item(),\n",
        "        }, checkpoint_path)\n",
        "\n",
        "        end_time = time.time()\n",
        "        print('(Epoch {}/{}): Time per epoch: {:.2f}s'.format(\n",
        "            epoch + 1, num_epochs, end_time - start_time))\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    # plot the training losses\n",
        "    plt.plot(loss_history)\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss History')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZsNEXmzAMfvC",
        "outputId": "29263992-32c0-48b6-d00f-dc563f662ca0"
      },
      "outputs": [],
      "source": [
        "\n",
        "RPNSolver = DetectionSolver\n",
        "num_sample = 10\n",
        "for lr in [1e-3]:\n",
        "  print('lr: ', lr)\n",
        "  rpn = RPN()\n",
        "  RPNSolver(rpn, busy_torch_dataset, learning_rate=lr, num_epochs=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Overfitting to small dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nms(boxes, scores, iou_threshold=0.5, topk=None):\n",
        "  \"\"\"\n",
        "  Non-maximum suppression removes overlapping bounding boxes.\n",
        "\n",
        "  Inputs:\n",
        "  - boxes: top-left and bottom-right coordinate values of the bounding boxes\n",
        "    to perform NMS on, of shape Nx4\n",
        "  - scores: scores for each one of the boxes, of shape N\n",
        "  - iou_threshold: discards all overlapping boxes with IoU > iou_threshold; float\n",
        "  - topk: If this is not None, then return only the topk highest-scoring boxes.\n",
        "    Otherwise if this is None, then return all boxes that pass NMS.\n",
        "\n",
        "  Outputs:\n",
        "  - keep: torch.long tensor with the indices of the elements that have been\n",
        "    kept by NMS, sorted in decreasing order of scores; of shape [num_kept_boxes]\n",
        "  \"\"\"\n",
        "\n",
        "  if (not boxes.numel()) or (not scores.numel()):\n",
        "    return torch.zeros(0, dtype=torch.long)\n",
        "\n",
        "  keep = None\n",
        "\n",
        "  keep = []\n",
        "  # print(keep.dtype)\n",
        "  indexing = torch.argsort(scores, descending=True)\n",
        "  boxes_sort = boxes[indexing, :]\n",
        "  # print(boxes_sort)\n",
        "  areas = torch.prod(boxes[:, 2:] - boxes[:, :2], dim=1)\n",
        "  # print(areas.shape)\n",
        "  while indexing.size()[0] > 0:\n",
        "    # still left\n",
        "    # print(indexing.size()[0])\n",
        "    idx = indexing[0]\n",
        "    max_box = boxes[idx] # current max\n",
        "    # print(keep)\n",
        "    # print(idx)\n",
        "    #torch.cat((keep, idx))\n",
        "    keep.append(idx)\n",
        "    # compute iou:\n",
        "    tl = torch.max(max_box[:2], boxes[indexing][:, :2]) # should broadcast\n",
        "    # print(\"tl is\", tl)\n",
        "    br = torch.min(max_box[2:], boxes[indexing][:, 2:])\n",
        "    #print(torch.prod(br - tl, dim=3))\n",
        "    intersect = torch.prod(br - tl, dim=1) * (tl < br).all(dim=1)\n",
        "    # print(intersect.shape)\n",
        "    a = areas[idx] # (1, )\n",
        "    b = areas #(N, 1)\n",
        "\n",
        "    iou_mat = torch.div(intersect, a + b[indexing] - intersect).squeeze() #(N, )\n",
        "    # print(iou_mat)\n",
        "    left = torch.where(iou_mat <= iou_threshold)\n",
        "    indexing = indexing[left]\n",
        "    # print(indexing.shape)\n",
        "    # print(left)\n",
        "  if topk is None:\n",
        "    pass\n",
        "  else:\n",
        "    keep = keep[:topk]\n",
        "  keep = torch.tensor(keep, **to_long_cuda).to(scores.device)\n",
        "  return keep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def DetectionInference(detector, torch_dataset, idx_to_class, thresh=0.8, nms_thresh=0.3, output_dir=None):\n",
        "  data_loader = torch.utils.data.DataLoader(\n",
        "    torch_dataset, batch_size=2, shuffle=True, num_workers=0,\n",
        "    collate_fn=coco_collate_fn)\n",
        "  # ship model to GPU\n",
        "  detector.to(**to_float_cuda)\n",
        "\n",
        "  detector.eval()\n",
        "  start_t = time.time()\n",
        "\n",
        "  if output_dir is not None:\n",
        "    det_dir = 'mAP/input/detection-results'\n",
        "    gt_dir = 'mAP/input/ground-truth'\n",
        "    if os.path.exists(det_dir):\n",
        "      shutil.rmtree(det_dir)\n",
        "    os.mkdir(det_dir)\n",
        "    if os.path.exists(gt_dir):\n",
        "      shutil.rmtree(gt_dir)\n",
        "    os.mkdir(gt_dir)\n",
        "\n",
        "  for iter_num, data_batch in enumerate(data_loader):\n",
        "    images, boxes, w_batch, h_batch, img_ids = data_batch\n",
        "    images = images.to(**to_float_cuda)\n",
        "\n",
        "    final_proposals, final_conf_scores, final_class = detector.inference(images, thresh=thresh, nms_thresh=nms_thresh)\n",
        "\n",
        "    # clamp on the proposal coordinates\n",
        "    batch_size = len(images)\n",
        "    for idx in range(batch_size):\n",
        "      torch.clamp_(final_proposals[idx][:, 0::2], min=0, max=w_batch[idx])\n",
        "      torch.clamp_(final_proposals[idx][:, 1::2], min=0, max=h_batch[idx])\n",
        "\n",
        "      # visualization\n",
        "      # get the original image\n",
        "      # hack to get the original image so we don't have to load from local again...\n",
        "      i = batch_size*iter_num + idx\n",
        "      img, _ = torch_dataset.__getitem__(i)\n",
        "\n",
        "      valid_box = sum([1 if j != -1 else 0 for j in boxes[idx][:, 0]])\n",
        "      final_all = torch.cat((final_proposals[idx], \\\n",
        "        final_class[idx].float(), final_conf_scores[idx]), dim=-1).cpu()\n",
        "      resized_proposals = coord_trans(final_all, w_batch[idx], h_batch[idx])\n",
        "\n",
        "      # write results to file for evaluation (use mAP API https://github.com/Cartucho/mAP for now...)\n",
        "      if output_dir is not None:\n",
        "        file_name = img_ids[idx].replace('.jpg', '.txt')\n",
        "        with open(os.path.join(det_dir, file_name), 'w') as f_det, \\\n",
        "          open(os.path.join(gt_dir, file_name), 'w') as f_gt:\n",
        "          print('{}: {} GT bboxes and {} proposals'.format(img_ids[idx], valid_box, resized_proposals.shape[0]))\n",
        "          for b in boxes[idx][:valid_box]:\n",
        "            f_gt.write('{} {:.2f} {:.2f} {:.2f} {:.2f}\\n'.format(idx_to_class[b[4].item()], b[0], b[1], b[2], b[3]))\n",
        "          for b in resized_proposals:\n",
        "            f_det.write('{} {:.6f} {:.2f} {:.2f} {:.2f} {:.2f}\\n'.format(idx_to_class[b[4].item()], b[5], b[0], b[1], b[2], b[3]))\n",
        "      else:\n",
        "        data_visualizer(img, idx_to_class, boxes[idx][:valid_box], resized_proposals)\n",
        "\n",
        "  end_t = time.time()\n",
        "  print('Total inference time: {:.1f}s'.format(end_t-start_t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RPNInference = DetectionInference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  RPNInference(rpn, busy_torch_dataset, idx_to_class, thresh=0.8, nms_thresh=0.3)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "1HNRS3cs41WZ",
        "vZDc2VuwNDm0",
        "qRKqLZrWV0VR"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myproject",
      "language": "python",
      "name": "myproject"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
