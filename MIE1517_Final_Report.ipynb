{"cells":[{"cell_type":"markdown","source":["## **Overview**"],"metadata":{"id":"ESPRPINkEx-i"}},{"cell_type":"markdown","source":["Our project is focused on building a pipeline for a household object locator. The pipeline will use our voice as input and process the object that we want to detect. Then we will use object localization and detection to search for the object in question using image feed from a camera. We will then use an LLM to process the outputs of the localization algorithm to help guide the user where to look. Currently, we have results for two sections. **Part 1** will be focused on the Object Localization and Detection module results and **Part 2** will be focused on our Speech to Text module. Finally, **Part 3** will explain what we have left to do and why we believe we will succeed in this project."],"metadata":{"id":"KoU4wqesE78n"}},{"cell_type":"markdown","source":["## **Part 1: Object Localization and Detection**\n","\n"],"metadata":{"id":"V_RuP2ZEf0zr"}},{"cell_type":"markdown","source":["##Overview\n","We are using Faster RCNN model architecture for object localization and detection. The Faster RCNN is implemented in 2 stages namely: Region Proposal Network (RPN) and Object Detection/Classification.\n","\n","**Stage 1: Region Proposal Network (RPN)**\n","\n","Objective: To generate region proposals where there might be an object based on objectness scores.\n","Architecture: It comprises a feature extraction backbone (using a pre-trained model such as MobileNet) and a proposal module (network of 2 convolutional layers)\n","Outputs: RPN generates proposed regions with probabilities of having an object in them\n","\n","**Stage 2: Object Detection/Classification Module**\n","\n","Objective: To classify the proposed regions into specific object categories and further refine the bounding box positions.\n","Architecture: currently we have 1 convolutional layer and 2 linear layers (not shown in this progress report)\n","Outputs: class labels and coordinates of the bounding boxes for each detected object.\n","Dataset\n","\n","We are training our model on a subset of COCO dataset; a large-scale object detection, segmentation, and captioning dataset.\n","Classes of interest are: \"mouse\", \"keyboard\", \"laptop\",\"cellphone\""],"metadata":{"id":"HKB_i0QvNbs1"}},{"cell_type":"code","source":["%%sh\n","\n","git clone https://github.com/pytorch/vision.git\n","cd vision\n","git checkout v0.3.0\n","\n","cp references/detection/utils.py ../\n","cp references/detection/transforms.py ../\n","cp references/detection/coco_utils.py ../"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1d3iS--dADzg","executionInfo":{"status":"ok","timestamp":1712219731635,"user_tz":240,"elapsed":119,"user":{"displayName":"yanpan chung","userId":"01040412682694527349"}},"outputId":"bdf9e38d-e684-4a73-ec4a-2ba0e4fda70a"},"execution_count":95,"outputs":[{"output_type":"stream","name":"stderr","text":["fatal: destination path 'vision' already exists and is not an empty directory.\n","HEAD is now at be376084d8 version check against PyTorch's CUDA version\n"]}]},{"cell_type":"code","source":["import os\n","import shutil\n","import math\n","import sys\n","import time\n","import torch\n","import json\n","import tempfile\n","import numpy as np\n","import copy\n","import torch\n","import six\n","import utils\n","import random\n","import matplotlib.pyplot as plt\n","import cv2\n","\n","import torchvision\n","from torchvision import models\n","from torchsummary import summary\n","import torch.nn as nn\n","from torchvision import transforms\n","from PIL import Image\n","from pycocotools.cocoeval import COCOeval\n","from pycocotools.coco import COCO\n","from coco_utils import get_coco_api_from_dataset\n","from collections import defaultdict\n","import pycocotools.mask as mask_util\n","import fiftyone as fo\n","import fiftyone.zoo as foz\n","import fiftyone.utils.coco as fouc\n","import torch.nn.functional as F\n","from torch import optim\n","import transforms as T\n","from fiftyone import ViewField"],"metadata":{"id":"H5wr58edhm3k","executionInfo":{"status":"ok","timestamp":1712219731825,"user_tz":240,"elapsed":3,"user":{"displayName":"yanpan chung","userId":"01040412682694527349"}}},"execution_count":96,"outputs":[]},{"cell_type":"markdown","source":["Helper function to preprocess image inputs."],"metadata":{"id":"se-QF1Zw4UOJ"}},{"cell_type":"code","source":["#Transform torch img to Python image library (PIL)\n","\n","def img_PIL(img):\n","  numpy_array = img.permute(1, 2, 0).mul(255).byte().numpy()\n","  pil_image = Image.fromarray(numpy_array)\n","  return pil_image\n","\n","# Naeimeh added to debug\n","def rel_error(x, y):\n","    \"\"\"Returns relative error between x and y\"\"\"\n","    return torch.max(torch.abs(x - y) / (torch.maximum(torch.abs(x), torch.abs(y)) + 1e-8))\n","\n","# Fix random seedds for all modules\n","def fix_random_seed(seed_no=0):\n","  torch.manual_seed(seed_no)\n","  torch.cuda.manual_seed(seed_no)\n","  random.seed(seed_no)\n","\n","# for plotting\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# data type and device for torch.tensor\n","to_float = {'dtype': torch.float, 'device': 'cuda'}\n","to_double = {'dtype': torch.double, 'device': 'cuda'}\n","to_long = {'dtype': torch.long, 'device': 'cuda'}\n"],"metadata":{"id":"GRJFNoMt4WUa","executionInfo":{"status":"ok","timestamp":1712219731825,"user_tz":240,"elapsed":3,"user":{"displayName":"yanpan chung","userId":"01040412682694527349"}}},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":["###Data Preprocessing\n"],"metadata":{"id":"Ic-TkRwqpc7e"}},{"cell_type":"markdown","source":["Load the CoCo dataset with FiftyOne"],"metadata":{"id":"qvJGZG-G4dMi"}},{"cell_type":"code","source":["classes = [\"mouse\",\n","           \"keyboard\",\n","           \"laptop\",\n","          \"cell phone\"\n","              ]\n","\n","dataset = foz.load_zoo_dataset(\n","    \"coco-2017\",\n","    splits=[\"validation\",\"train\"],\n","    classes=classes,\n","    max_samples=20,# Change the sample number if you need more sample\n","    only_matching=True,\n","    dataset_dir=\"/fiftyone\",\n","    dataset_name=\"test stage\"\n",")\n","\n","class_to_idx = {\"mouse\":0,\n","                \"keyboard\":1,\n","                \"laptop\":2,\n","                \"cell phone\":3\n","                }\n","idx_to_class = {i:c for c, i in class_to_idx.items()}"],"metadata":{"id":"6LtIVSuJ4cgB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FiftyOneTorchDataset(torch.utils.data.Dataset):\n","    \"\"\"A class to construct a PyTorch dataset from a FiftyOne dataset.\n","\n","    Args:\n","        fiftyone_dataset: a FiftyOne dataset or view that will be used for training or testing\n","        transforms (None): a list of PyTorch transforms to apply to images and targets when loading\n","        gt_field (\"ground_truth\"): the name of the field in fiftyone_dataset that contains the\n","            desired labels to load\n","        classes (None): a list of class strings that are used to define the mapping between\n","            class names and indices. If None, it will use all classes present in the given fiftyone_dataset.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        fiftyone_dataset,\n","        transforms=None,\n","        gt_field=\"ground_truth\",\n","        classes=None,\n","    ):\n","        self.samples = fiftyone_dataset\n","        self.transforms = transforms\n","        self.gt_field = gt_field\n","        self.img_paths = self.samples.values(\"filepath\")\n","\n","\n","        self.classes = classes\n","        if not self.classes:\n","            # Get list of distinct labels that exist in the view\n","            self.classes = self.samples.distinct(\n","                \"%s.detections.label\" % gt_field\n","            )\n","\n","        if self.classes[0] != \"background\":\n","            self.classes = [\"background\"] + self.classes\n","\n","        self.labels_map_rev = {c: i for i, c in enumerate(self.classes)}\n","\n","    def __getitem__(self, idx):\n","        img_path = self.img_paths[idx]\n","        sample = self.samples[img_path]\n","        metadata = sample.metadata\n","        img = Image.open(img_path).convert(\"RGB\")\n","\n","        boxes = []\n","        labels = []\n","        area = []\n","        iscrowd = []\n","        detections = sample[self.gt_field].detections\n","        for det in detections:\n","            category_id = self.labels_map_rev[det.label]\n","            coco_obj = fouc.COCOObject.from_label(\n","                det, metadata, category_id=category_id,\n","            )\n","            x, y, w, h = coco_obj.bbox\n","            boxes.append([x, y, x + w, y + h])\n","            labels.append(coco_obj.category_id-1)\n","            area.append(coco_obj.area)\n","            iscrowd.append(coco_obj.iscrowd)\n","\n","        target = {}\n","        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n","        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n","        target[\"image_id\"] = torch.as_tensor([idx])\n","        target[\"area\"] = torch.as_tensor(area, dtype=torch.float32)\n","        target[\"iscrowd\"] = torch.as_tensor(iscrowd, dtype=torch.int64)\n","\n","        if self.transforms is not None:\n","            img, target = self.transforms(img, target)\n","            img=img_PIL(img)\n","\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","\n","    def get_classes(self):\n","        return self.classes"],"metadata":{"id":"LiaM7RXC4440","executionInfo":{"status":"ok","timestamp":1712219763059,"user_tz":240,"elapsed":12,"user":{"displayName":"yanpan chung","userId":"01040412682694527349"}}},"execution_count":99,"outputs":[]},{"cell_type":"code","source":["# Data spliting\n","transforms_ = T.Compose([T.ToTensor()])\n","\n","# split the dataset in train and test set\n","train_view = dataset.match_tags('train') # we will use it to train later\n","test_view = dataset.match_tags('validation')\n","# use our dataset and defined transformations\n","train_dataset = FiftyOneTorchDataset(train_view, transforms_,\n","        classes=classes)\n","val_dataset = FiftyOneTorchDataset(test_view, transforms_,\n","        classes=classes)"],"metadata":{"id":"Lz7Cynzp460f","executionInfo":{"status":"ok","timestamp":1712221393569,"user_tz":240,"elapsed":204,"user":{"displayName":"yanpan chung","userId":"01040412682694527349"}}},"execution_count":130,"outputs":[]},{"cell_type":"code","source":["# Use Fiftyone API to select some of the image in coco dataset for demonstration\n","# You can access the API and pick around 10 images for testing the inference\n","session=fo.launch_app(dataset)"],"metadata":{"id":"QPmltTBe9vUO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Visualization\n","\n","In this section, we use the functions and datasets we defined above to initialize, train, and do sanity test"],"metadata":{"id":"5z2U28iL5AKg"}},{"cell_type":"code","source":["# Tranform between image to activation (feature map) or the other way around\n","# Tranform between image to activation (feature map) or the other way around\n","def coord_trans(bbox, w_pixel, h_pixel, w_amap=7, h_amap=7, mode='a2p'):\n","  #  w_amap=7, h_amap=7 will follow the backbone feature map\n","  assert mode in ('p2a', 'a2p'), 'invalid coordinate transformation mode!'\n","  assert bbox.shape[-1] >= 4, 'the transformation is applied to the first 4 values of dim -1'\n","\n","  if bbox.shape[0] == 0: # corner cases\n","    return bbox\n","\n","  resized_bbox = bbox.clone()\n","  # could still work if the first dim of bbox is not batch size\n","  # in that case, w_pixel and h_pixel will be scalars\n","  resized_bbox = resized_bbox.view(bbox.shape[0], -1, bbox.shape[-1])\n","  invalid_bbox_mask = (resized_bbox == -1) # indicating invalid bbox\n","\n","  if mode == 'p2a':\n","    # pixel to activation\n","    width_ratio = w_pixel * 1. / w_amap\n","    height_ratio = h_pixel * 1. / h_amap\n","    resized_bbox[:, :, [0, 2]] /= width_ratio.view(-1, 1, 1)\n","    resized_bbox[:, :, [1, 3]] /= height_ratio.view(-1, 1, 1)\n","  else:\n","    # activation to pixel\n","    width_ratio = w_pixel * 1. / w_amap\n","    height_ratio = h_pixel * 1. / h_amap\n","    resized_bbox[:, :, [0, 2]] *= width_ratio.view(-1, 1, 1)\n","    resized_bbox[:, :, [1, 3]] *= height_ratio.view(-1, 1, 1)\n","\n","  resized_bbox.masked_fill_(invalid_bbox_mask, -1)\n","  resized_bbox.resize_as_(bbox)\n","  return resized_bbox\n","\n","# To visualize bbox image gt\n","def data_visualizer(img, idx_to_class, bbox=None, pred=None):\n","    img_copy = np.array(img).astype('uint8')\n","\n","    if bbox is not None:\n","        for bbox_idx in range(bbox.shape[0]):\n","            one_bbox = bbox[bbox_idx][:4].int()  # Ensure integer type\n","            cv2.rectangle(img_copy, (int(one_bbox[0]), int(one_bbox[1])), (int(one_bbox[2]), int(one_bbox[3])), (255, 0, 0), 2)\n","            if bbox.shape[1] > 4:  # if class info provided\n","                obj_cls = idx_to_class[bbox[bbox_idx][4].item()]\n","                cv2.putText(img_copy, '%s' % obj_cls, (int(one_bbox[0]), int(one_bbox[1])+15), cv2.FONT_HERSHEY_PLAIN, 1.0, (0, 0, 255), thickness=1)\n","\n","    if pred is not None:\n","        for bbox_idx in range(pred.shape[0]):\n","            one_bbox = pred[bbox_idx][:4].int()  # Ensure integer type\n","            cv2.rectangle(img_copy, (int(one_bbox[0]), int(one_bbox[1])), (int(one_bbox[2]), int(one_bbox[3])), (0, 255, 0), 2)\n","\n","            if pred.shape[1] > 4:  # if class and conf score info provided\n","                obj_cls = idx_to_class[pred[bbox_idx][4].item()]\n","                conf_score = pred[bbox_idx][5].item()\n","                cv2.putText(img_copy, '%s, %.2f' % (obj_cls, conf_score), (int(one_bbox[0]), int(one_bbox[1])+15), cv2.FONT_HERSHEY_PLAIN, 1.0, (0, 0, 255), thickness=1)\n","\n","    plt.imshow(img_copy)\n","    plt.axis('off')\n","    plt.show()\n","\n"],"metadata":{"id":"pV6N06Wb5H8G","executionInfo":{"status":"ok","timestamp":1712219763868,"user_tz":240,"elapsed":3,"user":{"displayName":"yanpan chung","userId":"01040412682694527349"}}},"execution_count":103,"outputs":[]},{"cell_type":"markdown","source":["Define some useful functions"],"metadata":{"id":"qTNtAbko5Nx9"}},{"cell_type":"code","source":["#helper function\n","def IoU(proposals, bboxes):\n","  iou_mat = None\n","  B, A, H, W, _ = proposals.shape\n","  proposals = proposals.reshape(B, A*H*W, 4)\n","  tl = torch.max(proposals[:, :, :2].unsqueeze(2), bboxes[:, :, :2].unsqueeze(1))\n","  br = torch.min(proposals[:, :, 2:].unsqueeze(2), bboxes[:, :, 2:4].unsqueeze(1))\n","  intersect = torch.prod(br - tl, dim=3) * (tl < br).all(dim=3)\n","  a = torch.prod(bboxes[:, :, 2:4] - bboxes[:, :, :2], dim=2)\n","  b = torch.prod(proposals[:, :, 2:] - proposals[:, :, :2], dim=2)\n","  iou_mat = torch.div(intersect, a.unsqueeze(1) + b.unsqueeze(2) - intersect)\n","\n","  return iou_mat\n","\n","\n","\n","def GenerateGrid(batch_size, w_amap=7, h_amap=7, dtype=torch.float32, device='cuda'):\n","  \"\"\"\n","  Return a grid cell given a batch size (center coordinates).\n","\n","  Inputs:\n","  - batch_size, B\n","  - w_amap: or W', width of the activation map (number of grids in the horizontal dimension)\n","  - h_amap: or H', height of the activation map (number of grids in the vertical dimension)\n","  - W' and H' are always 7 in our case while w and h might vary.\n","\n","  Outputs:\n","  grid: A float32 tensor of shape (B, H', W', 2) giving the (x, y) coordinates\n","        of the centers of each feature for a feature map of shape (B, D, H', W')\n","  \"\"\"\n","  w_range = torch.arange(0, w_amap, dtype=dtype, device=device) + 0.5\n","  h_range = torch.arange(0, h_amap, dtype=dtype, device=device) + 0.5\n","\n","  w_grid_idx = w_range.unsqueeze(0).repeat(h_amap, 1)\n","  h_grid_idx = h_range.unsqueeze(1).repeat(1, w_amap)\n","  grid = torch.stack([w_grid_idx, h_grid_idx], dim=-1)\n","  grid = grid.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n","\n","  return grid\n","\n","\n","\n","def GenerateAnchor(anc, grid):\n","    anchors = None\n","    B, H, W, _ = grid.shape\n","    A, _ = anc.shape\n","    anchors = torch.zeros((B, A, H, W, 4), device = grid.device, dtype = grid.dtype)\n","    for a in range(A):\n","      anchors[:,a,:,:,0] = grid[:,:,:,0] - anc[a,0]/2\n","      anchors[:,a,:,:,1] = grid[:,:,:,1] - anc[a,1]/2\n","      anchors[:,a,:,:,2] = grid[:,:,:,0] + anc[a,0]/2\n","      anchors[:,a,:,:,3] = grid[:,:,:,1] + anc[a,1]/2\n","\n","    return anchors\n","\n","\n","def ReferenceOnActivatedAnchors(anchors, bboxes, grid, iou_mat, pos_thresh=0.7, neg_thresh=0.3):\n","    B, A, h_amap, w_amap, _ = anchors.shape\n","    N = bboxes.shape[1]\n","\n","    # activated/positive anchors\n","    max_iou_per_anc, max_iou_per_anc_ind = iou_mat.max(dim=-1)\n","    max_iou_per_box = iou_mat.max(dim=1, keepdim=True)[0]\n","    activated_anc_mask = (iou_mat == max_iou_per_box) & (max_iou_per_box > 0)\n","    activated_anc_mask |= (iou_mat > pos_thresh)\n","    activated_anc_mask = activated_anc_mask.max(dim=-1)[0] # Bx(AxH’xW’)\n","    activated_anc_ind = torch.nonzero(activated_anc_mask.view(-1)).squeeze(-1)\n","\n","    GT_conf_scores = max_iou_per_anc[activated_anc_mask] # M\n","    box_cls = bboxes[:, :, 4].view(B, 1, N).expand((B, A*h_amap*w_amap, N))\n","    GT_class = torch.gather(box_cls, -1, max_iou_per_anc_ind.unsqueeze(-1)).squeeze(-1) # M\n","    GT_class = GT_class[activated_anc_mask].long()\n","\n","    bboxes_expand = bboxes[:, :, :4].view(B, 1, N, 4).expand((B, A*h_amap*w_amap, N, 4))\n","    bboxes = torch.gather(bboxes_expand, -2, max_iou_per_anc_ind.unsqueeze(-1) \\\n","      .unsqueeze(-1).expand(B, A*h_amap*w_amap, 1, 4)).view(-1, 4)\n","    bboxes = bboxes[activated_anc_ind]\n","\n","    activated_anc_coord = anchors.view(-1, 4)[activated_anc_ind]\n","\n","\n","    wh_offsets = torch.log((bboxes[:, 2:4] - bboxes[:, :2]) \\\n","      / (activated_anc_coord[:, 2:4] - activated_anc_coord[:, :2]))\n","\n","    xy_offsets = (bboxes[:, :2] + bboxes[:, 2:4] - \\\n","      activated_anc_coord[:, :2] - activated_anc_coord[:, 2:4]) / 2.\n","\n","    xy_offsets /= (activated_anc_coord[:, 2:4] - activated_anc_coord[:, :2])\n","\n","    GT_offsets = torch.cat((xy_offsets, wh_offsets), dim=-1)\n","\n","    # negative anchors\n","    negative_anc_mask = (max_iou_per_anc < neg_thresh) # Bx(AxH’xW’)\n","    negative_anc_ind = torch.nonzero(negative_anc_mask.view(-1)).squeeze(-1)\n","    negative_anc_ind = negative_anc_ind[torch.randint(0, negative_anc_ind.shape[0], (activated_anc_ind.shape[0],))]\n","    negative_anc_coord = anchors.view(-1, 4)[negative_anc_ind.view(-1)]\n","\n","    return activated_anc_ind, negative_anc_ind, GT_conf_scores, GT_offsets,GT_class, \\\n","          activated_anc_coord, negative_anc_coord\n","def GenerateProposal(anchors, offsets):\n","  proposals = None\n","  proposals = torch.zeros_like(anchors)\n","  anc_trans = torch.zeros_like(anchors)\n","\n","  anc_trans[:, :, :, :, 2:] = (anchors[:, :, :, :, 2:] - anchors[:, :, :, :, :2]) # w, h = br - tl\n","  anc_trans[:, :, :, :, :2] = (anchors[:, :, :, :, 2:] + anchors[:, :, :, :, :2]) / 2 # (br + tl) / 2\n","  new_anc_trans = anc_trans.clone() # avoid inplace operation\n","  new_anc_trans[:, :, :, :, :2] = anc_trans[:, :, :, :, :2] + offsets[:, :, :, :, :2] * anc_trans[:, :, :, :, 2:]\n","  new_anc_trans[:, :, :, :, 2:] = torch.mul(anc_trans[:, :, :, :, 2:], torch.exp(offsets[:, :, :, :, 2:]))\n","\n","  # tansform back\n","  proposals[:, :, :, :, :2] =  new_anc_trans[:, :, :, :, :2] - (new_anc_trans[:, :, :, :, 2:] / 2)\n","  proposals[:, :, :, :, 2:] =  new_anc_trans[:, :, :, :, :2] + (new_anc_trans[:, :, :, :, 2:] / 2)\n","  # print(\"From 1\")\n","  return proposals\n","\n","\n"],"metadata":{"id":"em6LFMGd5MXh","executionInfo":{"status":"ok","timestamp":1712219764003,"user_tz":240,"elapsed":138,"user":{"displayName":"yanpan chung","userId":"01040412682694527349"}}},"execution_count":104,"outputs":[]},{"cell_type":"markdown","source":["Two loss function for the model: ConfScoreRegression is used for classification loss function and BboxRegression is used for RPN loss function"],"metadata":{"id":"EH1cvc5w8jop"}},{"cell_type":"code","source":["def ConfScoreRegression(conf_scores, batch_size):\n","  \"\"\"\n","  Binary cross-entropy loss\n","\n","  Inputs:\n","  - conf_scores: Predicted confidence scores, of shape (2M, 2). Assume that the\n","    first M are positive samples, and the last M are negative samples.\n","\n","  Outputs:\n","  - conf_score_loss: Torch scalar\n","  \"\"\"\n","  # the target conf_scores for positive samples are ones and negative are zeros\n","  M = conf_scores.shape[0] // 2\n","  GT_conf_scores = torch.zeros_like(conf_scores)\n","  GT_conf_scores[:M, 0] = 1.\n","  GT_conf_scores[M:, 1] = 1.\n","\n","  conf_score_loss = F.binary_cross_entropy_with_logits(conf_scores, GT_conf_scores, \\\n","                                     reduction='sum') * 1. / batch_size\n","  return conf_score_loss\n","def BboxRegression(offsets, GT_offsets, batch_size):\n","  \"\"\"\"\n","  Use SmoothL1 loss as in Faster R-CNN\n","\n","  Inputs:\n","  - offsets: Predicted box offsets, of shape (M, 4)\n","  - GT_offsets: GT box offsets, of shape (M, 4)\n","\n","  Outputs:\n","  - bbox_reg_loss: Torch scalar\n","  \"\"\"\n","  bbox_reg_loss = F.smooth_l1_loss(offsets, GT_offsets, reduction='sum') * 1. / batch_size\n","  return bbox_reg_loss"],"metadata":{"id":"NQyNCHxI8SQE","executionInfo":{"status":"ok","timestamp":1712219764004,"user_tz":240,"elapsed":5,"user":{"displayName":"yanpan chung","userId":"01040412682694527349"}}},"execution_count":105,"outputs":[]},{"cell_type":"markdown","source":["We utilized the VGG16 model as a backbone model for feature extraction in our project. Specifically, we constructed a feature extractor tailored to the pretrained VGG16 model, restricting the layers to include only those leading up to and encompassing the activation of conv5_3."],"metadata":{"id":"LQnTGbgB8-sq"}},{"cell_type":"code","source":["# Transfer learning to extract features using pre-trained VGG16 model\n","class VGG16FeatureExtractor(nn.Module):\n","  \"\"\"\n","  Image feature extraction with MobileNet.\n","  \"\"\"\n","  def __init__(self, reshape_size=224, pooling=False, verbose=False):\n","    super().__init__()\n","\n","    self.vgg16 = models.vgg16(pretrained=True)\n","    # output of conv5_3 of vgg16 is N x 512 x 14 x 14\n","    self.vgg16 = nn.Sequential(*list(self.vgg16.features)[:30]) # layers up to and including the activation of conv5_3\n","\n","    # adding a conv layer to make the output N x 1280 x 7 x 7\n","    self.conv = nn.Conv2d(in_channels=512, out_channels=1280, kernel_size=3, stride=2, padding=1)\n","\n","    self.vgg16.add_module('LastConv', self.conv) # oitput is N x 1280 x 7 x 7\n","\n","    # average pooling\n","    if pooling:\n","      self.vgg16.add_module('LastAvgPool', nn.AvgPool2d(math.ceil(reshape_size/32.))) # input: N x 512 x 14 x 14\n","\n","    for i in self.vgg16.named_parameters():\n","      i[1].requires_grad = True # fine-tune all\n","\n","    if verbose:\n","      summary(self.vgg16.cuda(), (3, reshape_size, reshape_size))\n","\n","  def forward(self, img, verbose=False):\n","    \"\"\"\n","    Inputs:\n","    - img: Batch of resized images, of shape Nx3x224x224\n","\n","    Outputs:\n","    - feat: Image feature, of shape Nx1280 (pooled) or Nx1280x7x7\n","    \"\"\"\n","    num_img = img.shape[0]\n","\n","    img_prepro = img\n","\n","    feat = []\n","    process_batch = 500\n","    for b in range(math.ceil(num_img/process_batch)):\n","      feat.append(self.vgg16(img_prepro[b*process_batch:(b+1)*process_batch]\n","                              ).squeeze(-1).squeeze(-1)) # forward and squeeze\n","    feat = torch.cat(feat)\n","    if verbose:\n","      print('Output feature shape: ', feat.shape)\n","    return feat"],"metadata":{"id":"c9DXYNcm9O_7","executionInfo":{"status":"ok","timestamp":1712219764004,"user_tz":240,"elapsed":4,"user":{"displayName":"yanpan chung","userId":"01040412682694527349"}}},"execution_count":106,"outputs":[]},{"cell_type":"code","source":["def nms(boxes, scores, iou_threshold=0.5, topk=None):\n","  \"\"\"\n","  Non-maximum suppression removes overlapping bounding boxes.\n","\n","  Inputs:\n","  - boxes: top-left and bottom-right coordinate values of the bounding boxes\n","    to perform NMS on, of shape Nx4\n","  - scores: scores for each one of the boxes, of shape N\n","  - iou_threshold: discards all overlapping boxes with IoU > iou_threshold; float\n","  - topk: If this is not None, then return only the topk highest-scoring boxes.\n","    Otherwise if this is None, then return all boxes that pass NMS.\n","\n","  Outputs:\n","  - keep: torch.long tensor with the indices of the elements that have been\n","    kept by NMS, sorted in decreasing order of scores; of shape [num_kept_boxes]\n","  \"\"\"\n","\n","  if (not boxes.numel()) or (not scores.numel()):\n","    return torch.zeros(0, dtype=torch.long)\n","\n","  keep = None\n","\n","  keep = []\n","  # print(keep.dtype)\n","  indexing = torch.argsort(scores, descending=True)\n","  boxes_sort = boxes[indexing, :]\n","  # print(boxes_sort)\n","  areas = torch.prod(boxes[:, 2:] - boxes[:, :2], dim=1)\n","  # print(areas.shape)\n","  while indexing.size()[0] > 0:\n","    # still left\n","    # print(indexing.size()[0])\n","    idx = indexing[0]\n","    max_box = boxes[idx] # current max\n","    # print(keep)\n","    # print(idx)\n","    #torch.cat((keep, idx))\n","    keep.append(idx)\n","    # compute iou:\n","    tl = torch.max(max_box[:2], boxes[indexing][:, :2]) # should broadcast\n","    # print(\"tl is\", tl)\n","    br = torch.min(max_box[2:], boxes[indexing][:, 2:])\n","    #print(torch.prod(br - tl, dim=3))\n","    intersect = torch.prod(br - tl, dim=1) * (tl < br).all(dim=1)\n","    # print(intersect.shape)\n","    a = areas[idx] # (1, )\n","    b = areas #(N, 1)\n","\n","    iou_mat = torch.div(intersect, a + b[indexing] - intersect).squeeze() #(N, )\n","    # print(iou_mat)\n","    left = torch.where(iou_mat <= iou_threshold)\n","    indexing = indexing[left]\n","    # print(indexing.shape)\n","    # print(left)\n","  if topk is None:\n","    pass\n","  else:\n","    keep = keep[:topk]\n","  keep = torch.tensor(keep, **to_long).to(scores.device)\n","  return keep"],"metadata":{"id":"jqwziH53-Yc0","executionInfo":{"status":"ok","timestamp":1712219764004,"user_tz":240,"elapsed":4,"user":{"displayName":"yanpan chung","userId":"01040412682694527349"}}},"execution_count":107,"outputs":[]},{"cell_type":"markdown","source":["\n","###Region Proposal Network (RPN)\n","---\n","#### Activated (positive) and negative anchors\n","When training the RPN, we compare the anchor boxes with the ground-truth boxes in order to determine a ground-truth label for the anchor boxes -- should each anchor predict object or background?\n","\n","We assign a positive label to two kinds of anchors:\n","\n","(i) the anchor/anchors with the highest Intersection-overUnion (IoU) overlap with a ground-truth box, or\n","\n","(ii) an anchor that has an IoU overlap higher than 0.7 with any ground-truth box. Note that a single ground-truth box may assign positive labels to multiple anchors.\n","\n","Usually the second condition is sufficient to determine the positive samples; but we still adopt the first condition for the reason that in some rare cases the second condition may find no positive sample.\n","\n","We assign a negative label to a non-positive anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes. Anchors that are neither positive nor negative do not contribute to the training objective\n"],"metadata":{"id":"Zm_Nfx1r5Uin"}},{"cell_type":"code","source":["class ProposalModule(nn.Module):\n","  def __init__(self, in_dim, hidden_dim=256, num_anchors=9, drop_ratio=0.3):\n","    super().__init__()\n","\n","    assert(num_anchors != 0)\n","    self.num_anchors = num_anchors\n","\n","    self.predictHead = nn.Sequential(\n","          nn.Conv2d(in_dim,hidden_dim,3,padding=1),\n","          nn.Dropout(drop_ratio),\n","          nn.LeakyReLU(),\n","          nn.Conv2d(hidden_dim,6*self.num_anchors,1)\n","        )\n","\n","  def _extract_anchor_data(self, anchor_data, anchor_idx):\n","    \"\"\"\n","    Inputs:\n","    - anchor_data: Tensor of shape (B, A, D, H, W) giving a vector of length\n","      D for each of A anchors at each point in an H x W grid.\n","    - anchor_idx: int64 Tensor of shape (M,) giving anchor indices to extract\n","\n","    Returns:\n","    - extracted_anchors: Tensor of shape (M, D) giving anchor data for each\n","      of the anchors specified by anchor_idx.\n","    \"\"\"\n","    B, A, D, H, W = anchor_data.shape\n","    anchor_data = anchor_data.permute(0, 1, 3, 4, 2).contiguous().view(-1, D) #make sure the anchor data is contiguous\n","    extracted_anchors = anchor_data[anchor_idx]\n","    return extracted_anchors\n","\n","  def forward(self, features, pos_anchor_coord=None, \\\n","              pos_anchor_idx=None, neg_anchor_idx=None):\n","\n","    if pos_anchor_coord is None or pos_anchor_idx is None or neg_anchor_idx is None:\n","      mode = 'eval'\n","    else:\n","      mode = 'train'\n","    conf_scores, offsets, proposals = None, None, None\n","\n","    anchor_features=self.predictHead(features) #Bx(Ax6)x7x7\n","    # split features into conf_package and offsets_package\n","    B,_,H,W = anchor_features.shape\n","    anchor_features = anchor_features.reshape(B,self.num_anchors,6,H,W)\n","    conf_package = anchor_features[:,:,:2,:,:]\n","    offsets_package = anchor_features[:,:,2:,:,:]\n","\n","    if mode == 'eval':\n","      conf_scores, offsets = conf_package, offsets_package\n","    else:\n","      # train mode\n","      extracted_conf_package_pos = self._extract_anchor_data(conf_package,pos_anchor_idx)\n","      extracted_conf_package_neg = self._extract_anchor_data(conf_package,neg_anchor_idx)\n","      conf_scores = torch.cat((extracted_conf_package_pos,extracted_conf_package_neg), dim=0)[:,0:2]\n","\n","      offsets = self._extract_anchor_data(offsets_package,pos_anchor_idx)\n","\n","      M,_=pos_anchor_coord.shape\n","      proposals = GenerateProposal(pos_anchor_coord.reshape(1,1,1,M,4),\n","                                   offsets.reshape(1,1,1,M,4)).reshape(M,4)\n","    if mode == 'train':\n","      return conf_scores, offsets, proposals\n","    elif mode == 'eval':\n","      return conf_scores, offsets"],"metadata":{"id":"t0t_a07a8TlP","executionInfo":{"status":"ok","timestamp":1712219764004,"user_tz":240,"elapsed":4,"user":{"displayName":"yanpan chung","userId":"01040412682694527349"}}},"execution_count":108,"outputs":[]},{"cell_type":"code","source":["class RPN(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    # READ ONLY\n","    self.anchor_list = torch.tensor([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [2, 3], [3, 2], [3, 5], [5, 3]])\n","    #self.feat_extractor = FeatureExtractor()\n","    self.feat_extractor = VGG16FeatureExtractor()\n","    self.prop_module = ProposalModule(1280, num_anchors=self.anchor_list.shape[0]) #input size for mobile net 1280\n","\n","  def forward(self, images, bboxes):\n","\n","    # weights to multiply to each loss term\n","\n","    w_conf = 1 # for conf_scores\n","    w_reg = 5 # for offsets\n","    total_loss = None\n","    conf_scores, proposals, features, GT_class, pos_anchor_idx, anc_per_img = \\\n","      None, None, None, None, None, None\n","    # i) Image feature extraction\n","    features = self.feat_extractor(images)\n","\n","    # ii) Grid and anchor generation\n","    batch_size = images.shape[0]\n","    grid = GenerateGrid(batch_size)\n","    # anchors = GenerateAnchor(self.anchor_list.cuda(), grid)\n","    anchors = GenerateAnchor(self.anchor_list.to(grid.device, grid.dtype), grid) # why this affects the # of proposals\n","\n","    # iii-1) Compute IoU between anchors and GT boxes\n","    iou_mat = IoU(anchors, bboxes)\n","    # iii-2) determine activated & negative anchors, and GT_offsets, GT_class\n","    pos_anchor_idx, negative_anc_ind, _, GT_offsets,GT_class,activated_anc_coord,_ = \\\n","      ReferenceOnActivatedAnchors(anchors, bboxes, grid, iou_mat)\n","\n","    # iv) Compute conf_scores, offsets, proposals through the prediction network\n","    conf_scores, offsets, proposals = self.prop_module(features,activated_anc_coord,\n","                                                       pos_anchor_idx, negative_anc_ind)\n","    anc_per_img = torch.prod(torch.tensor(anchors.shape[1:-1]))\n","\n","    # v) Compute total loss\n","    conf_loss = ConfScoreRegression(conf_scores, features.shape[0]) # conf_loss\n","    # print(conf_scores)\n","    reg_loss = BboxRegression(offsets, GT_offsets, features.shape[0]) # reg_loss\n","    total_loss = w_conf * conf_loss + w_reg * reg_loss\n","\n","    return total_loss, conf_scores, proposals, features, GT_class, pos_anchor_idx, anc_per_img ## updated by Pacifique\n","\n","  def load_weights(self, weights_file):\n","      # Load trained weights from the file\n","      trained_weights = torch.load(weights_file)\n","\n","      # Extract the model's state dictionary from the loaded state dictionary\n","      model_state_dict = trained_weights['model_state_dict']\n","\n","      # Print keys for the extracted model's state_dict and the model's state_dict\n","      # print(\"Keys in extracted model's state_dict:\")\n","      # print(model_state_dict.keys())\n","      # print(\"\\nKeys in model's state_dict:\")\n","      # print(self.state_dict().keys())\n","\n","      # Check if the extracted model's state_dict matches the model's architecture\n","      if set(model_state_dict.keys()) != set(self.state_dict().keys()):\n","          print(\"\\nError: Model architecture mismatch. Loaded weights don't match model's architecture.\")\n","          return\n","\n","      # Assign the trained weights to the model parameters\n","      self.load_state_dict(model_state_dict)\n","\n","  def inference(self, images, thresh=0.5, nms_thresh=0.7, mode='RPN'):\n","\n","    assert mode in ('RPN', 'FasterRCNN'), 'invalid inference mode!'\n","\n","    features, final_conf_probs, final_proposals = None, None, None\n","\n","    # Here we predict the RPN proposal coordinates `final_proposals` and        #\n","    # confidence scores `final_conf_probs`.                                     #\n","    # The overall steps are similar to the forward pass but now you do not need  #\n","    # to decide the activated nor negative anchors.                              #\n","    # Threshold the conf_scores based on the threshold value `thresh`.     #\n","    # Then, apply NMS to the filtered proposals given the threshold `nms_thresh`.#\n","\n","\n","    final_conf_probs, final_proposals = [],[]\n","    # i) Image feature extraction\n","    features = self.feat_extractor(images)\n","\n","    # ii) Grid and anchor generation\n","    batch_size = images.shape[0]\n","    grid = GenerateGrid(batch_size)\n","    # anchors = GenerateAnchor(self.anchor_list.cuda(), grid)\n","    anchors = GenerateAnchor(self.anchor_list.to(grid.device, grid.dtype), grid)\n","\n","    # iii) Compute conf_scores, proposals, class_prob through the prediction network\n","    conf_scores, offsets = self.prop_module(features)\n","    #offsets: (B, A, 4, H', W')\n","    #conf_scores: (B, A, 2, H', W')\n","    B,A,_,H,W = conf_scores.shape\n","    # Need to dig out from here 2024 03 11\n","\n","    offsets = offsets.permute((0,1,3,4,2))\n","    proposals = GenerateProposal(anchors, offsets) #proposals:B,A,H,W,4\n","    # proposals is torch.Size([1, 13, 7, 7, 4])\n","    # transform\n","    conf_scores = torch.sigmoid(conf_scores[:,:,0,:,:]) # only look at the 1st confidence score which represent obj_conf\n","    conf_scores = conf_scores.permute((0,2,3,1)).reshape(batch_size,-1)\n","    proposals = proposals.permute((0,2,3,1,4)).reshape(batch_size,-1,4)\n","\n","    for i in range(batch_size):\n","      # get proposals, confidence scores for i-th image\n","      sub_conf_scores = conf_scores[i]\n","      sub_proposals = proposals[i]\n","\n","      # filter by conf_scores\n","      mask1 = sub_conf_scores > thresh\n","      sub_conf_scores = sub_conf_scores[mask1]\n","      sub_proposals = sub_proposals[mask1,:]\n","\n","      # filter by nms\n","      mask2 = nms(sub_proposals, sub_conf_scores, iou_threshold=nms_thresh)\n","      # append result\n","      final_proposals.append(sub_proposals[mask2,:])\n","      final_conf_probs.append(sub_conf_scores[mask2].unsqueeze(1))\n","\n","\n","    if mode == 'RPN':\n","      features = [torch.ones_like(i) for i in final_conf_probs] # dummy class\n","    return final_proposals, final_conf_probs, features"],"metadata":{"id":"u9XEgNqQ5bvk","executionInfo":{"status":"ok","timestamp":1712219764004,"user_tz":240,"elapsed":4,"user":{"displayName":"yanpan chung","userId":"01040412682694527349"}}},"execution_count":109,"outputs":[]},{"cell_type":"code","source":["# It is used for loading the the CoCo dataset into dataloader\n","def coco_collate_fn(batch_lst, reshape_size=224):\n","  preprocess = transforms.Compose([\n","    transforms.Resize((reshape_size, reshape_size)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n","\n","  batch_size = len(batch_lst)\n","\n","  img_batch = torch.zeros(batch_size, 3, reshape_size, reshape_size)\n","\n","  max_num_box = max(len(batch_lst[i][1]['labels']) \\\n","                    for i in range(batch_size))\n","\n","  box_batch = torch.Tensor(batch_size, max_num_box, 5).fill_(-1.)# create box_batch filled with -1 due to inconsistency of box numbers\n","  w_list = []\n","  h_list = []\n","  img_id_list = []\n","\n","  for i in range(batch_size):\n","    img, ann = batch_lst[i]\n","    w_list.append(img.size[0]) # image width\n","    h_list.append(img.size[1]) # image height\n","    img_id_list.append(ann['image_id'].item()) #Image Id\n","    img_batch[i] = preprocess(img)\n","    all_bbox = ann['boxes']\n","    for bbox_idx, one_bbox in enumerate(all_bbox):\n","      bbox = one_bbox\n","      obj_cls = idx_to_class[ann['labels'][bbox_idx].item()]\n","      box_batch[i][bbox_idx] =torch.Tensor([float(bbox[0]), float(bbox[1]),\n","            float(bbox[2]), float(bbox[3]), class_to_idx[obj_cls]])\n","  h_batch = torch.tensor(h_list)\n","  w_batch = torch.tensor(w_list)\n","\n","  return img_batch, box_batch, w_batch, h_batch, img_id_list\n"],"metadata":{"id":"WiCuK2989p6B","executionInfo":{"status":"ok","timestamp":1712219764281,"user_tz":240,"elapsed":281,"user":{"displayName":"yanpan chung","userId":"01040412682694527349"}}},"execution_count":110,"outputs":[]},{"cell_type":"markdown","source":["Two Stage Detector\n","\n"],"metadata":{"id":"5A4cVYb0-ogn"}},{"cell_type":"code","source":["class TwoStageDetector(nn.Module):\n","  def __init__(self, in_dim=1280, hidden_dim=256, num_classes=4, \\\n","               roi_output_w=2, roi_output_h=2, drop_ratio=0.3):\n","    super().__init__()\n","\n","    assert(num_classes != 0)\n","    self.num_classes = num_classes # number of classes (excluding the background)\n","    self.roi_output_w, self.roi_output_h = roi_output_w, roi_output_h\n","    self.rpn = RPN() # RPM model\n","    self.classificationLayer = nn.Sequential( # Define the classifier\n","          nn.Linear(in_dim,hidden_dim),\n","          nn.Dropout(drop_ratio),\n","          nn.ReLU(),\n","          nn.Linear(hidden_dim,self.num_classes)\n","        )\n","    self.MeanPool = nn.AvgPool2d((7,7))#added\n","  def load_weights(self, weights_file):\n","    # Load trained weights from the file\n","    trained_weights = torch.load(weights_file)\n","\n","    # Extract the model's state dictionary from the loaded state dictionary\n","    model_state_dict = trained_weights['model_state_dict']\n","\n","    # Print keys for the extracted model's state_dict and the model's state_dict\n","    # print(\"Keys in extracted model's state_dict:\")\n","    # print(model_state_dict.keys())\n","    # print(\"\\nKeys in model's state_dict:\")\n","    # print(self.state_dict().keys())\n","\n","    # Check if the extracted model's state_dict matches the model's architecture\n","    if set(model_state_dict.keys()) != set(self.state_dict().keys()):\n","        print(\"\\nError: Model architecture mismatch. Loaded weights don't match model's architecture.\")\n","        return\n","\n","    # Assign the trained weights to the model parameters\n","    self.load_state_dict(model_state_dict)\n","  def forward(self, images, bboxes):\n","    \"\"\"\n","    Training-time forward pass for our two-stage Faster R-CNN detector.\n","\n","    Inputs:\n","    - images: Tensor of shape (B, 3, H, W) giving input images\n","    - bboxes: Tensor of shape (B, N, 5) giving ground-truth bounding boxes\n","      and category labels, from the dataloader.\n","\n","    Outputs:\n","    - total_loss: Torch scalar giving the overall training loss.\n","    \"\"\"\n","    total_loss = 0\n","    # torch.cuda.synchronize()#added\n","    # run the RPN model\n","    B = images.shape[0]\n","    rpn_loss, conf_scores, proposals, features, GT_class, pos_anchor_idx, anc_per_img = \\\n","      self.rpn(images, bboxes) # proposals: M,4\n","    boxes = torch.cat((pos_anchor_idx.unsqueeze(1) // anc_per_img, proposals), dim=1)\n","    # print(features)\n","    # print(boxes)\n","    roi_output = torchvision.ops.roi_align(features, boxes,\n","                                               (self.roi_output_w, self.roi_output_h))\n","    roi_output = torch.mean(roi_output, dim=(2, 3))\n","    class_prob = self.classificationLayer.forward(roi_output)\n","    # print(B)\n","    # print(class_prob)\n","    # print(GT_class)\n","    cls_loss = nn.functional.cross_entropy(class_prob, GT_class, reduction='sum') / B\n","\n","    total_loss = rpn_loss + cls_loss\n","\n","    return total_loss\n","  def inference(self, images, thresh=0.5, nms_thresh=0.7):\n","\n","    final_proposals, final_conf_probs, final_class = None, None, None\n","\n","    final_class=[]\n","    final_proposals, final_conf_probs, features = self.rpn.inference(images, thresh,\n","                                                                     nms_thresh,mode='FasterRCNN')\n","\n","    aligned_features = torchvision.ops.roi_align(features, final_proposals,\n","                                                 (self.roi_output_w, self.roi_output_h))\n","    pooled_features = torch.mean(aligned_features,(2,3))\n","    cls_scores = self.classificationLayer(pooled_features)\n","    cls = torch.max(cls_scores,1)[1].to(torch.int64).unsqueeze(1)\n","    # slice cls into groups\n","    count = 0\n","    for i in range(len(final_proposals)):\n","      tmp_len=len(final_proposals[i])\n","      final_class.append(cls[count:count+tmp_len])\n","      count += tmp_len\n","\n","    return final_proposals, final_conf_probs, final_class"],"metadata":{"id":"e24LAeLr-o00","executionInfo":{"status":"ok","timestamp":1712219764281,"user_tz":240,"elapsed":5,"user":{"displayName":"yanpan chung","userId":"01040412682694527349"}}},"execution_count":111,"outputs":[]},{"cell_type":"markdown","source":["Detection Solver and Detection Inference"],"metadata":{"id":"lJ0wBjDy50xs"}},{"cell_type":"code","source":["# added on 2024-03-30\n","\n","def DetectionSolver(detector,train_loader, val_loader,optimizer,learning_rate=3e-3,\n","                    lr_decay=1, num_epochs=20, checkpoint_path='detector_checkpoint', patience=5):\n","\n","    detector.to(**to_float)\n","\n","    # # optimizer setup\n","    # optimizer = optim.SGD(\n","    #     filter(lambda p: p.requires_grad, detector.parameters()), learning_rate)\n","\n","    lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer,\n","                                               lambda epoch: lr_decay ** epoch)\n","\n","    loss_history = []\n","    val_loss_history = []  # Added for validation loss tracking\n","\n","    detector.train()\n","    best_val_loss = float('inf')\n","    no_improvement_counter = 0\n","    for epoch in range(num_epochs):\n","        start_time = time.time()\n","\n","        # Training phase\n","        total_loss = 0\n","        for iter_num, data_batch in enumerate(train_loader):\n","            images, boxes, w_batch, h_batch, _ = data_batch\n","            # boxes[4] is the gt label of the bbox\n","            resized_boxes = coord_trans(boxes, w_batch, h_batch, mode='p2a')\n","\n","            images = images.to(**to_float)\n","            resized_boxes = resized_boxes.to(**to_float)\n","\n","            # if detector == rpn:\n","            #     loss, _, _, _, _, _, _ = detector(images, resized_boxes) # Forward pass for RPN model\n","            # else:\n","            loss = detector(images, resized_boxes) # Forward pass for TwoStageDetector\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","\n","        avg_loss = total_loss / len(train_loader)\n","        loss_history.append(avg_loss)\n","\n","        # Validation phase\n","        total_val_loss = 0\n","        detector.eval()  # Switch to evaluation mode\n","        with torch.no_grad():\n","            for data_batch in val_loader:\n","                images, boxes, w_batch, h_batch, _ = data_batch\n","                resized_boxes = coord_trans(boxes, w_batch, h_batch, mode='p2a')\n","                images = images.to(**to_float)\n","                resized_boxes = resized_boxes.to(**to_float)\n","\n","                # if detector == rpn:\n","                #     val_loss, _, _, _, _, _, _ = detector(images, resized_boxes)\n","                # else:\n","                val_loss = detector(images, resized_boxes)\n","\n","                total_val_loss += val_loss.item()\n","\n","        avg_val_loss = total_val_loss / len(val_loader)\n","        # print(len(val_loader),len(train_loader))\n","        val_loss_history.append(avg_val_loss)\n","\n","        print('(Epoch {}/{}) Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(\n","            epoch + 1, num_epochs, avg_loss, avg_val_loss))\n","\n","        end_time = time.time()\n","        if epoch == num_epochs - 1:\n","            print('(Epoch {}/{}): Time per epoch: {:.2f}s'.format(\n","                epoch + 1, num_epochs, (end_time - start_time) / num_epochs))\n","\n","        lr_scheduler.step()\n","        if avg_val_loss < best_val_loss:\n","            best_val_loss = avg_val_loss\n","            no_improvement_counter = 0\n","        else:\n","            no_improvement_counter += 1\n","\n","        if no_improvement_counter >= patience:\n","            print(f'Early stopping at epoch {epoch + 1}')\n","            break\n","        # Format the checkpoint_path to include the epoch number\n","        # epoch_checkpoint_path = f\"{checkpoint_path}_epoch_{epoch + 1}.pth\"\n","\n","        # Save checkpoint\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': detector.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': avg_loss,\n","            'val_loss': avg_val_loss,  # Save the validation loss too\n","        }, checkpoint_path)\n","\n","    # # Plot the training and validation losses\n","    # plt.figure(figsize=(10, 5))\n","    # plt.plot(loss_history, label='Training Loss')\n","    # plt.plot(val_loss_history, label='Validation Loss')\n","    # plt.xlabel('Epoch')\n","    # plt.ylabel('Loss')\n","    # plt.title('Loss History')\n","    # plt.legend()\n","    # plt.show()\n","    return loss_history,avg_loss,val_loss_history,avg_val_loss"],"metadata":{"id":"OyVrCVYu57dJ","executionInfo":{"status":"ok","timestamp":1712219764281,"user_tz":240,"elapsed":4,"user":{"displayName":"yanpan chung","userId":"01040412682694527349"}}},"execution_count":112,"outputs":[]},{"cell_type":"code","source":["def DetectionInference(detector, data_loader, dataset, idx_to_class, thresh=0.8, nms_thresh=0.3, output_dir=None):\n","\n","  # ship model to GPU\n","  detector.to(**to_float)\n","\n","  detector.eval()\n","  start_t = time.time()\n","\n","  if output_dir is not None:\n","    det_dir = 'mAP/input/detection-results'\n","    gt_dir = 'mAP/input/ground-truth'\n","    if os.path.exists(det_dir):\n","      shutil.rmtree(det_dir)\n","    os.mkdir(det_dir)\n","    if os.path.exists(gt_dir):\n","      shutil.rmtree(gt_dir)\n","    os.mkdir(gt_dir)\n","\n","  for iter_num, data_batch in enumerate(data_loader):\n","    images, boxes, w_batch, h_batch, img_ids = data_batch\n","    images = images.to(**to_float)\n","\n","    final_proposals, final_conf_scores, final_class = detector.inference(images, thresh=thresh, nms_thresh=nms_thresh)\n","\n","    # clamp on the proposal coordinates\n","    batch_size = len(images)\n","    for idx in range(batch_size):\n","      torch.clamp_(final_proposals[idx][:, 0::2], min=0, max=w_batch[idx])\n","      torch.clamp_(final_proposals[idx][:, 1::2], min=0, max=h_batch[idx])\n","\n","      # visualization\n","      # get the original image\n","      # hack to get the original image so we don't have to load from local again...\n","      i = batch_size*iter_num + idx\n","      img, _ = dataset.__getitem__(i)\n","\n","      valid_box = sum([1 if j != -1 else 0 for j in boxes[idx][:, 0]])\n","      final_all = torch.cat((final_proposals[idx], \\\n","        final_class[idx].float(), final_conf_scores[idx]), dim=-1).cpu()\n","      resized_proposals = coord_trans(final_all, w_batch[idx], h_batch[idx])\n","\n","      # write results to file for evaluation (use mAP API https://github.com/Cartucho/mAP for now...)\n","      if output_dir is not None:\n","        file_name = img_ids[idx].replace('.jpg', '.txt')\n","        with open(os.path.join(det_dir, file_name), 'w') as f_det, \\\n","          open(os.path.join(gt_dir, file_name), 'w') as f_gt:\n","          print('{}: {} GT bboxes and {} proposals'.format(img_ids[idx], valid_box, resized_proposals.shape[0]))\n","          for b in boxes[idx][:valid_box]:\n","            f_gt.write('{} {:.2f} {:.2f} {:.2f} {:.2f}\\n'.format(idx_to_class[b[4].item()], b[0], b[1], b[2], b[3]))\n","          for b in resized_proposals:\n","            f_det.write('{} {:.6f} {:.2f} {:.2f} {:.2f} {:.2f}\\n'.format(idx_to_class[b[4].item()], b[5], b[0], b[1], b[2], b[3]))\n","      else:\n","        data_visualizer(img, idx_to_class, boxes[idx][:valid_box], resized_proposals)\n","\n","  end_t = time.time()\n","  print('Total inference time: {:.1f}s'.format(end_t-start_t))"],"metadata":{"id":"llm543Bt57aL","executionInfo":{"status":"ok","timestamp":1712219764281,"user_tz":240,"elapsed":4,"user":{"displayName":"yanpan chung","userId":"01040412682694527349"}}},"execution_count":113,"outputs":[]},{"cell_type":"markdown","source":["###Two Stage detector\n","We train the two stage detector since we found it is easier to implement. There are multiple ways to train Faster RCNN models. We used the approximate joint training which is simply training the RPN and the classifier in one box in the two stage detector. There are another way mentioned in Ren, S. et al. (2016) which used a 4-step training method which has higher accuracy but much slower than approximate joint training."],"metadata":{"id":"PhP2Tgou6O8w"}},{"cell_type":"code","source":["train_loader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size=32, shuffle=False, num_workers=0,\n","    collate_fn=coco_collate_fn)"],"metadata":{"id":"8N0Zv6YfCvTE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Simple training of the two stage detector\n","Solver = DetectionSolver\n","for lr in [1e-4]:\n","  print('lr: ', lr)\n","  detector = TwoStageDetector()\n","  Solver(detector, train_loader, learning_rate=lr, num_epochs=200,checkpoint_path='twostage.pth')\n"],"metadata":{"id":"CeP-Zriw6OeK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Calculating the mAP of the models which is one of our evaluation metrics."],"metadata":{"id":"XDZCif92DSpi"}},{"cell_type":"code","source":["from torchvision.ops import box_iou\n","\n","def Calculate_mAP(detector, data_loader, dataset, idx_to_class, thresh=0.8, nms_thresh=0.3, iou_threshold=0.5, output_dir=None):\n","    # # ship model to GPU\n","    detector.to(**to_float)\n","\n","    detector.eval()\n","    start_t = time.time()\n","\n","    iou_sum, total_iou_count = 0.0, 0\n","    precisions = []\n","\n","    for iter_num, data_batch in enumerate(data_loader):\n","        images, boxes, w_batch, h_batch, img_ids = data_batch\n","        images = images.to(**to_float)\n","\n","        final_proposals, final_conf_scores, final_class = detector.inference(images, thresh=thresh, nms_thresh=nms_thresh)\n","\n","        batch_size = len(images)\n","        for idx in range(batch_size):\n","            torch.clamp_(final_proposals[idx][:, 0::2], min=0, max=w_batch[idx])\n","            torch.clamp_(final_proposals[idx][:, 1::2], min=0, max=h_batch[idx])\n","\n","            final_all = torch.cat((final_proposals[idx], final_class[idx].float(), final_conf_scores[idx]), dim=-1).detach().clone().cpu()\n","            resized_proposals = coord_trans(final_all, w_batch[idx], h_batch[idx])\n","\n","            # Norman added to delete -1 in tensor boxes\n","            # Boolean mask to find rows with all -1 values\n","            mask = torch.all(boxes[idx,:,:4] == -1, dim=1)\n","            filter_boxes= boxes[idx,:,:4][~mask]\n","\n","            iou = box_iou(resized_proposals[:, :4], filter_boxes)\n","\n","            iou_sum += iou.sum().item()  # Sum IoU values\n","\n","            total_iou_count += iou.numel()  # Count total IoU values for mean calculation\n","            # calculate precision\n","            max_iou, max_indices = iou.max(dim=1)\n","            true_positives = max_iou >= iou_threshold\n","            num_true_positives = true_positives.sum().item()\n","            num_false_positives = (true_positives == False).sum().item()\n","            precision = num_true_positives / (num_true_positives + num_false_positives + 1e-6)\n","            precisions.append(precision)\n","\n","    # Calculate mean IoU and mAP\n","    mean_iou = iou_sum / total_iou_count if total_iou_count > 0 else 0\n","    mAP = sum(precisions) / len(precisions) if precisions else 0\n","\n","    print(f\"Mean IOU: {mean_iou:.4f}, mAP: {mAP:.4f}\")\n","    return mAP"],"metadata":{"id":"Ek3P4iwLDQa5","executionInfo":{"status":"ok","timestamp":1712219776578,"user_tz":240,"elapsed":249,"user":{"displayName":"yanpan chung","userId":"01040412682694527349"}}},"execution_count":114,"outputs":[]},{"cell_type":"markdown","source":["##Hyperparameter Tuning"],"metadata":{"id":"fYUDPOcwENJ5"}},{"cell_type":"code","source":["import itertools\n","import matplotlib.pyplot as plt\n","\n","# Define the hyperparameter ranges\n","learning_rates = [1e-4]\n","batch_sizes = [64,128]\n","num_epochs_list = [200]\n","momentum_values = [0.9, 0.95]\n","weight_decay_values = [0.0, 1e-3]\n","\n","best_hyperparameters = {}\n","best_validation_loss = float('inf')\n","train_losses = {}\n","valid_losses={}\n","for lr, momentum, weight_decay, batch_size, num_epochs in itertools.product(learning_rates, momentum_values, weight_decay_values, batch_sizes, num_epochs_list):\n","    print(f'Training Begins:\\n lr is {lr}, momentum is {momentum}, weight decay is {weight_decay}, bs is {batch_size}, ep is {num_epochs}.')\n","    train_loader = torch.utils.data.DataLoader(\n","        train_dataset, batch_size=batch_size, shuffle=False, num_workers=0,\n","        collate_fn=coco_collate_fn)\n","    val_loader = torch.utils.data.DataLoader(\n","        val_dataset, batch_size=32, shuffle=False, num_workers=0,\n","        collate_fn=coco_collate_fn)\n","    detector = TwoStageDetector()\n","    # Call DetectionSolver directly and capture the validation loss\n","    optimizer = optim.SGD(\n","        filter(lambda p: p.requires_grad, detector.parameters()),\n","        lr=lr, momentum=momentum, weight_decay=weight_decay)\n","\n","    # Call DetectionSolver directly and capture the validation loss\n","    train_history,train_loss,val_history,validation_loss = DetectionSolver(detector, train_loader, val_loader, optimizer, num_epochs=num_epochs, checkpoint_path=f'classifier_checkpoint_lr{lr}_mom{momentum}_wd{weight_decay}_bs{batch_size}_ep{num_epochs}.pth'\n","                                                                           ,patience=10)\n","\n","    train_losses[(lr, momentum, weight_decay, batch_size, num_epochs)] = train_history\n","    valid_losses[(lr, momentum, weight_decay, batch_size, num_epochs)] = val_history\n","    if validation_loss < best_validation_loss:\n","        best_hyperparameters = {\n","            'learning_rate': lr,\n","            'momentum': momentum,\n","            'weight_decay': weight_decay,\n","            'batch_size': batch_size,\n","            'num_epochs': num_epochs,\n","        }\n","        best_validation_loss = validation_loss\n","    torch.cuda.empty_cache()  # Release GPU memory\n","\n","plt.figure(figsize=(10, 6))\n","for hyperparams, loss in  train_losses.items():\n","    lr, momentum, weight_decay, batch_size, num_epochs = hyperparams\n","    # Plotting a single point for each hyperparameter combination\n","    plt.plot(loss, label=f\"Train LR={lr}, Mom={momentum}, WD={weight_decay}, BS={batch_size}, Epochs={num_epochs}\")\n","for hyperparams, loss in  valid_losses.items():\n","    lr, momentum, weight_decay, batch_size, num_epochs = hyperparams\n","    # Plotting a single point for each hyperparameter combination\n","    plt.plot(loss, label=f\"Valid LR={lr}, Mom={momentum}, WD={weight_decay}, BS={batch_size}, Epochs={num_epochs}\")\n","plt.xlabel('Epoch')\n","plt.ylabel('Training Loss')\n","plt.title('Training Curve for Different Hyperparameter Combinations')\n","plt.legend(loc='upper right', fontsize=5)\n","plt.grid(True)\n","plt.show()\n","\n","print(\"Best hyperparameters:\", best_hyperparameters)\n","print(\"The loss of Best hyperparameters:\", best_validation_loss)\n","\n"],"metadata":{"id":"OVipFAWeERZ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10, 6))\n","for hyperparams, loss in  train_losses.items():\n","    lr, momentum, weight_decay, batch_size, num_epochs = hyperparams[0]\n","    # Plotting a single point for each hyperparameter combination\n","    plt.plot(loss, label=f\"Train LR={lr}, Mom={momentum}, WD={weight_decay}, BS={batch_size}, Epochs={num_epochs}\")\n","for hyperparams, loss in  valid_losses.items():\n","    lr, momentum, weight_decay, batch_size, num_epochs = hyperparams[0]\n","    # Plotting a single point for each hyperparameter combination\n","    plt.plot(loss, label=f\"Valid LR={lr}, Mom={momentum}, WD={weight_decay}, BS={batch_size}, Epochs={num_epochs}\")\n","plt.xlabel('Epoch')\n","plt.ylabel('Training Loss')\n","plt.title('Training Curve for Different Hyperparameter Combinations')\n","plt.legend(loc='upper right', fontsize=5)\n","plt.grid(True)\n","plt.show()\n","\n","print(\"Best hyperparameters:\", best_hyperparameters)\n","print(\"The loss of Best hyperparameters:\", best_validation_loss)"],"metadata":{"id":"3uyWCrLkMdk3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["![picture](https://drive.google.com/uc?id=1tf4cXh3zg31GAtL0xAObxguCR4DPk4tx)"],"metadata":{"id":"pCA4EL41NpxC"}},{"cell_type":"markdown","source":["Best hyperparameters: {'learning_rate': 0.0001, 'momentum': 0.95, 'weight_decay': 0.001, 'batch_size': 64, 'num_epochs': 200}"],"metadata":{"id":"K2habWOCPV7O"}},{"cell_type":"markdown","source":["![picture](https://drive.google.com/uc?id=17dYcR_RkC1PTyzrpwWvWCC276MOGnPDB)\n"],"metadata":{"id":"GutM32dZYExU"}},{"cell_type":"markdown","source":["###Tuning the optimal Threshold parameters for the best tuned model"],"metadata":{"id":"9eR5s51bRkKw"}},{"cell_type":"code","source":["import itertools\n","import pandas as pd\n","\n","# Initialize an empty list to store results\n","results = []\n","\n","# mAP and mean IOU for train set\n","detector = TwoStageDetector()\n","detector.load_weights('/twostage.pth')\n","\n","# Define the ranges for hyperparameters\n","thresh_range = [0.3, 0.4, 0.5, 0.7, 0.8, 0.9]\n","nms_thresh_range = [0.3, 0.4, 0.5, 0.7, 0.8, 0.9]\n","iou_thresh_range = [0.5, 0.6]\n","\n","# Perform grid search over the hyperparameter space\n","for thresh, nms_thresh, iou_thresh in itertools.product(thresh_range, nms_thresh_range, iou_thresh_range):\n","    mAP = Calculate_mAP(detector, val_loader, val_dataset, idx_to_class, thresh, nms_thresh, iou_thresh)\n","    results.append({'thresh': thresh, 'nms_thresh': nms_thresh, 'iou_threshold': iou_thresh, 'mAP': mAP})\n","\n","# Create a DataFrame from the results list\n","df = pd.DataFrame(results)\n","\n","# Display the DataFrame\n","print(df)"],"metadata":{"id":"ifaElxN_DrpY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Best mAP: 0.3763330464558055\n","Best hyperparameters: {'thresh': 0.9, 'nms_thresh': 0.3, 'iou_threshold': 0.5}"],"metadata":{"id":"zJl7nJdDUXIn"}},{"cell_type":"markdown","source":["###Inference\n","Inference with validation data"],"metadata":{"id":"y5aK3QZa_2Yd"}},{"cell_type":"code","source":["# Inference with Test set\n","# Green box is prediction, Red box is Ground truth\n","\n","RCNNInference = DetectionInference\n","with torch.no_grad():\n","  RCNNInference(detector, val_loader, val_dataset, idx_to_class, thresh=0.9, nms_thresh=0.3)"],"metadata":{"id":"tc3cDYUr6b_O","executionInfo":{"status":"error","timestamp":1712221409641,"user_tz":240,"elapsed":8654,"user":{"displayName":"yanpan chung","userId":"01040412682694527349"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1e2dMlXlO0WvOFBvdKXR6AVPTxZPCbs4g"},"outputId":"6802f10e-c66a-4aa9-f3f4-1022329eb0d0"},"execution_count":131,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["###Real time camera detection\n","Testing the model with real-time video at 8 FPS\n","\n","![picture](https://drive.google.com/uc?id=1FDDkTTakg7eqIb3OeXfPRyge5ZKW4qbe)\n","![picture](https://drive.google.com/uc?id=10EwlqzFQfVQq73KVaux2QleL-p-kWjCA)"],"metadata":{"id":"1XZ8CchvWbRZ"}},{"cell_type":"markdown","source":["# **Part 2: Automatic Speech Recognition (Speech to Text Pipeline)**"],"metadata":{"id":"ZriNF-js7mGt"}},{"cell_type":"markdown","source":["The second part of our pipeline focuses on training a speech to text model so that we tell our object detection model which object to look for through our voice."],"metadata":{"id":"MNn4N_MigJsx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ItPp27ypflVS"},"outputs":[],"source":["import torch, torchaudio\n","import torch.nn as nn\n","from torch.nn import functional as F\n","import torch.optim as optim\n","import wandb\n","import time\n","from torchsummary import summary\n","from torchmetrics.text import WordErrorRate, CharErrorRate\n","\n","from torch.utils.data import DataLoader, Dataset\n","from tqdm import tqdm"]},{"cell_type":"markdown","source":["The following block of code initializes and creates a Dataset with the **LibriSpeech** dataset with which we can build a DataLoader object during training. We chose the LibriSpeech dataset for the following reasons:\n","1.   The dataset is open source and is one of the largest available dataset with over 1000 hours of English speech.\n","2.   Contains high quality recordings and near-perfect transcriptions. Also is void of background noise which may disrupt the performance of the model.\n","3.   Diverse set of voices with different ages, genders and accents. Training on this dataset may generalize well to our own voices during test time.\n","\n","As a result, we didn't need to perform much data cleaning. The audio was processed by converting it to a **mel spectogram**, which is a visual representation of the frequency content of an audio signal in log scale. This scale would improve the prediction performance because it better aligns with how humans listen and hear different pitches and volumes, thus it is likely to improve the training of a neural network. Each mel spectogram is further augmented by randomly masking portions of the spectogram it in both the frequency and time direction which increases the diversity of audio data and helps prevent overfitting.\n","\n"],"metadata":{"id":"gH8wDHmanSH_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OMzXHKMDflVV"},"outputs":[],"source":["class LibriSpeechOne(Dataset):\n","    \"\"\"\n","    Creates Dataset object with one audio wave.\n","    \"\"\"\n","    def __init__(self, audio, label):\n","        self.audio = audio\n","        self.label = label\n","\n","    def __len__(self):\n","        return 1\n","\n","    def __getitem__(self, idx):\n","        return self.audio, None, self.label, None, None, None\n","\n","\n","class LibriSpeechDataset(Dataset):\n","    \"\"\"\n","    Dataset class for LibriSpeech. Performs data download, data augmentation,\n","    conversion of audio files to mel spectograms and processing of text.\n","    \"\"\"\n","    def __init__(self, dataset_type, data=None):\n","\n","        self.audio_transform = nn.Sequential(\n","            torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n","            torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n","            torchaudio.transforms.TimeMasking(time_mask_param=100)\n","        )\n","\n","        self.dataset_dir = \"/home/asblab2/sinarasi/mie1517/new_code/data\"\n","        if dataset_type == \"train\":\n","            self.dataset = torchaudio.datasets.LIBRISPEECH(self.dataset_dir, url=\"train-clean-100\", download=True)\n","        elif dataset_type == \"valid\":\n","            self.dataset = torchaudio.datasets.LIBRISPEECH(self.dataset_dir, url=\"test-clean\", download=True)\n","        elif dataset_type == \"one\":\n","            self.dataset = LibriSpeechOne(*data)\n","        else:\n","            raise Exception(\"Invalid dataset type!\")\n","\n","\n","        self.text_to_int = {\"'\": 0, \" \": 1, \"a\": 2, \"b\": 3, \"c\": 4,\n","                            \"d\": 5, \"e\": 6, \"f\": 7, \"g\": 8, \"h\": 9,\n","                            \"i\": 10, \"j\": 11, \"k\": 12, \"l\": 13, \"m\": 14,\n","                            \"n\": 15, \"o\": 16, \"p\": 17, \"q\": 18, \"r\": 19,\n","                            \"s\": 20, \"t\": 21, \"u\": 22, \"v\": 23, \"w\": 24,\n","                            \"x\": 25, \"y\": 26, \"z\": 27}\n","        self.int_to_text = {v: k for k, v in self.text_to_int.items()}\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, index):\n","        data = self.dataset[index]\n","        audio, _, sentence, _, _, _ = data\n","        spectogram = self.audio_transform(audio).squeeze(0).transpose(0, 1)\n","        label = [self.text_to_int[s] for s in sentence.lower()]\n","        spectogram_length = spectogram.shape[0] // 2\n","        label_length = len(label)\n","        return spectogram, label, spectogram_length, label_length\n","\n","\n","def collate(data):\n","    \"\"\"\n","    Pad spectograms and labels within the batch to the same length.\n","    \"\"\"\n","    spectograms, labels, spectogram_lengths, label_lengths = [], [], [], []\n","    for spectogram, label, spectogram_length, label_length in data:\n","        spectograms += [torch.Tensor(spectogram)]\n","        labels += [torch.Tensor(label)]\n","        spectogram_lengths += [spectogram_length]\n","        label_lengths += [label_length]\n","    spectograms = nn.utils.rnn.pad_sequence(spectograms, batch_first=True).transpose(1, 2)\n","    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n","\n","    return spectograms, labels, torch.tensor(spectogram_lengths), torch.tensor(label_lengths)"]},{"cell_type":"markdown","source":["The next section defines our neural network. The input to the neural network is an audio wave file in the form of a masked mel spectogram. We employ two important network modules:\n","\n","1.   **ResidualCNN**: We employ a residual CNN because we were inspired by the residual networks of ResNet. This is particularly useful for audio tasks because each subsequent layer is conditioned and on the past layer, which is important when you have a time series data.\n","2.   **Bi-directional GRU**: We employ a GRU recurrent network module because it has less parameters than an LSTM and is often said to outperform them due to their simplicity, they improve the speed of convergence and require less memory. A bi-directional GRU makes sense because the future portions of the sentence can help determine the past words of a sentence, and the past words of a sentence can also help determine the future words of a sentence.\n","\n","In total our model contains upwards of 23M parameters.\n","\n"],"metadata":{"id":"yQEen11wrVV0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hWniFMlMflVV"},"outputs":[],"source":["\n","hp = {\"batch_size\": 15,\n","      \"learning_rate\": 5e-4,\n","      \"lr_factor\": 0.75,\n","      \"lr_patience\": 10,\n","      \"epochs\": 100,\n","      \"n_mels\": 128,\n","      \"sample_rate\": 16000,\n","      \"dropout\": 0.1,\n","      \"hidden_size\": 512,\n","      \"rnn_layers\": 5,\n","      \"cnn_layers\": 3,\n","      \"architecture\": 1}\n","\n","class CNNLayerNorm(nn.Module):\n","\n","    def __init__(self, n_feats):\n","        super(CNNLayerNorm, self).__init__()\n","        self.layer_norm = nn.LayerNorm(n_feats)\n","\n","    def forward(self, x):\n","        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n","        x = self.layer_norm(x)\n","        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time)\n","\n","\n","class ResidualCNN(nn.Module):\n","\n","    def __init__(self, channels, dropout, n_feats):\n","        super(ResidualCNN, self).__init__()\n","\n","        self.layers = nn.Sequential(\n","            CNNLayerNorm(n_feats),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Conv2d(channels, channels, 3, 1, 1),\n","            CNNLayerNorm(n_feats),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Conv2d(channels, channels, 3, 1, 1)\n","        )\n","\n","    def forward(self, x):\n","        residual = x    # (batch, channel, feature, time)\n","        x = self.layers(x)\n","        x += residual\n","        return x        # (batch, channel, feature, time)\n","\n","class BiGRU(nn.Module):\n","\n","    def __init__(self, rnn_dim, hidden_size, dropout):\n","        super(BiGRU, self).__init__()\n","        self.bigru = nn.Sequential(\n","            nn.LayerNorm(rnn_dim),\n","            nn.GELU(),\n","            nn.GRU(input_size=rnn_dim, hidden_size=hidden_size, num_layers=1, batch_first=True, bidirectional=True),\n","        )\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x, _ = self.bigru(x)\n","        x = self.dropout(x)\n","        return x\n","\n","\n","class ASR(nn.Module):\n","\n","    def __init__(self, dropout, hidden_size, rnn_layers, rescnn_layers, n_mels):\n","\n","        super(ASR, self).__init__()\n","\n","        self.dropout = dropout\n","        self.n_mels = n_mels // 2\n","        self.lin_start = 128\n","        self.lin_end = 29\n","        self.hidden_size = hidden_size\n","        self.gru_layers = rnn_layers\n","        self.rescnn_layers = rescnn_layers\n","\n","        # Process Mel Spectogram via Residual Conv2D Layers\n","        self.rescnn_layers = nn.Sequential(\n","            nn.Conv2d(1, 32, 3, stride=2, padding=1),\n","            *[ResidualCNN(32, dropout=dropout, n_feats=self.n_mels) for _ in range(rescnn_layers)]\n","        )\n","\n","        # Linear layers\n","        self.fc1 = nn.Sequential(\n","            nn.LayerNorm(self.n_mels * 32),\n","            nn.GELU(),\n","            nn.Dropout(self.dropout),\n","            nn.Linear(self.n_mels * 32, self.hidden_size),\n","            nn.LayerNorm(self.hidden_size),\n","            nn.GELU(),\n","            nn.Dropout(self.dropout)\n","        )\n","\n","        # GRU architecture\n","        self.gru = nn.Sequential(*[\n","                    BiGRU(rnn_dim=hidden_size if i==0 else hidden_size*2,\n","                                    hidden_size=hidden_size, dropout=dropout)\n","                    for i in range(self.gru_layers)\n","                ])\n","\n","        # Linear Layers\n","        self.fc2 = nn.Sequential(\n","            nn.Linear(self.hidden_size * 2, self.lin_end),\n","            nn.LayerNorm(self.lin_end),\n","            nn.GELU(),\n","            nn.Dropout(self.dropout)\n","        )\n","\n","    def forward(self, x):\n","        x = x.unsqueeze(1)\n","        x = self.rescnn_layers(x)\n","        x = x.reshape(x.shape[0], x.shape[1] * x.shape[2], x.shape[3])\n","        x = x.transpose(1,2) # Since linear layers require input of shape (batch, time, channels=n_mels)\n","        x = self.fc1(x)\n","        x = self.gru(x)\n","        x = self.fc2(x)\n","        return x, None\n","\n","asr_model = ASR(hp[\"dropout\"], hp[\"hidden_size\"], hp[\"rnn_layers\"], hp[\"cnn_layers\"], hp[\"n_mels\"])\n","asr_model = asr_model.cuda()\n","total_params = sum(p.numel() for p in asr_model.parameters())\n","print(f\"Number of parameters: {total_params}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gnuaQaNnflVW"},"outputs":[],"source":["\n","def compute_validation_loss(net, criterion, dataloader):\n","    net.eval()\n","    losses = []\n","    for data, label, data_len, label_len in tqdm(dataloader):\n","        data, label, data_len, label_len = data.cuda(), label.cuda(), data_len.cuda(), label_len.cuda()\n","        out, _ = net(data)\n","        out = F.log_softmax(out, dim=2)\n","        out = out.transpose(0, 1)\n","        loss = criterion(out, label, data_len, label_len)\n","        losses += [loss.item()]\n","\n","    return sum(losses) / len(losses)\n","\n","\n","def train():\n","\n","    # Create wandb logger\n","    wandb.login()\n","\n","    # Initialize model\n","    asr_model = ASR(hp[\"dropout\"], hp[\"hidden_size\"], hp[\"rnn_layers\"], hp[\"cnn_layers\"], hp[\"n_mels\"])\n","    asr_model = asr_model.cuda()\n","\n","    # Datasets\n","    train_dataset = LibriSpeechDataset(\"train\")\n","    valid_dataset = LibriSpeechDataset(\"valid\")\n","\n","    train_loader = DataLoader(dataset=train_dataset,\n","                                batch_size=hp[\"batch_size\"],\n","                                shuffle=True,\n","                                collate_fn=collate,\n","                                num_workers=3,\n","                                pin_memory=False)\n","    valid_loader = DataLoader(dataset=valid_dataset,\n","                                batch_size=hp[\"batch_size\"],\n","                                shuffle=False,\n","                                collate_fn=collate,\n","                                num_workers=3,\n","                                pin_memory=False)\n","\n","    # Train\n","    optimizer = optim.Adam(asr_model.parameters(), lr=hp[\"learning_rate\"])\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=hp[\"lr_factor\"], patience=hp[\"lr_patience\"])\n","    criterion = nn.CTCLoss(blank=28, zero_infinity=True)\n","\n","    min_valid_loss = 1e10\n","    min_train_loss = 1e10\n","\n","    with wandb.init(project=\"MIE1517\", config=hp):\n","        wandb.watch(asr_model, log=\"all\")\n","\n","        for epoch in range(hp[\"epochs\"]):\n","\n","            asr_model = asr_model.train(True)\n","\n","            train_losses = []\n","            for data, label, data_len, label_len in tqdm(train_loader, desc=\"Epoch {0} / {1}\".format(epoch, hp[\"epochs\"])):\n","                data, label, data_len, label_len = data.cuda(), label.cuda(), data_len.cuda(), label_len.cuda()\n","                out, _ = asr_model(data)\n","                out = F.log_softmax(out, dim=2)\n","                out = out.transpose(0, 1) # CTCLoss takes batch_size as second dim\n","                loss = criterion(out, label, data_len, label_len)\n","                loss.backward()\n","                optimizer.step()\n","                optimizer.zero_grad()\n","                train_losses += [loss.item()]\n","\n","            train_loss = sum(train_losses) / len(train_losses)\n","            avg_valid_loss = compute_validation_loss(asr_model, criterion, valid_loader)\n","            scheduler.step(avg_valid_loss)\n","\n","            # Save checkpoint if valid loss is at minimum\n","            if avg_valid_loss < min_valid_loss:\n","                print(\"Saved valid checkpoint!\")\n","                torch.save(asr_model.state_dict(), \"/home/asblab2/sinarasi/mie1517/MIE1517-Project/Speech Recog/best_model.pth\")\n","                min_valid_loss = avg_valid_loss\n","            if train_loss < min_train_loss:\n","                print(\"Saved train checkpoint!\")\n","                torch.save(asr_model.state_dict(), \"/home/asblab2/sinarasi/mie1517/MIE1517-Project/Speech Recog/mid_model.pth\")\n","                min_train_loss = train_loss\n","            print(\"Losses:\", train_loss, avg_valid_loss, optimizer.param_groups[0]['lr'])\n","\n","            wandb.log({\"train_loss\": train_loss,\n","                        \"valid_loss\": avg_valid_loss}, step=epoch)\n"]},{"cell_type":"markdown","source":["The above defines our training script, but is omitted for brevity. Our training script was modified from our Lab 2 code with the following main changes:\n","\n","1.   **CTCLoss**: We use the CTCLoss function because it computes the loss by simultaneously aligning the words and audio wave, while trying to minimize the difference between them. This in essence, is the problem we are trying to solve when performing ASR as we want to align the most likely part of the speech to the character in text that it corresponds to.\n","2.   **Learning Rate Scheduler:** Continuously reduces the learning rate during training when a plateau is observed in the validation loss.\n","\n","\n","\n","\n","To date, we trained our model for with two different batch sizes for 100 epochs each with our training code above. Our other hyperparameters are defined within the **hp** dictionary defined previously, some of which include number of gru layers, number of resnet layers, number of mels (parameter for the mel spectogram), hidden size, dropout and learning rate. We were unable to do very much hyperparameter tuning since it took at least two days to train each model and we were cut short of time. We also could not choose a larger batch sizes due to the limitations of VRAM in our GPU. We overall found that the batch size did not seem to make a difference in the performance of the model. See training graphs below. Our best model, determined by the lowest validation loss, was roughly around the 80-90th epoch."],"metadata":{"id":"KX1nzOayvLw1"}},{"cell_type":"markdown","source":["![train.png](data:image/png;base64,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)![valid.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAZAAAAGQCAYAAACAvzbMAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAEnQAABJ0Ad5mH3gAAFD1SURBVHhe7d0HYFRV3jbwZ2oy6T2QBiG00JEiith7L9jWvrqu67uWtazbX98t7377bnXtq4i9ggURAemC9NA7hPTee6Z+539mIsWAYYzmxjw/d5aZe6fcuZm5z/2fc+Zek08BERHRCTIH/iUiIjohDBAiIgoKA4SIiILCACEioqAwQIiIKCgMECIiCgoDhIiIgsIAISKioDBAiIgoKAwQIiIKCgOEiIiCwgAhIqKgMEDoe0WODdrW1g6v1xuY0jm324329vbAre8Hj8fzvXtPZGwMkD7O7fbgs0WLsW///sAUY9mYk4Ot27YFbvmt+HwlCouKAreOVFpWhqumX4/9B3Lhcrlw8eVXYvb7HwTmHjJLTfvRT/4rcKtz8vjq6prALeCFGTMx/fof6OnfRFNTE5YsXY5PPp2PT+cvwIHc3MCcY3M6naiuObQsnfl0/kLcfNsPj7t8uQcPYs3adYFbRN8MA6SPkw30bXf+CH/6f/8XmGIsny1eggcffjRwC8gvKMDdasNfVVUVmHIkqSyKiktUFdIKs8WCG667FtnZ2YG5h0iVUlpaFrjVuc1bt+K/HngwcAsYP24spl99Fczmb/a1KSgsxD0/vR9PP/s8nnnuP7jmuhvx0suvBuZ2bs269XjwoUProTPNzc0oKSmBzWYLTPmqOR/PxX//zx8Ct4i+GcvjSuA69UEvvPgShg8bhrKyckyZcjJioqPxwUdz9EYoLi5W3+fDOR8jLCwM0WreylVf4MWZL6OxsRHDhg3V8xepjbzc/91Zs9DQ0ICQkBC8+fY7+PiTTxDmCENaaqq+n2zgZEO5eOlSFBUVY/uOnRg1cqTeY575yquYO28+UlNS1OvG6fuLhIR4vPbm2zj/vHMQGxur99jz8wvw8M8e0I9/Q81bsmwZUvrL42LV6zfq6uKKyy9D/37JqK2tQ0pKf0RFRsLj9eL119/E0hUrUFNbq6qLalx/7XQ0t7RgkarC3nznHRw8mI+xY0ajrq4ezz7/Anbv3oO29nYMHDhAvReHbhobOGCAXjbZm5f7SIU0XK0Led/16v1LhWRR4TVj5ivYu28fRo4YoW93qFThN3/BQnw4+1388PZb1XppwazZH+D2W2/W1cn8hZ/hrXfeU1VWsV4Wuf+zKmj27T+ggq9Nrc80REREYK0KlRdfellVMAeRPXwYDublY5da3sGDs9T0mXpZMzMHBl7Vb9PmLfp5r79uemCKn9xX/mbvf/gRwsPD9d9BVKl19B/1GVm2fIVa/zFISkzsdBr1TaxA+jBpFpGN0B233YKkpEQVBIv19I9UYDz59DP6ujQF/e7x3yM0NBRz5n6C//79HxGnNuQSIq+89rq+z7P/eVE3B32xeg3sdjs+/OhjFBQUIlJttH+i9rS3bd+h73f/zx7G5ytXoaWlFb9Vz7ldTZd2+/sefAjrN2yEIzQEP1XXCwsPNU8NGzoUyWrZVn2xWt9etuJzTDttqq4C5PXdHjfq6xtw14/v0aEmry8sFv9H+2eP/ByrVvkf+8tf/1YF2CtwOV16I2+z+e+7Z89ezPrgQ70hfPOtt/F/f/uHXjcH8/Lg9Xmxd+8+tKsNtzQ5/eq3v9OP2bVrN2665Q69Qd+8ZRtuvv1O9b5a0NzUjEd+/gs8/oc/wely4u//fAKvvfGWfkwHk8mk+2okROZ9ugAbNubgph9cr+dtUWH00Zy56j0nqQB6GU88+bR67XYdml6vRwXSfhWEHhXyH+l1a1HrYc/evXp5HY5Q5OXn4331XlyqEpP1unPXLv28xyPhIVXeO+/N0uvvJ/c9oN+ruPenD+jgkpBfumyFvu+9an7HtMVLlun7UR8lZySkvmn1mrW+s869wOdxe3zP/edF3w033aqnqz1L3xnnnK+vvzhjpu+Ou+7W18+76BLfP5940qc28L5nn/+P74JLLtfTr7nuRt/PHnlMX+9QW1enn2fSKVN9L7/6us/tdvvGTpjsU5WOnn/+xZf61AbatzEnx3fS5FN8W7du8xUUFvouuuwKvSyH+/2f/uz78b0/9amNl+9MtbwqqAJzfL7i4hLfu7Nm+0aMGe/L2bTJpyoO34STp/q279ih58v7UFWLT+01+7JHj/Nt3rxFT1fVge/iy6/S1ztsUcvwwEOPqGW7TN9+b9b7vkuvvFpfF6ra8F1yhf8xKgx9P7z7Hn1dnHr6mer+s30qxHxjTprkU0Gppz/881/41IZeX++wa/dutYyn+m669Q7fbT+8yzftrHPVOnrNp8JUz5f3qSoF3z3/dZ/vsquu0dNmvvKa7+rrbtDX1YbbN+W0M3yvqPV6OFXF+KZMPf3L5zn97HN9KhT09Q7PPPe876rp1wdu+eXkbPJlq/VXVFysb6udB/23leWQdfGr3/zO16Del3A6Xb5zLrhYT5P3Sn0bK5A+TJqjyisqcP1Nt+DDOXOwPzdXN+ucNvVU3eSybdt2rNuwAZdcdJG+v4zwkeaa//fXv0FtbHHmGdP0dNkjHjp0sL6uPlOqenlW7x3vVHvp0uzV2tqqn0+aof7wv3+GCiFERUbpPf78/EL9mFdefwN/+evfkZqSikGDMvVzdTjvnLN1RSNViENVQiedNF4/5y9+/Rs8/sc/qQqkXjfpqI2b2r33P0b28jtYrVbdZyJ71ymp/qYZXakE7iPv5bof3Iy5n8xTe/GOL5ub5DXUO9LXhTyn/Ceks37oEP97FqmpqchXlZPcx26z6fct5LUPXxYhe/FSQf37X3/HyzNewJuvvoy//uNf+j1uVev8uhtv1tVJeFiYup9/WaRPp0Oder9S7YwcOSIwxU+eV5qfOvpopElN1u3Rjl4eeS+Rav0lJiTo20OHDEFNTY1+vpdn/Ed33p934SV4/oUZqmqz4pXAtHNl2oszOn0N6hsYIH2UdDYvXLQID9z3U/zhf36Hf/7tr3qD9anacMkGdJoKEWnflz6EjqCQPoihQ4fgqSf+iWef+jcee+RhPV2aoaQJRZSXl+PfTz+D//7Nr/CTH/9Id1Z3bGA8Hi+GDx+m2+jffuNV3a+SOXCAfrzcV573heee1oFxOOm8lvv+/V9PYMKEkxCiNv7r1m9QoTcX/3nmKVx5+WVobGr6csMo70023EI2gk6XC/3799fLuGXLVj39wIEDMAWWSwIvKTEJv/rFz5GcnKSbpYTJbEJDQ5O+LuS5pB9FjBk9WjeDCdmY7t6zFyeNH6dHtcl95D0JeUxn21eZXqY23NJnsmnzZqgqEPYQO/715FM6QH/x80cQFx+nQjuwLCaz7l8SCfHxuonrYxV4QoJO3rO8f/lXnlt0LMPRZD3Ic0nTn/S5SF+W/JuzabOe/9nixRiUmfnl8z339JP41WOP4t9PPa0/D7IsHdOe+PdTum+L+iYGSB8lnakyRPWaq67UnejSCSyh0dH2fdWVl2Pe/AUYMCAD8WqDJR7/7W+w8LNFuOHmW/XIIekTOZrseZ88aRIefPjnus9DNkJWq38vWvoopON1ydJluP/Bh/Xzj1PhcPutt+C2H96FW26/E7ff+SNdFR1O9qTHjRuj+086wiVr0CCkqWpi+g0/0P0yMdFRuk1e9qRjYqIx85XX9P2E9CFIJ/oP77hNVS2/xV0//onul4mKitLzzz7rDKxdt05Nvxdr167XAwkk9E6ZcrJ+zkuuuEr3k0REhKt7+9PgXhV4EeERaj3cgBvV+rjogvNx1hmno6m5SQdGR2j6/z0yQaRCkDCV/oUrr7kOf/rz/+GhB+9Higq5qaecgqXLluuRZps3b0V0YBml30cGCFxxzbW6T+Z///g/WLDwM1093nDzbbp6kT6QrxMa6sC+fftx9bU36Pd1730P6nX5swfuw2O//A1uuOlW9dnYit8//jsdpA89+hhuvOU2vPDSy7jx+uv0c0i/kkyTDvzp11ytw536JrUT1tn+EX3fSRNFndoDHXTYKB3ZQJVXlGPIYH/TjIwgSkxMRGxMjL4tJARkSGl8XBxGjRyhm4JkaG1EePiXQSPNK6vXrMXIEdkIdTj0RtjZ7tTNVzfdeIPegG7M2YTnX3gRq5YvURv8GN0RnJdfgJHZ2UhVwXB0M0uDel0Zdjs4a9CXTUwVKmikyefkkyfr15RRUtJxL8NkZd7ECROQezBPLVfclxti2ct2u10YrSqI6qoqpKWl6ekSqLLHPmb0KJSpKipdTZdlKC4uwfadO1UoTlQViVntgdd+OQpL7i+hJs1nE1VlJKTaKVDrQ0aeycADGd0mFYGMBOsgTYEyUMCllkNIdSSh1UE61WUdyfotL69Aerp/GeV9yaiwSRMn6GpQmhtlPWakp+sqQv42MkJKqgchgwBkwENHc5qoravTz+nzedVy+XQ4yzoVEkzyt5yi1qesRyGVnQxwkGCW19XT1Ous3yjTIr+cRn0TA4S+Ezt27tJVxh/Unq1sTN96+z0dVs8/87QKoWP/boGIjIsBQt8Zabr66ONPYFZ79tnZw3DLTT/QndZE1DsxQIiIKCjsRCcioqAwQIiIKCgMECIiCgr7QIiI+gAZ6i5DzzuGyPt/o2XVPxbtiAH5V4bJd/W3PaxA+jhXUyvq9pegMb8CjQUVqD9Qiva6Q7++PlzR8q0o37A3cOvENRZW4sAHXwRudY2zvhl589bLJzsw5cR5nK4vvyBelweNRZWoP1im3/vxeH0e1LTko661CPWtxahtkX9LAnOPVFVbhM9zZgdunTg5UOLidW+i3XX8ZTrauu3zUVNfGrh14uT3IG7PofOH1DdVIb/06w/ASL2PPpKCChC5yHW55Obmfnn98HldxQDp41wtToTERiAiLQERqQkI7xcDT3vnJySq3JKLBhU0x9Ne14ydMxcGbh2ppbwOZev2BG51jWz4PS534NaJ2/7Cp1hw8//BG3hPe95ehm3Pz0PhZ5vQXHr8EzR5vP6z+0WF9kdkaD9EO/qraU690T1aXWMldh5cE7h1bPO/mKk30l/lw+Y9S+WfE+LxuvURg4Ox++A6/OLfF2N3nv8EUyWVB/DBkiexZutcLNv4np5G3y9SdchFTr+wY8cOLF26FDt37tS3O+adCJ4PpI9ztzphtllgdYToYz/5vD69l24L/+phMWp3FaJmRz5qdheidk8hEscOQuWmA9g3ayVKv9iB2GFpKPl8O7bPWAB7ZCjisjOw790VyP1otX7e8OQYXcW4GlqR9+l6JIzOhCXkyB8R+tTeT94n61SlsloHWXRWf7SU1SJ6UH/sfXs5itXzFy/fhv6njkDxyu3Y985yNJfUIGZIKoqWbEbVtoOo2VmA1sp6RA1IhiMxBuUqtNLPGgdLqB35CzYi8+JJyLzsZITG+3+dfixenxtuFSIOW7T6Ypn1pd3dhFBb5Fe+aM2tDcjZvVj9W6+qgk+RmToK7c4WfLb2NT3dZpMDG3rx1vy/oLG5GkPSx+NgyQ4s+OJlFFXsRXq/4di673P1+Dqs2vwRYiIT9eVoB4q2YPHaN3SVMDBlFGobytA/MRP7Cjap152HteqSHD9AVTItmLfyRRUO65GWNESFw35sP7BK/1tUthfx0f0RERaDYhUasVH9kJY8RIXbyxgx6GScO+VmzF3xPMYOPQNWC3/k+X0hx0CTHTIJCznx2J49e/QRB+rq6vTRFOSIBVJ9yFEQOk6L8HVYgVCXeZxuvdHNvvVcVG3NQ8nKHXrDPvS6afC6vXoDL6ESl52OtDPH6uCo3lWAsT+9XIXFQN2U5GlzYcBFE3WgFC7eFHjmQ9pqmrDr9SXIvuUcHRKtVfXYN9t/0MKBl0xSYWdVu1H+5rCDc9dh+E3noO5AKYqWbkFEeiKiMlWlMKgfwvv7D6sSmZEIe3SY/uIIe1SYDpHVv3tVhWDnp8UNRkdVcurYy9EvIRNzlj0LR2gkpo69AiOzTsVHS5/WgZCSmIVTxlyOlrZGzFn+LC445Tb9GNlQt7Y1ITtzCkYPmYZFKiQ6M2/lDP0c50y+UT/m800foLy6AINSR2N45skortiPmIhEXUlMGHEeBvTPxscqDKIi4tE/YZAKl4FIThgIq9WOyPA4JMTI0Yn9y96kwitOBYsIsYejoalaX6fvF+nzkOC44IILcOGFF+p/JUCk+epEMUAIpsDJl4Q5cODDzvkQOzRVVRcOxI8aiKaiStTllmDXq4vgbmmHu90JW0Somh+mm8UqNuci49zxeqMdGhepK4rIjCSERIcjvF8cXKr6OZojIUqFwtlY///eQcmqHbCqqqGjGvJ5vKq6yMf4h65WAXYQbSpcDs6T5heffg3pv5EKqXp7PhoOHjpdrYRHR8Uw5p5LMOlXN2DA+ROw/UX/gSOPR6qOw3Uczv1o0o+QFJeGcEe0CoGTVZVRoy+yV3+weJs+jpbZbFWhEq7ul66rjuS4DMSrDXi02uBb1LwwRxRSVbUQH91P3lKnrjzzv7Bt/0p8oqoLt8eJMBVSsoyhIeFYuelDXH3OA7BYrCitOoh9+TkoKN2tK5Kq2mLklWzXlUt+6U44Xf6j/ErfS8d7lHUkt4UEojwPff/IMdr27t2LRx55BL/85S/x6KOPYt++ffq4aCeKAdLXqY1rW1WD2vNv1JeWijq9oe6MBIDs+UszU832PEQO7IcdLy7A6B9fgn6nZOtOadlQOhv8h/eWSqB09W593dnYoqsOZ6O/k9jd2q6qlq/u8UiFknHeeJzyh1t1MMnyyP3kNXP+NhsDLlDVi7odlhyrQioSI++8AJN+cT36q9fvN3kY0s4ei/RzxiFpwmD93qSJzqVe093m1BVQfa4/WOS2Lez4XxgJC2myanHWBi41cHn9G97OVNQU6Q3wztzVSEseqqqD95EYm4pLp90Nl9vfd+J0tqKxpRb94jNRVp2nO83bnM1qg96qm7wkFNzqvk53568TE5WEe6b/TVcaOw6shllt/EPsDqzbMV8//+D0sWpP0o2o8HjdBHXV2fepauUHennGDz9HT5OLIyRCv05LeyNa25v0ckuFkqvCTpZPAjG6kyY06v3kYJ6TJ09GdnY2VqxYgREjRujbMv1EsQ+kj9N9ENK8E9hDlwAIiYmA+bCqpINbbdybiqpQ+sUu3VSUeclkvYGWvgeP2iDHDklFwthBgT6SIgy74QzdJ1G0ZIveYEugyEZc+j6cdc1wxEfpforDuZvbsHPmZ6jI2a+bsBLHDNLBFKUql8KlW3S1VLn5AAZeOEmHTe6c1ajdW4SYrBRd5dgjHLpCsqpwkLDa/cYSuFR11Fpeh9jh6ShYmINCtbyNBZUYpcJHqqVjkYrBYrKrYuBQBSP9IVbLV4NHOrKr6tRevtq7l38vmXYXIsNisV1VC5Vq71+alUYMmqLuacKqzR9i3LAzVbUShRU576OofK/ewLe1tyArfRw8XpcKnHZkpY31P/lhVm/5WPexxKkqZdLIC/TGPl09duu+FWp5LdhXsBGxUckYPnASlqx/C/sKN6kqJUJVPRm6WpHgkIu0c6/Z9gnKq/PRpJ4jJjIJowZPxba9K/RznTHhWiTE+M9lT98PHX0gHc2548aN00ejnj59uu776Bh9dSJ9IPwdCJ0wqUQO7/w++raQQJFOayEhY3V0/oHcN+tzNJfVwmKz6IAadOnJuuP8eI85nLy29IvIAICukupHBg18G2TP/fCOZ6kKjm4Gk/tY1MZepuvrFrX8nTSNyeionblrEGJzqIqkXXeGS2hI1SLTukKCyGY9sfcq1UjHmRDp+0NO/CX9HB07QxIUcjBTOSFZR3hIHMjvQOTMll3BAKEe5VIVh24ykw+1+ijKhl1GhZF/4y8XCRepgiwqmLoaHERHO5FNfUfIfB0GCBERBYWd6EREFBQGCBERBYUBQkREQWGAEBFRUHo8QBoaGtHc1BK4RUREvUWPB0h7uxMud/BHWyUiop7R4wEi4427OuaYiIiMg30gREQUFAMEiJyD4siD97W1taGpyX9AvsrKKrz33hwUFXV+JjgiIuoZPR8gJv+xkg63aNFy5OUV6J/ev/TSG2hoaMCcOfPR2Nj5qVaJiOi7Z4g+kKMDpK6uAdnZQ5GTsxWxsbG4886bkZU1EIWFxYF7EBFRTzNEH4gcuRXeQ4fkGjgwHf/853O66rjwwrP1GbTy84uQkOA/yxwREfW8Hj+YYk1dPRoPlCFj7GCYDjsb3ubN2xEZGaErD7Fr115dlRARkTEYoA/EpE825PMcyjFpwho6NEuHR25uHp577uUun+CEiIi+G4boA5EmrMNHYi1ZsgIFBUVwudx4441ZSEvrj4ULl6KmpjZwDyIi6mmGqEA8HtcR58eWM2QNGJCOVavWYuDADFx66QUYNGgASksrAvcgIqKeZowKxOuCz3MoQLKzh+Fvf3sKy5evwoUXnqPP5XvwYAFSUo48fzYREfWcHu9Er2tuQf6yNRg2ZQJC46MDU6GqjXLExcUgJMR/Pmf5XYhUI0REZAwGqEBkBK8LXteRB1SUymT27Ll45pmX8NlnyxgeREQG0/N9IDDB43PDc1gfyL59uSo4ZiAszKGH7soQ3hkzXj+hk8ITEdG3yxB9IF4VIN7DDun++eerccMNV+PKKy/GWWedhvvvvxtOp0s3YxERkTEYogLxwgOvyxW4DVit1i8PptjBpebLdCIiMoYe70RvbG3Hts8+wtDBE5AwIktPKy4uxX/+8yoyMtIQHR2JvXsPICsrE9dff6WeT0REPc8wFYjnsAokNbU/Hnzwx0hOToTb7dHHw2J4EBEZiwEqECc2L3oPWf3HIGXi6MDUr3r//Y9xzjlnqIokKjCFiIh6kkEqEC/cziMP6X44r9eLPXv26/OnExGRMfR8gKj6x2eSAGkPTPgqs9kMh8OhrvHc6URERtHjTVhNLS6sX/Y6ssKz4c1Kx6rlqxEScuSRdz0eDzZs2IJf/OIBxMfHBqYSEVFPOqEKRH7gt25dzjFPLStDbaura1FZWY2qqhrd9NQlJp+/AlFRJr8LOfpiNltw3nlnIjIyPPAAIiLqaV2uQObOXYja2jokJsZj+/bduPnm6ejX78iDG86e/bHuq5AzB9psFlx77RWIiIgIzO1cU7Mb61a+goSmJIy55rLAVCIiMrouB4iciyMuzt989OGH8/QRcq+77sihtXLiJ6kUOs4i2BU6QL54GbHVcRh/w9WBqUREZHRdbsLqCA/R2tqG0NDQwK1D5DwectwqORRJQ0NjYOrXkE50sw+e9rbABCIi6g1OuBNd+jb+9a/n8NBDPzkiVIScSVAON9LW1q77Sn7849uQmJgQmHuIHKZEOsbVy8Pl8qKicg+s9WYkZw/TI65sNh6yhIjouyIDl6S/+USdUIDIr8L/8pcncMYZp+K006YEpnbu1VffQWxsDC677ILAlEO8XnlJ/8s2N3mQs/0t2Hf5cPIdt6gp0nGuZxER0XcgmPAQXQ4QGVH14ouvIT09FRdddG5gKlS10aYqBps+Wq7cJzw8TE9/+ukZGDZsCM4993R9+1iaGt1Yu3UmLBvbceb9Pw1MJSIio+tyH8ibb87Gxo1bdf/HG2/MwqefLtbT//Wv57Fs2SrdLPX886/ggw8+wcyZb+pfjU+dOknf57hUfJmsZnjc/JU5EVFv0uUKJD+/EC6XWwVDOzweLxyOUAwZMgj79x9EVFQkkpISUFdXjx07duuKZMKEsbBYLIFHH1tTgxvr974C5+JKXPDYLwJTiYjI6E64E7276QDZ/ypaPy3Gxb/+bWAqEREZnQEOpqhSzGLyN2H1aJQREdGJMMTBFCEB4lEBcui06EREZHDGqEDUUrg9LuDQOaWIiMjgDBEgMKsKxOuCz802LCKi3sIgFYgJXgkQF9uwiIh6C+NUID43PE53YAIRERmdcSoQnwteBggRUa/BCoSIiIJijArEBHjhUQHCYVhERL2FMSoQlSA6QNpZgRAR9RY9HyA+HyxmK3wmCRBWIEREvUWPB4j+IbrZBp9Z1SDtPCIvEVFvYYgmLH+AyGltWYEQEfUWBggQHywmfwXiZgVCRNRrGKICMasAgQXwtDFAiIh6CwP0gfg70aH+52aAEBH1Gj1fgcgoLFWByDlB2IRFRNR7GKACCTRhSQXS3u6fSEREhmeQTnQrTDYzm7CIiHoRQwSIWZUfJqsKEFYgRES9hkGasCyBCoQBQkTUWxiiE91sssJsN/OX6EREvYhBmrAsMNsscLEJi4io1zBIE5aqQFSAuJ0MECKi3sI4FYjdAo+TTVhERL1Fz1cgqgQx6SYsK9wuBggRUW9hjArExAAhIuptDBAgshDqP2nC8qgA4UkJiYh6BUNUICa1GBabDR63C16XNzCdiIiMrOf7QNRF+kAsdqsKECd8Lo9/BhERGZpB+kDMMFtVgHhd8LjYhkVE1Bv0fIDoUVj+Jiyvzw1vOwOEiKg3MEQnukkasSRA4IHHyfOiExH1BobpRDdbrHB7XKxAiIh6CWN0opvMsFit6rqqQNpYgRAR9QaGaMLS47BsdsDmg5sBQkTUKxinD8Rig9fkVRUIf41ORNQbGKcCsVjhhRueVlYgRES9gUEqEOgKBOp/7lZWIEREvYEBAiTwW3QVIKYQkwoQnhOEiKg3ME4TltkGn8XHCoSIqJcwSICoBTFZdRMWh/ESEfUOhukDMZutMIeY4eYoLCKiXsEwFYjFJH0gZg7jJSLqJQzVhKU70RkgRES9giECRMZh6QAJVRVIO/tAiIh6A4NUID5YVICYHVYGCBFRL2GcJiz4O9E9Th6Nl4ioNzBUBSKd6F6eD4SIqFcwTh+IVCDSB8JT2hIR9QrdFiAejwcbNmzG22+/j7feeh+lpeWBOV3g8+kAMdkt8LoZIEREvUG3BUh9fQMOHMjDaadNwfDhg/HKK2+jtbUtMPfrqAAxWfx9IAwQIqJeodsCJC4uFtdffyXS0lIwfvwYmM1mVFRUBeZ+DZMJFhUgXpsbbhU6Po83MIOIiIzK5FMC17uNBMdzz83Eo4/+FA6HIzD1EKfTCa/Xp3LDpCqXRoTaHWizlmPr9rmwvevAyf99I2wRodKyRURE3zKr1RK4dmK+lQD5+9+fwbhxo3DOOacHphyppaVVBYhUGSZ1vQVWSwg81nrklqyBe5YTY+66ACEJUf7edSIi+lbZbFa9Q3+iuj1A3nnnAzQ2NuGuu24JTDm+qqoahIVGwhlSgs/W/w22x6Nw/ssPIywlLnAPIiIyom7rA5EmqQ8/nIe9ew/g2muvQFNTM5xd/E2Hz+dVSWZFSHgULOF2OBtaA3OIiMioui1AampqsHXrTiQlJeK99z7CjBlvYMeOXYG5xyclkAzjtYRYYLKa4GxkgBARGV23NWHJ03S0ofn7N9STq9tf164mTVihIWEwhTZhecHTwOM2jLnjUqSdPSZwDyIiMqJuq0AODwoZwiuXLnfKqPDRvwOxWGAJtcJZ3xKYQURERtVtAfJN6CYsORaW2QRLmA3OBgYIEZHRGSJAJEKkAlElCywRDBAiot7AUBWIsEZwFBYRUW9goArEvyjWyBBWIEREvYBBAgQwwaJLEWtUCNzN7YGpRERkVMYIED2S2AR4pQKxw93CACEiMjrDVCCaz6T7QHhedCIi4zNEgHT8ktGkKhAZheV1egJTiIjIqAxSgQQiRP1jURWInA+EVQgRkbEZqgnL5DPDEubvTGc/CBGRsRmuD8QUqhbJZIKruaunwyUiop5gqAAxy+LYTTBbLXA18ceERERGZqwmLPktiA2w2Gw8pDsRkcEZqwKR42FZvLCE2NBe3xyYSkRERmSwJiwLfGYvbGE8nAkRkdEZK0DMVnhNHtjCHXCyAiEiMjSDNWFZ4fG5YYt0oJ0VCBGRoRkqQCwqQLxeF0Kiw9mERURkcMYKELMNHhUg9qhwuDgKi4jI0AzWB2KD2+NUFUgYXM0MECIiIzNYE5aqQDxSgagA4aFMiIgMzWBNWFZ43P4mLE8bD6ZIRGRkxusD8ThVgDh4NF4iIoMzWIDYdROWNSIEXreHIUJEZGCG7AOxhYVCTnPLfhAiIuMyVIDoX6J73DDLOUFMJrh5SHciIsMyXgXidQN2tWA8pDsRkaEZrhPd6/WfD91it/KQ7kREBmawALHC1xEgoXYezoSIyMCM1QdiUhWIz6uv2xwhPCcIEZGBGa8CCQSINTyUFQgRkYEZrAJRAQJ/gNgjw9BexwqEiMiojBcgPp++LgHibGCAEBEZleGasKBqEKHPCcJRWEREhmWwCsSi4iNQgehDuvOHhERERmWoALFbI3QB0o5GhEZHws1DmRARGZahAsRqtsNqCUGDuxyOmCh42p2BOUREZDSGChDhsEWjoaUMIaoC8TjdgalERGQ0hguQ8JAEf4BEhsPn9vKQ7kREBmW4AIkKTUJjazmsYSF6SC/7QYiIjMlwARIZmoyW9jqYQk0wmUwciUVEZFDGC5CQJLjcLfDa3DBbLHDykO5ERIZkuAAJs8fp42E5TS2w2nlEXiIiozJcgNgsofqovK2+OtgcDgYIEZFBGS5ARIg1Ak3uKtjD5XhYDBAiIiMyZICE2+PR6KyAPSIcTp4ThIjIkAwZIFGhyWhyqgokkk1YRERGZcgAkaG8rc5aWCLsaGeAEBEZkjEDJCQJbe0NcGREoamgMjCViIiMJKgA6TjpU2fa251obW1DW1vbce93PGEhcXC1tyF8SCxayxvg8/jPUkhERMbR5QBxudxYsGAJHn/8L8jLKwhMPdLcuQvxj388g5kz38Jrr72LpqbgOsBD5bDubhMsKVaY1CI2FrEKISIymhOoQHxwOByqsmhHS0vnvw4vKirGpZeej3vvvQN33nkzIiLCA3NOlAl2Uxjc4W0Ii49Bzc7CwHQiIjKKLgeIzWbD6aefgrS0FLjdnsDUI0nzVWVlNbZt2wmv16ePZRWsMHssmr3ViM1KQ/WOvMBUIiIyCpPvBDsqnnjieZx11jSMGTMiMOWQjz+eD4vFovtAcnPzcffdtyI6Oiow9xBp2vJ4vDpg5L5msxkhIXbdZ2KxqOv2UGwpmQ2Pox0Jy4cjd+EqnPmf++B1dR5cREQUPLvdFtQOf7cGyOFefPE1pKb2x0UXnRuYcoiEhz53rVJTU6+qGysiIzuau0w6UHKrViK/eT0mtt2MZY89jUs/+N03qmiIiKhzwW5bT3gUllQYDkdo4NYhLpfrqFFXskCdL5RUGfI8cpHlloXvuC3zZFpUaD9VndQjfGCMPrFUS0mNvh8vvPDCCy/dewlWlwOkuLgUH344D4WFxfjss+VYv36Tnv6PfzyL5cu/UJVEHZ588gUsXrwCs2d/jPLySpx66iR9n2CEh8TD7WqDOdQMR2wUavcUBeYQEZERnFAnekxMNK6//krdfBUeHqann3HGqcjMzEByciKuvPIS3ZEeGRmBBx64u9P+j66Sc6PLEN42NCFmUCqqtrMjnYjISE64D6S7VVXV6HCKjo4MTDnk0x1/wITs69H0Xg3ylq/H2c/cF5hDREQ97YT7QL5LDnsM6huLkTgqC82ltYGpRERkBIYOEDkmVk1DASKzEuFxutBSzhAhIjIKQwdISvQoVNTtBUKBkMgw1O5mRzoRkVEYOkCSo4bB7XGiBTWIzUxH9Y78wBwiIupphg4Qs8mK+PCByGtah6TRQ1C7h8fEIiIyCkMHiBgQPxlFlZsQMzwFjUVVgalERNTTDB8gKdGj0dJcC2+KC94WL9pqGgNziIioJxk+QGyWUETaklEbVoDIxASUrt4VmENERD3J8AEi0mNPQknLNqSdPRa5H68JTCUiop7UawKktqYIcWeno3F/BZz1LYE5RETUU3pFgMhxsUIRiZbkakSn9UfBopzAHCIi6im9IkBEasw4FNbnYOBFk3Fw3rrAVCIi6im9JkAGxE1EZeU+JJ6Viab8Ko7GIiLqYb0mQOQEUxavHa1JdfpX6XmfbgjMISKintBrAkQkRQ5DYW0Osi45FfkLGCBERD2pVwVIRtxJKC3bgf5nZ6O5uIZH5yUi6kG9KkASI4bA2d4Cd1Qr4oYMYDMWEVEP6lUBYjHbEO1IRWHzJgy/9izsf39VYA4REX3XelWAiPTYccgrXIt+5wyDz+1B3nxWIUREPaHXBUhqzBg0tlbACzfG33c1tvz7o8AcIiL6LvW6AAmzxyHEFoni+q3IuGi8egcmHh+LiKgH9LoAEf2ispFf6W+6Gv/Aldj69Mf6OhERfXd6ZYDIcN7qloPqmg8Z550ES4idHepERN+xXhkgCeGD4PV6UNNcoG+Pf1BVIc+wCiEi+i71ygAxmSyIDU9HUd1mfTvtrLEIjY/Clifn6NtERPTt65UBIrISpmJP+RJ4vG59e9pf78L2GfNRuSVX3yYiom9Xrw0QOclUQkQmPj/wrL4dmZGEKf99M5b99Gm425x6GhERfXt6bYCI0wffi4qGPdhXsUzfzrrqVCRNGILPH3lB3yYiom9Prw4Qi9mOc4Y/hDV5r6C+tVRPO+0vd6J6Wx52v7FU3yYiom9Hrw4QER8+COPTpuOzXX+B1+eBJcSGs5+/Hzn/mI2Nf5sVuBcREXW3Xh8gYlTKJUiMHIyPt/4a7e4mxA1Px+VzHkfpmt34+Mr/QWN+ReCeRETUXb4XASLOGPJTJEcNx4ebf4661iJEpCbg0lm/QdqZYzBHhUj5hr2BexIRUXcw+ZTA9R5RVVUDm82G6OjIwJRvZmfpfOQUvqs72DPiJupp+2etxNbnPsFVC/8XJrNJTyMiom/me1OBdBjR/0KcOfR+rMp9Ecv3PYU2dyMGTz8Njrgo7Ji5IHAvIiL6pr53ASLSYsbh6nF/h8lkxgebH8We6sU47fd3YueLn6G9vjlwLyIi+ia+d01YRytv2K1/bJg56BR4/uSDx+7CKX+8NTCXiIiC9b2sQA4nHetXjf0rCotz4LmpFRXLDqAhv1zPk8OefPGrl7H79cX6NhERdd33vgLp0OKsxfziP8H0hhXWRWEISQ2Hq6ENyeOHomTtDvQ/czgm/fKGwL2JiOjr9JkAEbVtBfg457eI/iQNQ089E9Gn9Ic7sg22mjBsvfsThGfF4bR/3Bm4NxERHU+fChBR0rANaypmItQcA1M7YEEIWs01SAgZjNpflSHCloBpz9wFS7gt8AgiIupMnwsQ0e5uhsVshdUcom+3uRqQU/YOypp3o/r/FSGpZRjO+8tDCMuM0fOJiOir+mSAHEtV6wFsrZ6D/PXrYW8Px0nnT9c/RgxHfOAeKnzqmpG/YANyP14Ld0s7LnjtUdjCQwNziYj6DgZIJ9rQiLzCNVj17ovwJLmQftJ4hO9PgHupG007q/VhUoZcNQ1Vm3NRsWk/zn/zYZgcXR/Q1lbTiNA447xfIqJgWB5XAtd7REtLKywWC0JD/c1JRmBFCBKiszDQdgoc+2MQkZ6AmoQ85JvXwXKmCRE3xKIpowLmSWbkrl6N2t2FGHDaRP3Dxa9TtS0Ps8/6OWxhIfrcJUREvRUrkBPQjGo0tpajraUBba5GtHrq0YAylOzaro+xFZeegXBHAiJDEhHmiINb1TINNeVobqtBVHQ/pGIM1t36DrJvOBe5c9YgfvQATPzttcD3vL9+d9lCVDUfxLi0qxGh1g0RfT8wQLqBs6kFCx77P1SU7oM9ywHrQBta2+vhqXFhwt3XIiImEdWVB7Fl6UcIj43HuPOuQn1eKba+NwcJZ2cicchgjOt3NWKt6YFn7JqWhlqERcUGbvW8xrYKhNqiYLP4+4QKazchp+AdhIREIDoiBYXlm5AWO1YHSZg9Tt+HiHovBkg38bZ40V7VBFtoqD6pVXN5LTb879torW7E1D/fjj2vL0WruwGD/zgFucVfIDQ0CnEJA1H02lZsXfgxbNPtGHvm5RibeDUcpujAs/r54IUHLjidLbriKW3ejoIDOWhprEN4ZCxi49KRnDQMA8InI8QUEXjUsXm8Tn3yLbPJApO6yL9Hq20pRG7VKtQ052Nw4unITJiipnZ+JOPiuq3YUPAW3J42FR4OfaZIr9cFt3qdUamXYkj06XBVt8OZ3IKNB95GeeNuxIUPREr0KH2JdqQEnulIze3V+nlPzrwNodaufz7kR6Ol9TvQL3oEwjsJKo9aNlmnHaPwiCg4DJBv2Z43l2LDX95F1KD+uOTtX8McctTG2guUL9mLXYsWYZ9tGWwn2xERnaCny/Za/jxejxvWsBCE9YtB2co9aPq8BiedfS1GX3wxygv2Ys/qxSgq3oKIM+Nwxin/hXT7BP9zB7jUhr2sYSfKG/agqjkXrc463V/TERzyr1VVDSHWCHUJR11rCdpc9UiOHIao0H7Iq16rn2NQ4lS1wR+tqoxIvUGXaevz31CBsAcTMq7HgLhJcLpb0Oys0Sf2So0Zo5/780dexMF563DT2qdgibSrSqUc+TXr9Ua+vq0MIZYwTBp4M/pFZevlEdXNB7F0zxOwq+VxeVr1qYtjHGmBuYdISLWqwGj11aPamYfCyhw0t1RDvSO0qWUYlnQOxqZdoe5p0s8jh/s/ULVS35amxsSIwYgOS0W7q1EtdzWa2isRHpKAYcnnqPlJ+jV6I1n/Euad7RwQdRcGyHegqagKllA7HAlRgSmda9pVhQNbV6kNXwPMap1Iv4qztgWV6w6itagOIaHhQIMZE++/HulXjAk8SvEANWsLsejFf6Biyi5MuvZGjI+ejrrmEuypWITKpv1qQxyGOMcAJKlQSIgYpPe+pQrxqYvH5Eabp0FVSLVoUZdoeypSw0frSqJDWesuHGhcqTew7tZ2eNpdaqPbhJSY0ZisNv42syNwzyNteWoOildsR8KYgajeno+L3v5lYI6fhND+yhXYXPQ+MmInYErmHSht2I5le/+NUwbfgay4achtXYWcondxWuo96BeWjXpXCQ6UrdQB5IET9ugINB+sRvu+ZmSPPx9Ds89CuCMOjajAmsKZcLW3ITEsC6WtOxEdnoLRSZfBYY5GSf12lFfv1k1vEpwOe6yuWKT6KmuQKmmACpKzERuWrgIzSq2Pjs4qnw5KpwqkCBU2RlLfWopdZfNxUIV+XFgGzs/+RZcGdxAFgwHSS7QU16Fy8wEkTR4MR+KRTVwd2oqasOj+fyA3dRVS7x+J8NA4JIYMxoCQyUiIHhS4VydUteNpdMNV34r2mmaEZ8TDGndkz357abNuhnOjHQMun4CoYUlqL98Gc2AEgLOxFWaLWVdKHfLmrcfq372KKz/9o17medf/LxJGZ2Lyb24M3OMQn4qCVWUvoqxqJ8xWK84cfh/i3ANRum4XXFVt2LV7IfIHrkXi4Cy01tRj4IiTkdV/KkKaInHg5dUo+GATwmJi4fOqQPS5YIt1IDYzHbFDUlARvxdlnl1IrRsLe14EqrYfRFRWMsbdc4U+Jlpn2tGE3UWfobA2Rzd5mcxmOKJiYLFZ0VJfC7fTqUMkLjwDpwz6oQqeQ78V+jotqkJz2GLVhr37Tm5WXLdFBcdnOvyk8huRdiG2Fn2EppYqXDzqt+oex34tn/pPmipL6rehtG67Wn8eFeS3q+D8asVHdLhuDRCv14uSkjJUVlZh/PjD9pCPgwHSvdyNTiy589+obDuAtKHj4Kvyqg1eHTyedkRmJiPl1BFIOmkwmstqUbp6F6q35aGttlHayvSeqnwYvE4XwvvHYeBFExGRnog9by1H1dZc9Js8TG1AbajYuB9hSbHoN2kYmiqqUH+gFO11TfqxMVkpyLx0MuyRDiy7/zmcO+NBJI0frJdN7vPB+b/GKb+/BQMunIjy9Xv0DzJr9xQhfthA9J+YjbqUQtiawtC4oALFOdthdYQgND4KkTGJcJweiZqmAtS8WoyE1IGIHZGGvMUbYI8Nw8THrkXC2Ey4mtt0xVefW4ravcWo31cCZ2UrvG1eFSqhiBySqEJsoHoP+1CwaBP6TRmOYTeciZbyWlTk7FdVUh6iBvbDlF/fDGu0Pwylv6QV9dj8xgeoOVCIiT++DonJWbpfalPebByoXIlxaddgRP8L9P0PJ01jtc4i1KFIbdwL0NSmKiVXgyq9zJiSeruuBg+3o2QeCmo3qmsqUn1u+X9VHUUhMXwQkqOyEe3o72/WlMrR69T3zataq+8/MGEKslPOg90UDl+r+nvaTFhR/rT++1844tf6+eVxDapKqW7JU6FRgLqWIrS4anUlGhXaH2mx43QfkjT1De93Lk5Kv04/rjtJ31arq/4r7526V1N71bdeIXdrgOzevQ8zZ74Fu92GP/zhyKaKY2GAfAvUn3Tbk59CbUkQNSgZ0Zn9dLNTyaodeuMvG1h7lAMxQ1KRPHEI4kYO1D9slI2+HgCgwqVwyWYULdmi75tx3ngMv+lshPXzj/hytbSjaPFmFH++HRGp8YjLzkDciAy429QGbeFGlHyxC1XbDmLqn25XYXKyfkwH2Uh/duc/EZESp6oFH/pPVYE2Lgs1uwt1mDnLW2EKsSBpyiAMuuwUxI8aEHjkIdKsl79oo17GwVdPVct3UmDOsXmcbljs1sAtv+bSGmx/4VP9PiQwY4emqtfLROmaXShevg3jHrhCh0vlxlzk/N8sFaxehCXGoPZAMTIvm4yRd10Aa4QdlS37sWrfC+oZTaoSidVNXWaT9ME0wm1rg7fBg6I3tyHWno7+KSOREJ+J9uFNyI9ZiyFhZ2J0v8v0QIR1Ba8jLDwG2RkXqAolCmavBZ5mD5pNVahsP4Cahny4XW2w2kJ0laa+vbqPanDCGUhAYGPsAbbP+BS7XluMkIhwTH7sB9g/YikaGsrh8EWr8CiTrIHDHoNoFRjSTCeX2LCMI/pL6ltL8Pn+5+D2tmNw4jS1sc9CjCNVj7L7JraXzFWV0kJ9OKGpg+5EZsKpgTnda0/5Yl2VDU06C6kqFE3HqcAkUCWspany+0Aq5lW5L6CkbhsiQ5N0X+aA+MmID888Zp+Yz+cNqqmzWwPE4/GgsLAY77zzIR577P7A1ONjgHz3vGpjaj5qY9rdnA0tKqTCAreOJBtn+dilnfnVKlUOD3N4M1hPKVu7G+v/9x1dNZmtFgy94QyM+tFFel7VllxsfmoOancVYtgPzsKYey/V04sbtsLlatEbXenbiY5Jha3CgTU/fg3RaamIHJCA1roGtFbXoTW/HtHn9UPjFSVwOVQomEMwLPIcJJRloWpDHupUSDUWVqC1qgFhCbFIUCEfNyEd5ngrmotr0JxfjeaiGjgc0XD0i0FIcjhaquqw+9XFurlwws+vRVtVPXL+8j6iBifDdrsdSZmD0T9xJKIs/fXydsW+imW6ypFmN+krC7GHw2GW8EnRG9yE8Kwu7eVKU+DGgrf1+5ya9SOVYT7M3/EnVbldpSo3/3o9nFR9TW2VqFFVm/RPyWCHw/vkWt31qHDvUR9mE9LCxsIC/zwZKPH5/mf1QI2UmDEoa9ilB1qkRo/GqJRLvjJ8fH/FCuSUvAtHpFqPobFICx+DeHcmHPExusJsa65HuC0hqN8vNTur1A7FkevG6WlRIfoxCmpyMEiFp/SxhXRhhKF8pnaWfood6nLqoLv0gJXOyCAZ6T+U9SV9k/7RlF+gqilXN5nK4BcJFRmQIqMfZWdnXd7reodn2uB7jjki8li6vQ+kuLgEr78+67gB4nK59AZE9tjq6xtgVXtT4eH+jY28SbO5+9qG6WhqX8yi9jS83sDf4DjU30L+Hj513xOi/4Zm3aQp1dDR5MgDQi0BfJ7DnjvwONm4HDH9mORz8k0/vuo9qs+bXhcdy6qWw6KWw6OWP/ej1Ug9fTTC4qP8y6uqJpknynP2Yf1f3kO72shPevQ6ZFx4ZCVUu6UYC374Vwy77SyMv19Ggh1SvacQOX96H42l5Qi/Ngox9emoXV2ClpZ6FTSJiB7UHzGDUxA1IElXSuXr96JuXwncTe2wR4cjLDkG4aqKk76Y1oo6fXgcr1pnI24/DwPPPbQcTlV5bv7nhyhbskdXneZQM0yqMo0cmITMSyYjadLQ4+ybH6mutgRl+Tvg6a+qImsNausK0dJag1B7BJKih+oh2XZvOKwmu/4tkDShyFDwspY9sITZMTzuPAyJnvbln6yupRTzd/0Bw9LOQnrCBNQ2F6rKpxj1LWVobK6Ex+1CiCVCB4BHbUBjwlLUxi8Zte3qdV21qPh0vz6EUNjwaAweMQ1JMUOxo3weEsIG45Tk2/0vopQ17sHu0sUoqd+KIUnTVGhNV8sXgtW5L6HYuR2nJ9+LumVF+OL9mXCNb0XUycloOlgFlZd6/fjCfQhT1dvAqMnIiJmk35vaYdfNgX7qc6v23mUHXj5CB6pWqI39ArQ6GxAeEod+0cP1abYlzHaXLlLV3GBkxE/Awco1qG0tREbceF3dyehFGVEo1ZnDHq2rS5kuIbCnbIkK7AG6f2tT0Xs4d/ijqrIYqj+j+vOo/ojbVDBtLvwQE9JvxIiUc/3rOfDHdat1WafWbV1LsX5NqbrkILJ1ziI0uSoQZovFhIwbMCLpQv8DuqjbA0QqkDffnH3cAGlubvFvXNS7a21t1RuNkBD/Xqdct9k6L7OIvkvScW5TOzdutbXwutTW5AgmWO02/f088NEX2Pb0XMBuQVhK/JfHOStZvAUTf3U9hlw1FU6103QoTFVAqcdKDOV/ugH7Xv5cD0rof85oJE0ehlDHoQrM/y3xX2Rz5W53whZi/8qpRA//EssOWsdrmVRY29SlSVUnLWV1cNY368CTKqpszW7ZgUfSycN0ldWigqhdBZGEd0RGkm7SkxBrKq5G0bKtaCushcmj3neYQ21YhyFNLS/ifCir3oGi8o2oay9G/PgB8Fl8aGtsUDlsRlrKOMTUDkDlm3kYcOYk9D99iF5Wr1qnVjW/Fc2YM/8xNFdXIzIsWW00kxEXMxApmWOR2G+wqiv8lUV1UxHKG3eojWAhUvuPx57ffg5LmRljH7oS+bvWYk/hIlRZcpFcPhKjBl8Be0oYwvqpqiJBBW18HEKi7VDvGpuK30Z1ZZ6sGMT3T0X6jlOx7c+fICTegcxzJqP/maNhTw9Dsfrb7X5xkVrfHiRfMBSu7AY09iuFM6wNGdFTMDzyPISFBAZgqD9Mi6cZ+Q3rsKtkHmxqr35E8qVIDh+JShWexfWbUNGwF1ER/TE+8zrEWzP8j1MqG/JxoHoJmttrdCUXHhKvPkYROiAb28r0qEcZyp6ddDFSZJi7+sPvr1yF1Qdn4OLsPyI+PEUFTxOW73sC7b46TMv8qQqoAarScesBH0I+O/J5kfVukg+RTFDkI1JUsQULd/0JYeGxOCPzPqQkjfLP7KJuD5CSklJdgfz85/cFphyfNGHZ7XZERX39D+CIjMrr9qBYbWRlI9xW3YDmkhoMuf4MJI4zcEex+uqXrt6NgkU5egRdeL843c8lm4S6PWpvNbdUhY6qMFQg9j8lG6lnjNHzK3P2I3/hRj0sWxLIZnPAYrWhraEBlkgrht56JgZcMh5WhGLfzJXY+doCRAyKR1NpNUJiIjDup5ercEpD3vwNKPg0B61qfZlDrLDbwnQbvdfrUXvM7apqsakqK1b3cWVccKiq2vvSCux4YwEum/NbWANHwvZUu1G6Yada7hLUF5eo56yHq7ENHhW4UjVKlZkwfBAGXXIKXKNaUOPMR9t/WlG4IAcn/foaDLyo8yYhGUmY/8kGtBWrYFUb3sawMpQN2I6ESzMxJOwMhLfGoza1ELl7VsFb4cGkiTcje9j5gUcfRe0NlC7fhb3vL0Xt/hKc/tcfIa6TPr6u2F33GXYXfobxqdOxPv9N9IsajqmD7tah3aGpsBKupla421x6cIn0NVojjjxyeN7767DwN39HZFYSJt92I7Kmn1ifVLcGiBwYMS+vAO+++xEeeugncDgc6sN1/LZ29oEQ9V6yYZKBF1LBeF0eHPhwNXa9vEiPnpOqTUaCTf71jUgcn6UHMux+Ywn2z16pNmxtiB7UDwMvmYwB503QzWt6l9hk0s/TVFKNur3FejBG4eLN8Ho8unnOGmrHuj+9jcs++B0i0r6+70UGj7TVNqlA91dR8pskm/wGqs2L0PQITP3LD7t0ZGxpxq3dXYTarcXwNLnQNrgehbEb0NRSiXhXFnwLffDtNcHTpgIrxKeWLV4vn/xeSoa4u5vbUbe/WPc9StOhrLNN//pAr5vB15wWeBW1A75yB/a99znSzh6LQZee7G9uDihcukXPk2bNwRedhv2xy7C/dQVOz7oXKZZD/YkyICXnb7NQn1um+yGl2pXK1aWWQ4bQp589LnBPYPnPnsPWf83VTabTnrjrK4Nevk63BsjHHy9Abm6e7kyXULj44vOQlTUwMLdzDBCi7599s1bKVhdDrj09MOUQqdZaK+v1yLeuKliYg12vL9ZNb+e/+igSxwZf2Ul/UmtV/TGrjhMh/XWHj/CSvqjavUV6RKGMYJQKSc4XJBcZPp8wJjNwT+jfdS2550kMnn6aqlSzsO35ebpiSJk6EtU7VIWkKjPZoEsI7Jv1uX7MgAsm6mWv3HAAMPsQNjQWqUNHw5EWBUdiFPZ/+IUePTnk2mkYedeFCIk+9DsnGba++revqueYgNF3X4w1v38dzvoWPexegnnQFafoHYET0e1NWB3kabvyQykGCBF1lfThyCCC7wsJHAkRGe034ofn68CV5kQhQSehKRv54TefjYxzx+vpQravlTm5Kkj2oy6/RA+0aFGXBBWsJz10jR5k0Rl5nVW/nKnDJOuqU3Han394RJVzor61AOkqBggR0XdLKjkJm28q+OghIqJeqTvCQzBAiIgoKAwQIiIKCgOEiIiCwgAhIqKgMECIiCgoDBAiIgoKA4SIiILCACEioqAwQIiIKCgMECIiCgoDhIiIgsIAISKioDBAiIgoKAwQIiIKCgOEiIiCwgAhIqKgMECIiCgoDBAiIgoKA4SIiILCACEioqAwQIiIKCgMECIiCgoDhIiIgsIAISKioDBAiIgoKAwQIiIKCgOEiIiCwgAhIqKgMECIiCgoDBAiIgoKA4SIiILCACEioqAwQIiIKCgMECIiCgoDhIiIgsIAISKioDBAiIgoKAwQIiIKCgOEiIiCwgAhIqKgMECIiCgoDBAiIgoKA4SIiILCACEioqAwQIiIKCgMECIiCgoDhIiIgsIAISKioJxQgLhcbuzatRctLa2BKV/V1tau57e2tsLn8wWmEhHR941JbeS7tJVvaGjE66+/h/j4WFRUVOG6665AcnJSYK7f3LkLsG3bLsTERMNms+L6669CZGREYG7nqqpq1H1tiI6ODEwhIqLeoMsB8v77c2EymXDVVZdg+fIvkJubhzvu+EFgrt9zz83EtGlTMHJktq4+5P5fhwFCRNQ7dbkJq6ioBNnZQ/X1YcMGo7q69itNVG1tTpSVVWLLlu1wuz2BqV+vCzlDREQG0+UK5J//fBZXX30JBgzIQE1NLV544TU8/PC9sFqtgXv4m7AsFovuB9m3Lxd3332rbs46WlNTC7xeCRgTWlvb1GPMsNvtep7ZbNbNX0RE9N2w221dajE6WpcD5Iknnscll5yHwYMHobKyCi+//DYeeeS/jvmiM2a8jv79++Hii88NTDnEX534X7a2tl6HUGRkuL4toWI2syQhIvquyI57MLocIK+99i7S01Nx5plTsXXrDqxcuRb33vvDL/s6nE6XCgLLlwsiFUp6egouvPAcfftY2AdCRNQ7dTl2JDg2bNiEL75YjyVLPte3xT/+8SyWL1+lKok6PPnkC/jss2V4772PdJVy6qmT9X2IiOj7p8sViCguLsXmzdsxfPhgZGVl6mk5OVuRmBivqxPpaJdhvFKJTJ06GWFhYfo+x8MKhIiodzqhAPk2MECIiHqn4HpOiIioz2OAEBFRUBggREQUFAYIEREFhQFCRERBYYAQEVFQGCBERBQUBggREQWFAUJEREFhgBARUVAYIEREFBQGCBERBYUBQkREQWGAEBFRUBggREQUFAYIEREFhQFCRERBYYAQEVFQGCBERBQUBggREQWFAUJEREFhgBARUVAYIEREFBQGCBERBYUBQkREQWGAEBFRUBggREQUFAYIEREFhQFCRERBYYAQEVFQGCBERBQUBggREQWFAUJEREFhgBARUVAYIEREFBQGCBERBYUBQkREQWGAEBFRUBggREQUFAYIEREFhQFCRERBYYAQEVFQGCBERD2opaUVtbV1qKmpQ3V1Ldrb2wNzvrmGhkbMmTMfHo8nMKV7MUCIiHqI1+vFU0+9gGeeeQkvvfQGnn32JezevT8w95uTcFq/fhNMJlNgSvdigBAR9RCpDNxuN+6++1Y89NBP8Ktf/QxjxozQwdLc3ILW1jZs3rwNpaXlgUf4ybyNG7egrKwiMMXP5/Nh27ZdKoT26ee2WMyIiYlSQdKCTZu26QqnO1keVwLXe4QkpMViQWhoSGAKEVHfIEGxdu1GjB8/BmFhDl0pyMXpdOGJJ55HYWExqqpqMH/+EkRGRiAlpR/y8grw4ouvqfs48fnnq/XzZGSk6VB5+ukZ+v55efmIiAhHQkIclixZqZuyiotL8emnizBy5HCEh4fpx31TrECIiHqI2WzWYfH667Pw7LMz8e67H+rpEiJSfZx//lm46abpuO66K7Bw4VJdYcyePRdnnXUabrnlOtx11y06XCRMFixYoqqNaNx5502qorkNw4YNVs/Rrh9z0UXn4tZbr0daWn9s375Lv0Z3YIAQEfUQqUBsNhuuvfZy3HHHD3DFFRd/OV1aZRyOUH07LS1FB0FTU7MKhVZVRWTr6VJhREVFoqSkTF3KMXHieD1dSDjJ80hlIxchlYdM6y4MECKiHmQ2m5CUlKA38h1N+RIWcgkJsevbNTX+vouOQKmvr9f/Cmm6ksojNNSOiorKwFS/jufpCA2v19etHeoMECKiHiIb8/r6Bsya9THmzl2gm6f27TsAu92G9nYnFi9eoTvR33xzNk49dTKsVqv+9403Zqnp2/XIrQED0nWAnH76qfr+0qeyfPkX2Lp1h34e6Wfu0NbWrpvMuku3dqJLr/+SJZ/rN5aQEN+ljhp2oh+bx+PVbZvyoaGvkjZiq9XSrXtU3xeykZC9TRmFQ0eSvXHZOBvheyWf3dDQUP09l0rB7XYhOTkJsbEx2Lhxsw6H/fsPYtKk8Zg2bYp+TGbmAP2Ybdt2IjU1BVdffYl+nvj4OH3/LVu26+cbMWK4bt6SizSBdbxWamp/PTKrO5jUQvsC17+xt956X5dcgwYNxKJFy/GTn9zxtSEiIwakDTA6OjIwhTq4XG4dsFw3nautrdfrRtp66UjSVm4ymdX3z9/2TYfIsNnmZmN/r2Tn6M9//hd+8YsHvuy/MKJu++bJMDEZXnbllRdj3LhRiIuL1aVXV3APsnPSNsqN47HJuuH66RzXzbH1hu+V7FSPGjVcXeu2/ftvRbdVILm5efon8w8+eI++PXfuQl1GXX31pfr24WQPoONV6+rq9Z5SR+cQHeL1etQ6dLN57xhkL03WDXdAvkq+e7JeZENER+oN3yv5SNvtdrWcLt209W2S15LtbzDfo24LEOn4mTdvMR544G59e968RbqMlvHLR5NRA9JfIgssK8jlch218NwgiI4/DTeQnZP14181XD9H42fn2LhujiSrIT4+VvdFn6huCxD5qb2MFHj44Xv17Vmz5iAsLAwXX3yuvn0sEiZSTrIC+SrpA5G97KioiMAUOlxdXYNeN2yq+Sr5XkllHxbG79XRpAWkpYXfq+7Qbd88Gccsoxvkp/ciNzcfw4cP0dePp5vyi4iIvmPdFiBS/lx22QWYM2cBXnrpTYwalY1BgwYE5lKwWGUfG5sgKFj86HSPbh3GK6R0bm5uVhVJYmDK8XW8PDcGnZP1w3XTOfmdg4yooa/i9+r4+L3qHt0eIERE1Dew95GIiILCACEioqAwQIiIKCg9HiAFBUWoqKgK3KKiohI9BPpolZVVel2R3+E9d3JgvP37c/XBJ/sy+UGuHHhPfsB7uIMH81FbWxe41TfJOTTkx87yC/3DyfcqP5/fq2D1aCe6HPpEPtjyY7nx40fj5JMnBOb0PfLLfDmUs8slh6Awq41iO26++Vp9KIp163KQk7NV/9hSDtt8xRUXBR7V98gX/sknX8Rdd92sT+NZWVmtz+IWGxuNxsZmffY2OZVnXyM7HnIwUzntqfxQ7sYbr9ZHZ33nnQ/0D1LlkOFnnHGqHl7f18iOlxxaSU6+JAdvnT79cv27tfXrN2HDhs36B89ydNq+/L0KVo9VIPn5hdi1ay9uu+0G/aVfsmSFDpK+SoYUyiGbb7rpWvzgB9fog1PKyfHlx5lyZGOZJutqz579+qCVfdVHH83Xw8Q7znEwb95nyM4eqtbPdL2BWLx4uZ7el8g+oATFueeegXvuuV2f0lQO4S2nLi0rq9A7IrJx/PTTxX2ySpNTwZ500hhcd92VGDw4EytXrtHr7ND36nr9vZJKjU5MjwXIvn25SE9P1ddlr8nhCNMnfe+r5HAcmZkZgVv+o3HKuRzKyyvVunHoDYLIyEjVzRR9kZz7WULipJPGqr1sj54mzZ8SIELOf1BQ4D8SQl9SWlqmz20xenS23jGTz4t8fmSjmJWVqe8j54NQUYPq6hp9uy/JyEjX2xvZGZPgSEnppz83UtFHR/vPiyHfK7kPnZgeCxDZgzz8+Fc2m1X/CJGkOvP3C40YMUw38clZxTqEhIT0yfUkG0np55DTBcgBODuOpCobhY7Tfso06Qfoa6TpTo4LJnvaK1euxTPPzND9QrIuDv+Omc2WLyu3vkROxCRV+9///gx27tyDKVMm6iA9/IRScqKlvrhuvqkeCxDZKEpbbQfZEBy+oeyr5JSTcprKK664UO9Fyi+tZd10kD3vvraeZK/xtdfe1Scnk9N1lpWVY9OmrbqzWHY8Oppl5PMkG8m+SH5Ufd55Z+nmYKlA5Gx2Hf0hHWQ9yvrqS+RoBbNnf6z7PW6//QYdFHK6V1k3Mq+DrCce+v7E9ViAyGkVDx99JRuDfv2SArf6JulInzHjNZx11lSMHz9GT+vXL/mIikNOmi/rri+RDd+FF56jO4A7wlSmCekAlWY+UVJShri4GH29L5EOYQnXjnCQgRayNy2nOy0trdDTWlpadEUiHet9SVtbm+5El2bOxMQEXH75hVizZoM+fLmMzOogn6G+9r3qDt16TvQTIX/AjRu36I5z2VuStsiJE8cH5vY9Eh5PPz0DBw7kYejQwdi+fbf+0sv5j2XDKKV3dXWtDhDZmPalc13LAAM5T7R8waXfbOvWHboZQm7LSXek41wO5vnFF+tx0UXn6g1oXyLNVLIBlHNhy+g9GbF3/vln63W1YsUX+j6rVq3DwIEZGDHC31/UV0gzVVFRsR6QIvscS5euxLBhg3WgSLPojh27UVNTqwcbyGeH55A/MT06jFc2kCtWrNZ7kdJO2ZcPbiYBIl986ehsbW3Xe4tSkY0cOVzvbcvIEalEpk075WvPM/99d+DAQSQkxH/ZAbp79z7s3bsfY8eOwoAB6XpaXyTNezKsedKkcTpwhZzxU8JDdtgkdPsi2cKtXbsBVVXVKlTT1OdkZGC6fK/W6taP00/n9yoYPJgiEREFhfUaEREFhQFCRERBYYAQEVFQGCBERBQUBggREQWFAUJEREFhgBARUVAYIEREFBQGCBERBYUBQkREQWGAEBFREID/D886tco0BXGrAAAAAElFTkSuQmCC)"],"metadata":{"id":"kWx39Bfe1GHW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lh1okw7jflVX"},"outputs":[],"source":["# Test code\n","class GreedyCTCDecoder:\n","\n","    def __init__(self):\n","        self.text_to_int = {\"'\": 0, \" \": 1, \"a\": 2, \"b\": 3, \"c\": 4,\n","                            \"d\": 5, \"e\": 6, \"f\": 7, \"g\": 8, \"h\": 9,\n","                            \"i\": 10, \"j\": 11, \"k\": 12, \"l\": 13, \"m\": 14,\n","                            \"n\": 15, \"o\": 16, \"p\": 17, \"q\": 18, \"r\": 19,\n","                            \"s\": 20, \"t\": 21, \"u\": 22, \"v\": 23, \"w\": 24,\n","                            \"x\": 25, \"y\": 26, \"z\": 27}\n","        self.int_to_text = {v: k for k, v in self.text_to_int.items()}\n","        self.blank = 28\n","\n","    def __call__(self, emission, sentence, length):\n","\n","        # Compute prediction\n","        indices = torch.argmax(emission, dim=-1)  # [num_seq,]\n","        indices = torch.unique_consecutive(indices, dim=-1)\n","        indices = [i.item() for i in indices if i.item() != self.blank]\n","        joined = \"\".join([self.int_to_text[i] for i in indices])\n","\n","        # Compute actual\n","        truth = \"\"\n","        sentence = sentence.squeeze()\n","        for i in range(length):\n","            truth += self.int_to_text[int(sentence[i])]\n","\n","        return joined, truth\n","\n","\n","def test(model_name, num_samples=-1):\n","\n","    test_dataset = LibriSpeechDataset(\"valid\")\n","\n","    valid_loader = DataLoader(dataset=test_dataset,\n","                              batch_size=1,\n","                              shuffle=False,\n","                              collate_fn=collate,\n","                              num_workers=3,\n","                              pin_memory=False)\n","\n","    # Model\n","    model = ASR(hp[\"dropout\"], hp[\"hidden_size\"], hp[\"rnn_layers\"], hp[\"cnn_layers\"], hp[\"n_mels\"])\n","    model = model.cuda()\n","    checkpoint = torch.load(model_name)\n","    model.load_state_dict(checkpoint)\n","    model.eval()\n","\n","    # Output sample predictions\n","    greedy_decoder = GreedyCTCDecoder()\n","    wer = WordErrorRate()\n","    cer = CharErrorRate()\n","    avg_wer, avg_cer = [], []\n","    for spectograms, labels, data_lengths, label_lengths in iter(valid_loader):\n","        if num_samples != -1:\n","            num_samples -= 1\n","        if num_samples == -1:\n","            break\n","        spectograms, labels = spectograms.cuda(), labels.cuda()\n","        output, _ = model(spectograms)\n","        output = F.log_softmax(output, dim=2)\n","        output = output.squeeze(0)\n","        predicted = greedy_decoder(output, labels, label_lengths)\n","        word_error_rate = wer([predicted[0]], [predicted[1]])\n","        char_error_rate = cer([predicted[0]], [predicted[1]])\n","        avg_wer += [word_error_rate]\n","        avg_cer += [char_error_rate]\n","        if num_samples != -1:\n","            print(\"Predicted:\", predicted[0])\n","            print(\"Actual:\", predicted[1])\n","            print(\"Word Error Rate:\", word_error_rate)\n","            print(\"Char Error Rate:\", char_error_rate)\n","            print(\"_____________________________________\")\n","        # time.sleep(1)\n","\n","    print(\"Avg WER:\", sum(avg_wer)/len(avg_wer))\n","    print(\"Avg CER:\", sum(avg_cer)/len(avg_cer))\n","\n","\n","def test_from_voice(audio_file, ground_truth_text):\n","\n","    # Load data from file\n","    data_piece = torchaudio.load(audio_file)[0]\n","    label_piece = ground_truth_text\n","    one_dataset = LibriSpeechDataset(\"one\", (data_piece, label_piece))\n","    loader = DataLoader(dataset=one_dataset, batch_size=1, shuffle=True, collate_fn=collate)\n","\n","    # Model\n","    model = ASR(hp[\"dropout\"], hp[\"hidden_size\"], hp[\"rnn_layers\"], hp[\"cnn_layers\"], hp[\"n_mels\"])\n","    model = model.cuda()\n","    checkpoint = torch.load(\"best_model_bs10.pth\")\n","    model.load_state_dict(checkpoint)\n","    model.eval()\n","\n","    # Output sample predictions\n","    greedy_decoder = GreedyCTCDecoder()\n","    wer = WordErrorRate()\n","    cer = CharErrorRate()\n","    for spectograms, labels, data_lengths, label_lengths in iter(loader):\n","        spectograms, labels = spectograms.cuda(), labels.cuda()\n","        output, _ = model(spectograms)\n","        output = F.log_softmax(output, dim=2)\n","        output = output.squeeze(0)\n","        predicted = greedy_decoder(output, labels, label_lengths)\n","        word_error_rate = wer([predicted[0]], [predicted[1]])\n","        char_error_rate = cer([predicted[0]], [predicted[1]])\n","        print(\"Predicted:\", predicted[0])\n","        print(\"Actual:\", predicted[1])\n","        print(\"Word Error Rate:\", word_error_rate)\n","        print(\"Char Error Rate:\", char_error_rate)\n","        print(\"_____________________________________\")\n","        time.sleep(1)"]},{"cell_type":"markdown","source":["We build two test functions, which are shown above; one for extracting data from our librispeech dataset, and the other for processing and testing on our own audio files. We compute the **word error rate (WER)** and **character error rate (CER)** which are more intuitive metrics for evaluating the performance of our model as opposed to loss. Below, we test it first on 5 examples from our validation dataset, and we additionally test it on our own recorded voices, which act as our test dataset."],"metadata":{"id":"BjCYf3Wfwnn2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nfk7ra3aflVY"},"outputs":[],"source":["print(\"Error Rate for BS=10 Model\")\n","print(\"--------------------------\")\n","test(\"best_model_bs10.pth\", num_samples=15_000)\n","print(\"Error Rate for BS=15 Model\")\n","print(\"--------------------------\")\n","test(\"best_model_bs15.pth\", num_samples=15_000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jzLXOQNcflVY"},"outputs":[],"source":["print(\"##########################\")\n","print(\"Samples for BS=10 Model\")\n","print(\"##########################\")\n","test(\"best_model_bs10.pth\", num_samples=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4cdK11u2flVY"},"outputs":[],"source":["print(\"##########################\")\n","print(\"SAMPLES FOR BS=15 Model\")\n","print(\"##########################\")\n","test(\"best_model_bs15.pth\", num_samples=5)"]},{"cell_type":"markdown","source":["It appears that the average WER and CER are roughly the same between both models, however, the batch_size=15 model does slightly better. Let's use this model on our test dataset, which is a recording of our voices reading the following sentences:"],"metadata":{"id":"tWaUcx2g5j9d"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q5ace7gtflVZ"},"outputs":[],"source":["audio1_text = \"i am looking for a book\"\n","audio2_text = \"when setting the table for a meal remember to place the cups spoons\" \\\n","              \" forks in the correct spot the cups should go at the top right side\" \\\n","              \" while the spoons and forks are positioned at the side of the plate\" \\\n","              \" additionally dont forget to include a bottle of water within reach\" \\\n","              \" for everybody to stay hydrated\"\n","audio3_text = \"artificial intelligence or ai is a technology that allows machines to\" \\\n","              \" learn from data and perform tasks that typically require human intelligence\" \\\n","              \" this technology is continuously evolving and has the potential to revolutionize\" \\\n","              \" various industries as ai continues to advance it is important for us to \" \\\n","              \" understand its capabilities and implications in our society\"\n","\n","test_from_voice(\"audio1.wav\", audio1_text)\n","test_from_voice(\"audio2.wav\", audio2_text)\n","test_from_voice(\"audio3.wav\", audio3_text)"]},{"cell_type":"markdown","source":["Notably the model does appear to do much worse on our own voice, which is expected since we did not explicitly train for this data. While this is true, the predicted words and characters somewhat look similar to the ground truth, although the spelling of the words are very off. The model seems to do better with shorter sentences as well and when we speak slower, so we will aim to do that for our next set of recordings. In order to further improve our model performance, we can do the following things:\n","\n","1.   **Expand the dataset to include recordings of our own voice.** This will help better generalize to our own voices as opposed to other voices in the librispeech dataset which may not resemble our own.\n","2.   **More hyperparameter tuning**. It would be interesting to try different sets of hyperparameters and see the impact of other variables like number of residual CNN layers and number of GRU layers and dropout, which can improve our model complexity and help generalize our model better. We will look to achieve this considering we have more time.\n","3.   **Include WER and CER into the loss function**. This may also be something worthy to try, since our actual test performance is computed based on these metrics. It is possible that model does better if we instead use WER and CER incorporated into the loss function.\n","4.   **Use a Language Model to improve error rate.** We can use a language model to help with the word error rates as language models can determine how probable a sentence is correct given a reference sentence. This is achieved sin the language models are are built from large corpuses of data so they capture semantic understanding in sentences very well. This way, we can manually correct for spelling mistakes or words that are lost in the context as a post-processing technique without having to spend several days retraining our ASR model.\n","\n","Overall, we have a strong baseline model that we've trained for our speech to text pipeline and we will aim to do some further fine tuning over the next few weeks.\n","\n"],"metadata":{"id":"FuN6VzU83yEZ"}},{"cell_type":"markdown","source":["## **Part 3: What we have left to do and why we will succeed?**"],"metadata":{"id":"mvUp0uygI5jM"}},{"cell_type":"markdown","source":["Given our currently trained models, it is evident that we have some strong  results that we will further need to finetune.\n","\n","1.   **R-CNN:** Regarding the R-CNN, we currently have an RPN model that works decently with the COCO dataset. The model is able successfully predict object bounding boxes and overfit on a small dataset. The next step for this model is to incorporate the object classifier (last part of Faster RCNN), refine the model backbone (feature extractor) and track training/validation loss and accuracies on a larger dataset. We will further finetune the prediction confidence and test the module with image data from a real camera. This will give us a unique testing dataset with which we can verify the true performance of the model.\n","2.   **Speech to Text:** We have a decently performative model for automatic speech recognition that works with voice data that is not within the dataset. We, however, need to improve the WER and CER of the test dataset as the error still appears to be large. We can do this by the steps detailed at the end of Section 2.\n","3. **LLM:** We currently have some progress on the LLM pipeline, which was not shown in this document. Our next steps for us will be to integrate the outputs of the R-CNN and Speech to Text module to work with the LLM interface.\n","\n","Considering we have made significant progress in each element of our pipeline, we believe we will be able to successfully complete the project within the given timeframe. Our next week will be focused on fine tuning our models, and the week after will be integration of our LLM and our presentation.\n","\n"],"metadata":{"id":"1ZdE32slI_OY"}},{"cell_type":"markdown","source":["#Reference"],"metadata":{"id":"OuhtnbX8_eDW"}},{"cell_type":"markdown","source":["[1] Source: Ren, S. et al. (2016) Faster R-CNN: Towards real-time object detection with region proposal networks, arXiv.org. Available at: https://arxiv.org/abs/1506.01497 (Accessed: 30 March 2024).\n"],"metadata":{"id":"qUUiEbNR_ahl"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}